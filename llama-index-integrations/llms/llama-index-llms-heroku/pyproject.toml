[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[dependency-groups]
dev = [
    "black[jupyter]<=23.9.1,>=23.7.0",
    "codespell[toml]>=v2.2.6",
    "diff-cover>=9.2.0",
    "httpx<0.28.0",
    "ipython==8.10.0",
    "jupyter>=1.0.0,<2",
    "mypy==0.991",
    "pre-commit==3.2.0",
    "pylint==2.15.10",
    "pytest==7.2.1",
    "pytest-asyncio>=0.23.0,<0.24",
    "pytest-cov>=6.1.1",
    "pytest-httpx>=0.30.0",
    "pytest-mock==3.11.1",
    "respx>=0.21.1,<0.22",
    "ruff==0.11.11",
    "types-Deprecated>=0.1.0",
    "types-PyYAML>=6.0.12.12,<7",
    "types-protobuf>=4.24.0.4,<5",
    "types-redis==4.5.5.0",
    "types-requests==2.28.11.8",
    "types-setuptools==67.1.0.0",
]
test_integration = [
    "pytest-httpx",
    "requests-mock>=1.12.1,<2",
]

[project]
name = "llama-index-llms-heroku"
version = "0.1.0"
description = "llama-index llms heroku managed inference integration"
authors = [{name = "LlamaIndex", email = "dev@llamaindex.ai"}]
requires-python = ">=3.9,<4.0"
readme = "README.md"
license = "MIT"
dependencies = [
    "llama-index-llms-openai-like>=0.5.0,<0.6",
    "llama-index-core>=0.13.0,<0.14",
]

[tool.codespell]
check-filenames = true
check-hidden = true
skip = "*.csv,*.html,*.json,*.jsonl,*.pdf,*.txt,*.ipynb"

[tool.hatch.build.targets.sdist]
include = ["llama_index/"]

[tool.hatch.build.targets.wheel]
include = ["llama_index/"]

[tool.llamahub]
contains_example = true
import_path = "llama_index.llms.heroku"

[tool.llamahub.class_authors]
Heroku = "llama-index"

[tool.mypy]
disallow_untyped_defs = true
exclude = ["_static", "build", "examples", "notebooks", "venv"]
ignore_missing_imports = true
python_version = "3.8"

[tool.pytest.ini_options]
markers = [
    "integration: mark test as an integration test",
]

[tool.uv]
default-groups = [
    "dev",
]
