{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "fedcd46b",
            "metadata": {},
            "source": [
                "# Llama Debug Handler Demo\n",
                "\n",
                "Here we showcase the capabilities of our LlamaDebugHandler in logging events as we run queries\n",
                "within LlamaIndex.\n",
                "\n",
                "**NOTE**: This is a beta feature. The usage within different classes and the API interface\n",
                "    for the CallbackManager and LlamaDebugHandler may change!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "8e94187d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/anaconda3/envs/langchain/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "from llama_index.callbacks import CallbackManager, LlamaDebugHandler, CBEventType"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "49d93af9",
            "metadata": {},
            "outputs": [],
            "source": [
                "from llama_index import GPTListIndex, ServiceContext, SimpleDirectoryReader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "02e1e606",
            "metadata": {},
            "outputs": [],
            "source": [
                "docs = SimpleDirectoryReader(\"../paul_graham_essay/data\").load_data()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ee34d08b",
            "metadata": {},
            "source": [
                "## Callback Manager Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "c667d70b",
            "metadata": {},
            "outputs": [],
            "source": [
                "llama_debug = LlamaDebugHandler()\n",
                "callback_manager = CallbackManager([llama_debug])\n",
                "service_context = ServiceContext.from_defaults(callback_manager=callback_manager)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "25851e27",
            "metadata": {},
            "source": [
                "## Trigger the callback with a query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "66db8c3f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
                    ]
                }
            ],
            "source": [
                "index = GPTListIndex.from_documents(docs, service_context=service_context)\n",
                "query_engine = index.as_query_engine()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "11d4840b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 28944 tokens\n",
                        "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
                    ]
                }
            ],
            "source": [
                "response = query_engine.query(\"What did the author do growing up?\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4e69b186",
            "metadata": {},
            "source": [
                "## Explore the Debug Information\n",
                "\n",
                "The callback manager will log several start and end events for the following types:\n",
                "- CBEventType.LLM\n",
                "- CBEventyType.EMBEDDING\n",
                "- CBEventType.CHUNKING\n",
                "- CBEventType.RETRIEVE\n",
                "- CBEventType.SYNTHESIZE \n",
                "- CBEventType.TREE\n",
                "- CBEventType.QUERY\n",
                "\n",
                "The LlamaDebugHandler provides a few basic methods for exploring information about these events"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "83a4ba2d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "EventStats(total_secs=161.0, average_secs=13.416666666666666, total_count=12)\n"
                    ]
                }
            ],
            "source": [
                "# Print info on the LLM calls during the list index query\n",
                "print(llama_debug.get_event_time_info(CBEventType.LLM))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "c4831a1d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CBEvent(event_type=<CBEventType.LLM: 'llm'>, payload=None, time='05/01/2023, 19:58:56', id_='1fd2e2c5-a334-47cc-9f81-188035825633')\n",
                        "dict_keys(['stage', 'response', 'formatted_prompt'])\n",
                        "\n",
                        "Growing up, the author wrote short stories, programmed on an IBM 1401, and wrote programs for a TRS-80 microcomputer. He wrote simple games, a program to predict how high his model rockets would fly, and a word processor. He also studied philosophy in college, but switched to AI when he realized it was a more exciting field. He reverse-engineered SHRDLU for his undergraduate thesis, and wrote a book about Lisp hacking while in grad school. He also took art classes at Harvard and applied to art schools.\n"
                    ]
                }
            ],
            "source": [
                "# Print info on llm inputs/outputs - returns start/end events for each LLM call\n",
                "event_pairs = llama_debug.get_llm_inputs_outputs()\n",
                "print(event_pairs[0][0])\n",
                "print(event_pairs[0][1].payload.keys())\n",
                "print(event_pairs[0][1].payload['response'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "cf70da51",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "dict_keys(['documents'])\n",
                        "dict_keys(['nodes'])\n"
                    ]
                }
            ],
            "source": [
                "# Get info on any event type\n",
                "event_pairs = llama_debug.get_event_pairs(CBEventType.CHUNKING)\n",
                "print(event_pairs[0][0].payload.keys())  # get first chunking start event\n",
                "print(event_pairs[0][1].payload.keys())  # get first chunking end event"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "449af1e5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clear the currently cached events\n",
                "llama_debug.flush_event_logs()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dde936fa",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}