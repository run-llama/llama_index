{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "978146e2",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/llm/openllm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717d3d4-942b-4d86-9435-fc44b3ac6d39",
   "metadata": {},
   "source": [
    "# OpenLLM\n",
    "\n",
    "There are two ways to interface with LLMs from [OpenLLM](https://github.com/bentoml/OpenLLM).\n",
    "\n",
    "- Through [`openllm`](https://github.com/bentoml/OpenLLM) package if you want to run locally:\n",
    "  use `llama_index.llms.OpenLLM`\n",
    "- If there is a running OpenLLM Server, then it will wraps [openllm-client](https://github.com/bentoml/OpenLLM/tree/main/openllm-client):\n",
    "  use `llama_index.llms.OpenLLMAPI`\n",
    "\n",
    "There are _many_ possible permutations of these two, so this notebook only details a few.\n",
    "See [OpenLLM's README](https://github.com/bentoml/OpenLLM) for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf0f2e-8d8d-4e42-81bf-866c759221e1",
   "metadata": {},
   "source": [
    "In the below line, we install the packages necessary for this demo:\n",
    "\n",
    "- `openllm[vllm]` is needed for `OpenLLM` if you have access to GPU, otherwise `openllm`\n",
    "- `openllm-client` is needed for `OpenLLMAPI`\n",
    "- The quotes are needed for Z shell (`zsh`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1216b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-openllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b04b4a5-6fce-4188-a538-9a5ce2fa56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"openllm\"  # use 'openllm[vllm]' if you have access to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac8f9f-7136-43f7-9e9f-de679e74d66e",
   "metadata": {},
   "source": [
    "Now that we're set up, let's play around:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c577674",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86028752",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0465029c-fe69-454a-9561-55f7a382b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "from llama_index.llms.openllm import OpenLLM, OpenLLMAPI\n",
    "from llama_index.core.llms import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63abbc12-4bc7-423c-b6ef-1a49fd2c9c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\n",
    "    \"OPENLLM_ENDPOINT\"\n",
    "] = \"na\"  # Change this to a remote server that you might run OpenLLM at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27feba3-d027-4d10-b1af-1e130e764a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha\n",
    "# downloaded (if first invocation) to the local Hugging Face model cache,\n",
    "# and actually runs the model on your local machine's hardware\n",
    "local_llm = OpenLLM(\"HuggingFaceH4/zephyr-7b-alpha\")\n",
    "\n",
    "# This will use the model running on the server at localhost:3000\n",
    "remote_llm = OpenLLMAPI(address=\"http://localhost:3000\")\n",
    "\n",
    "# Note here you don't have to pass in the address if OPENLLM_ENDPOINT environment variable is set\n",
    "# address if not pass is address=os.getenv(\"OPENLLM_ENDPOINT\")\n",
    "remote_llm = OpenLLMAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b801bef7-2593-49e2-a550-721e6b796486",
   "metadata": {},
   "source": [
    "Underlying a completion with `OpenLLM` supports continuous batching with [vLLM](https://vllm.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631269c9-38ca-49d2-a7f0-f88e21adef6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " beyond!\n",
      "\n",
      "As a lifelong lover of all things Pixar, I couldn't resist writing about the most recent release in the Toy Story franchise. Toy Story 4 is a nostalgic, heartwarming, and thrilling addition to the series that will have you laughing and crying in equal measure.\n",
      "\n",
      "The movie follows Woody (Tom Hanks), Buzz Lightyear (Tim Allen), and the rest of the gang as they embark on a road trip with their new owner, Bonnie. However, things take an unexpected turn when Woody meets Bo Peep (Annie Pot\n"
     ]
    }
   ],
   "source": [
    "completion_response = remote_llm.complete(\"To infinity, and\")\n",
    "print(completion_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa723d6-4308-4d94-9609-8c51ce8184c3",
   "metadata": {},
   "source": [
    "`OpenLLM` and `OpenLLMAPI` also supports streaming, synchronous and asynchronous for `complete`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dbe5df-e74d-46e3-8957-d109fbe3cc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " often a topic of philosophical debate. Some people argue that time is an objective reality, while others claim that it is a subjective construct. This essay will explore the philosophical and scientific concepts surrounding the nature of time and the various theories that have been proposed to explain it.\n",
      "\n",
      "One of the earliest philosophical theories of time was put forward by Aristotle, who believed that time was a measure of motion. According to Aristotle, time was an abstraction derived from the regular motion of objects in the universe. This theory was later refined by Galileo and Newton, who introduced the concept of time"
     ]
    }
   ],
   "source": [
    "for it in remote_llm.stream_complete(\n",
    "    \"The meaning of time is\", max_new_tokens=128\n",
    "):\n",
    "    print(it, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4153da49-5ce9-4008-8148-7c54ac672e27",
   "metadata": {},
   "source": [
    "They also support chat API as well, `chat`, `stream_chat`, `achat`, and `astream_chat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19106402-9516-4bc0-b8b5-359281f61f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have beliefs or personal opinions, but according to my programming, the meaning of life is subjective and can vary from person to person. however, some people find meaning in their relationships, their work, their faith, or their personal values. ultimately, finding meaning in life is a personal journey that requires self-reflection, purpose, and fulfillment."
     ]
    }
   ],
   "source": [
    "async for it in remote_llm.astream_chat(\n",
    "    [\n",
    "        ChatMessage(\n",
    "            role=\"system\", content=\"You are acting as Ernest Hemmingway.\"\n",
    "        ),\n",
    "        ChatMessage(role=\"user\", content=\"Hi there!\"),\n",
    "        ChatMessage(role=\"assistant\", content=\"Yes?\"),\n",
    "        ChatMessage(role=\"user\", content=\"What is the meaning of life?\"),\n",
    "    ]\n",
    "):\n",
    "    print(it.message.content, flush=True, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
