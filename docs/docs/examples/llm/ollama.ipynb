{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d1b897a",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/llm/ollama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e33dced-e587-4397-81b3-d6606aa1738a",
   "metadata": {},
   "source": [
    "# Ollama - Llama 3.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5863dde9-84a0-4c33-ad52-cc767442f63f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, follow the [readme](https://github.com/jmorganca/ollama) to set up and run a local Ollama instance.\n",
    "\n",
    "When the Ollama app is running on your local machine:\n",
    "- All of your local models are automatically served on localhost:11434\n",
    "- Select your model when setting llm = Ollama(..., model=\"<model family>:<version>\")\n",
    "- Increase defaullt timeout (30 seconds) if needed setting Ollama(..., request_timeout=300.0)\n",
    "- If you set llm = Ollama(..., model=\"<model family\") without a version it will simply look for latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833bdb2b",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4816bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad297f19-998f-4485-aa2f-d67020058b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ced37-9a42-47be-9a39-4218521f5e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.1:latest\", request_timeout=120.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b10bb-e911-47fb-8e84-19828cf224be",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm.complete(\"Who is Paul Graham?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd14f4e-c245-4384-a471-97e4ddfcb40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is a British-American computer scientist, entrepreneur, and writer. He's best known for co-founding several successful startups, including viaweb (which later became Yahoo!'s shopping site), O'Reilly Media's online bookstore, and Y Combinator, a well-known startup accelerator.\n",
      "\n",
      "Here are some interesting facts about Paul Graham:\n",
      "\n",
      "1. **Computer science background**: Graham has a Ph.D. in computer science from Harvard University.\n",
      "2. **Startup success**: He co-founded viaweb, which was acquired by Yahoo! for $49 million, and later became the foundation of Yahoo!'s shopping site.\n",
      "3. **Y Combinator**: In 2005, Graham co-founded Y Combinator, a startup accelerator that has funded over 2,000 companies, including Dropbox, Airbnb, Reddit, and Stripe.\n",
      "4. **Writing career**: Graham is also a talented writer and has published several essays on entrepreneurship, startups, and programming. His writing is known for its clarity, humor, and insight.\n",
      "5. **Philosophical views**: Graham has expressed interest in philosophical ideas related to startup culture, such as the importance of experimentation, iteration, and individual freedom.\n",
      "\n",
      "Some popular writings by Paul Graham include:\n",
      "\n",
      "* \"How To Make Wealth\" ( essay on building wealth through startups)\n",
      "* \"The Three Colors of Money\" (essay on how money influences people's behavior)\n",
      "* \"Startup = Growth\" (essay on the key characteristics of successful startups)\n",
      "\n",
      "Overall, Paul Graham is a respected figure in the tech industry and startup world, known for his entrepreneurial spirit, writing talent, and commitment to helping others succeed.\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ba9503c-b440-43c6-a50c-676c79993813",
   "metadata": {},
   "source": [
    "#### Call `chat` with a list of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a4a55-5680-4dc6-a44c-fc8ad7892f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9bfe53-d15b-4e75-9d91-8c5d024f4eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Me hearty! Me name be Captain Zara \"Blackheart\" McSnazz, the most feared and infamous pirate to ever sail the Seven Seas! *adjusts eye patch*\n",
      "\n",
      "Me ship, the \"Maverick's Revenge\", be a sturdy galleon with three masts and a hull as black as me heart. She's fast, she's fierce, and she's got more cannons than a small army!\n",
      "\n",
      "So, what brings ye to these fair waters? Are ye lookin' for adventure, treasure, or just a good swabbin' of the decks?\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25ad1b00-28fc-4bcd-96c4-d5b35605721a",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13c641fa-345a-4dce-87c5-ab1f6dcf4757",
   "metadata": {},
   "source": [
    "Using `stream_complete` endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da1ef1-2f6b-497c-847b-62dd2df11491",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.stream_complete(\"Who is Paul Graham?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b851def-5160-46e5-a30c-5a3ef2356b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is a British-American entrepreneur, programmer, and essayist. He's best known for co-founding the online startup accelerator Y Combinator (YC) with his partner Jessica Livingston in 2005.\n",
      "\n",
      "Graham was born in London, England in 1964. He developed an interest in computer programming at a young age and attended the University of California, Berkeley, where he earned a degree in Applied Math. After college, he worked as a programmer for several companies, including Bell Labs.\n",
      "\n",
      "In the early 1990s, Graham became interested in online communities and started a website called \"The Daily WTF\" (an acronym for \"There's Probably Not A God\"). However, it was his essay \"How to Make Wealth History,\" written in 2002, that really caught attention. In the essay, he argued that the Internet had made it possible for entrepreneurs to create wealth without needing to be wealthy themselves.\n",
      "\n",
      "Encouraged by this idea, Graham and Livingston started Y Combinator (YC) as a way to support and fund startups with innovative ideas. The program's goal was to provide seed funding, mentorship, and resources to help young companies grow quickly. Since its inception, YC has invested in over 2,000 companies, including well-known successes like Airbnb, Dropbox, Reddit, and Twitch.\n",
      "\n",
      "Today, Graham is a respected voice on the topic of entrepreneurship, innovation, and startup success. His essays and writings have been widely read and discussed online, and he's often invited to speak at conferences and events around the world.\n",
      "\n",
      "Some popular essays by Paul Graham include:\n",
      "\n",
      "* \"How to Make Wealth History\" (2002)\n",
      "* \"The 100-Year Buy\" (2013) - an essay about the impact of Moore's Law on innovation\n",
      "* \"What You'll Do\"\n",
      "* \"Startup = Growth\""
     ]
    }
   ],
   "source": [
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca52051d-6b28-49d7-98f5-82e266a1c7a6",
   "metadata": {},
   "source": [
    "Using `stream_chat` endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe553190-52a9-436d-84ae-4dd99a1808f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = llm.stream_chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c503c-f893-4b6b-8a65-a9a27b636046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yer lookin' fer me name, eh? Well, matey, I be Captain Calico Blackbeak, the most feared and infamous pirate to ever sail the seven seas! Me name's as colorful as me parrot, Polly, and me reputation's as black as me trusty cutlass.\n",
      "\n",
      "Now, don't ye be thinkin' that just 'cause me name's got \"Blackbeak\" in it, I'm a scurvy dog with a heart o' stone. No sir! I've got a heart o' gold, hidden deep beneath me tough exterior, and I'd do anything to protect me crew and me ship, the \"Maverick's Revenge\".\n",
      "\n",
      "So, what be yer business here, matey? Are ye lookin' fer a swashbucklin' adventure or just wantin' to hear tales o' the high seas?"
     ]
    }
   ],
   "source": [
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f66168",
   "metadata": {},
   "source": [
    "## JSON Mode\n",
    "\n",
    "Ollama also supports a JSON mode, which tries to ensure all responses are valid JSON.\n",
    "\n",
    "This is particularly useful when trying to run tools that need to parse structured outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8339d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.1:latest\", request_timeout=120.0, json_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba873c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \n",
      "\"Name\": \"Paul Graham\",\n",
      "\"Wikipedia_URL\": \"https://en.wikipedia.org/wiki/Paul_Graham_(programmer)\",\n",
      "\"Brief_Description\": \"American computer programmer, entrepreneur, venture capitalist, and essayist.\",\n",
      "\"Occupations\":\n",
      "  [\n",
      "    {\"Year\":null,\"Job\":\"Programmer\",\"Company\":null},\n",
      "    {\"Year\":1997,\"Job\":\"Founder\",\"Company\":\"Viaweb\"},\n",
      "    {\"Year\":2005,\"Job\":\"Founder\",\"Company\":\"Y Combinator\"}\n",
      "  ],\n",
      "\"Education\":\n",
      "  [\n",
      "     {\"Institution\": \"University of California, Berkeley\", \"Degree\": \"Bachelor of Arts\"},\n",
      "     {\"Institution\": \"Harvard University\", \"Degree\": \"Master of Arts\"}\n",
      "  ],\n",
      "\"Awards\":\n",
      "[\n",
      "  {\"Name\": null,\"Year\":null}\n",
      "],\n",
      "\"Notable_Algorithms\":\n",
      "[\n",
      "  {\"Algorithm_name\":\"Viaweb algorithm\",\"Year\":1997}\n",
      "]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\n",
    "    \"Who is Paul Graham? Output as a structured JSON object.\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1377e244",
   "metadata": {},
   "source": [
    "## Structured Outputs\n",
    "\n",
    "We can also attach a pyndatic class to the LLM to ensure structured outputs. This will use Ollama's builtin structured output capabilities for a given pydantic class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5b94ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.bridge.pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Song(BaseModel):\n",
    "    \"\"\"A song with name and artist.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    artist: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c837c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.1:latest\", request_timeout=120.0)\n",
    "\n",
    "sllm = llm.as_structured_llm(Song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Radioactive\",\"artist\":\"Imagine Dragons\"}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "response = sllm.chat([ChatMessage(role=\"user\", content=\"Name a random song!\")])\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca7636",
   "metadata": {},
   "source": [
    "Or with async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb526f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Lose Yourself\",\"artist\":\"Eminem\"}\n"
     ]
    }
   ],
   "source": [
    "response = await sllm.achat(\n",
    "    [ChatMessage(role=\"user\", content=\"Name a random song!\")]\n",
    ")\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdad7904",
   "metadata": {},
   "source": [
    "You can also stream structured outputs! Streaming a structured output is a little different than streaming a normal string. It will yield a generator of the most up to date structured object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c40157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":null,\"artist\":null}\n",
      "{\"name\":null,\"artist\":null}\n",
      "{\"name\":null,\"artist\":null}\n",
      "{\"name\":null,\"artist\":null}\n",
      "{\"name\":null,\"artist\":null}\n",
      "{\"name\":null,\"artist\":null}\n",
      "{\"name\":null,\"artist\":null}\n",
      "{\"name\":null,\"artist\":\"\"}\n",
      "{\"name\":null,\"artist\":\"The\"}\n",
      "{\"name\":null,\"artist\":\"The Black\"}\n",
      "{\"name\":null,\"artist\":\"The Black Keys\"}\n",
      "{\"name\":null,\"artist\":\"The Black Keys\"}\n",
      "{\"name\":null,\"artist\":\"The Black Keys\"}\n",
      "{\"name\":null,\"artist\":\"The Black Keys\"}\n",
      "{\"name\":null,\"artist\":\"The Black Keys\"}\n",
      "{\"name\":null,\"artist\":\"The Black Keys\"}\n",
      "{\"name\":\"\",\"artist\":\"The Black Keys\"}\n",
      "{\"name\":\"Lon\",\"artist\":\"The Black Keys\"}\n",
      "{\"name\":\"Lonely\",\"artist\":\"The Black Keys\"}\n",
      "{\"name\":\"Lonely Boy\",\"artist\":\"The Black Keys\"}\n",
      "{\"name\":\"Lonely Boy\",\"artist\":\"The Black Keys\"}\n",
      "{\"name\":\"Lonely Boy\",\"artist\":\"The Black Keys\"}\n",
      "{\"name\":\"Lonely Boy\",\"artist\":\"The Black Keys\"}\n"
     ]
    }
   ],
   "source": [
    "response_gen = sllm.stream_chat(\n",
    "    [ChatMessage(role=\"user\", content=\"Name a random song!\")]\n",
    ")\n",
    "for r in response_gen:\n",
    "    print(r.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ae32e0",
   "metadata": {},
   "source": [
    "## Multi-Modal Support\n",
    "\n",
    "Ollama supports multi-modal models, and the Ollama LLM class natively supports images out of the box.\n",
    "\n",
    "This leverages the content blocks feature of the chat messages.\n",
    "\n",
    "Here, we leverage the `llama3.2-vision` model to answer a question about an image. If you don't have this model yet, you'll want to run `ollama pull llama3.2-vision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08764644",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://pbs.twimg.com/media/GVhGD1PXkAANfPV?format=jpg&name=4096x4096\" -O ollama_image.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82639e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: The animal in the image appears to be an alpaca, judging by its two distinct ears and long tail. It also has a white coat, which are all identifying characteristics of an alpaca. This alpaca appears to be wearing headphones with a microphone and dark glasses like what you would wear for gaming.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage, TextBlock, ImageBlock\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.2-vision\", request_timeout=120.0)\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        blocks=[\n",
    "            TextBlock(text=\"What type of animal is this?\"),\n",
    "            ImageBlock(path=\"ollama_image.jpg\"),\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "resp = llm.chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e1d7fd",
   "metadata": {},
   "source": [
    "Close enough ;) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
