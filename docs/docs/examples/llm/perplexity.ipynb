{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity\n",
    "\n",
    "Before we get started, make sure you install llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LLM\n",
    "\n",
    "As of Nov 14, 2023 - the following models are supported with the Perplexity LLM class in LLaMa Index:\n",
    "\n",
    "| Model | Context Length | Model Type |\n",
    "|-------|----------------|------------|\n",
    "| codellama-34b-instruct | 16384 | Chat Completion |\n",
    "| llama-2-13b-chat | 4096 | Chat Completion |\n",
    "| llama-2-70b-chat | 4096 | Chat Completion |\n",
    "| mistral-7b-instruct | 4096 [1] | Chat Completion |\n",
    "| openhermes-2-mistral-7b | 4096 [1] | Chat Completion |\n",
    "| openhermes-2.5-mistral-7b | 4096 [1] | Chat Completion |\n",
    "| replit-code-v1.5-3b | 4096 | Text Completion |\n",
    "| pplx-7b-chat-alpha | 4096 | Chat Completion |\n",
    "| pplx-70b-chat-alpha | 4096 | Chat Completion |\n",
    "\n",
    "[1] Context length of mistral-7b-instruct and openhermes-2-mistral-7b will be increased to 32k tokens (see perplexity roadmap).\n",
    "\n",
    "You can find the latest supported models here - https://docs.perplexity.ai/docs/model-cards \\\n",
    "Rate limits are found here - https://docs.perplexity.ai/docs/rate-limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.perplexity import Perplexity\n",
    "\n",
    "pplx_api_key = \"your-perplexity-api-key\"\n",
    "\n",
    "llm = Perplexity(\n",
    "    api_key=pplx_api_key, model=\"mistral-7b-instruct\", temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages_dict = [\n",
    "    {\"role\": \"system\", \"content\": \"Be precise and concise.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me 5 sentences about Perplexity.\"},\n",
    "]\n",
    "messages = [ChatMessage(**msg) for msg in messages_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: 1. Perplexity is the state of being puzzled or confused.\n",
      "2. It is a measure of how difficult it is to understand something.\n",
      "3. Perplexity can be caused by a lack of information or a mismatch between the information provided and what is being understood.\n",
      "4. It can also be caused by the complexity of a problem or the way it is presented.\n",
      "5. Perplexity can be reduced through further information, clarification, or simplification.\n"
     ]
    }
   ],
   "source": [
    "response = llm.chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: 1. Perplexity is a measure of how difficult it is to understand or solve a problem or concept.\n",
      "2. It is often used in fields such as cryptography, linguistics, and artificial intelligence.\n",
      "3. A high degree of perplexity indicates that a problem or concept is complex and difficult to understand.\n",
      "4. Perplexity can be calculated using various mathematical formulas, such as the entropy formula.\n",
      "5. Perplexity is an important concept in many areas of study, as it helps researchers to better understand and solve complex problems.\n"
     ]
    }
   ],
   "source": [
    "response = await llm.achat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Perplexity refers to the state of being confused or bewildered.\n",
      "2. It can be caused by a lack of understanding or a mismatch between one's expectations and reality.\n",
      "3. Perplexity can occur in various areas of life, such as personal relationships, work, or decision-making processes.\n",
      "4. It can lead to feelings of frustration or anxiety, and can be difficult to resolve.\n",
      "5. However, perplexity can also be a source of inspiration or creativity, as it can challenge one's assumptions and assumptions."
     ]
    }
   ],
   "source": [
    "resp = llm.stream_chat(messages)\n",
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Stream Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Perplexity refers to the state of being puzzled or confused.\n",
      "2. It is often associated with a lack of understanding or difficulty in comprehending something.\n",
      "3. Perplexity can be caused by a variety of factors, including complexity, ambiguity, or lack of information.\n",
      "4. It can manifest in different forms, such as confusion, uncertainty, or disorientation.\n",
      "5. Perplexity can be overcome through problem-solving, clarification, or seeking additional information."
     ]
    }
   ],
   "source": [
    "resp = await llm.astream_chat(messages)\n",
    "async for delta in resp:\n",
    "    print(delta.delta, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
