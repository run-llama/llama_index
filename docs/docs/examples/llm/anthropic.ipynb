{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6453d3d5",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/llm/anthropic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed6f61-28a7-4f90-8a45-e3f452f95dbd",
   "metadata": {},
   "source": [
    "# Anthropic\n",
    "\n",
    "Anthropic offers many state-of-the-art models from the haiku, sonnet, and opus families.\n",
    "\n",
    "Read on to learn how to use these models with LlamaIndex!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78b172f",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf8694-ad53-459a-84c1-64de2dadeaf5",
   "metadata": {},
   "source": [
    "#### Set Tokenizer\n",
    "\n",
    "First we want to set the tokenizer, which is slightly different than TikToken. This ensures that token counting is accurate throughout the library.\n",
    "\n",
    "**NOTE**: Anthropic recently updated their token counting API. Older models like claude-2.1 are no longer supported for token counting in the latest versions of the Anthropic python client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac37cb-b588-44c7-8fd9-8eab454900a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "from llama_index.core import Settings\n",
    "\n",
    "tokenizer = Anthropic().tokenizer\n",
    "Settings.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81a3ef6-2ee5-460d-9aa4-f73708774014",
   "metadata": {},
   "source": [
    "## Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fbba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-ant-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712ea8f4",
   "metadata": {},
   "source": [
    "You can call `complete` with a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b50ad-c55e-487e-8808-5905dfaa78b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "# To customize your API key, do this\n",
    "# otherwise it will lookup ANTHROPIC_API_KEY from your env variable\n",
    "# llm = Anthropic(api_key=\"<api_key>\")\n",
    "llm = Anthropic(model=\"claude-3-7-sonnet-latest\")\n",
    "\n",
    "resp = llm.complete(\"Who is Paul Graham?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfda925e-89c5-47a6-9311-16916ab08b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is a computer scientist, entrepreneur, venture capitalist, and essayist. He co-founded Viaweb (one of the first web application companies, later sold to Yahoo! and became Yahoo! Store), and later co-founded Y Combinator, an influential startup accelerator that has helped launch companies like Airbnb, Dropbox, Stripe, and Reddit. \n",
      "\n",
      "Graham is also well-known for his essays on technology, startups, and programming, which are published on his website. He created the Lisp dialect called Arc, and authored books including \"On Lisp,\" \"ANSI Common Lisp,\" and \"Hackers & Painters.\" He has a PhD in Computer Science from Harvard and studied painting at the Rhode Island School of Design and in Florence, Italy.\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c27c58",
   "metadata": {},
   "source": [
    "You can also call `chat` with a list of chat messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79dd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: # THE TREASURE OF CRIMSON COVE\n",
      "\n",
      "*Arrr, gather 'round, ye curious soul, for I be havin' a tale that'll chill yer very bones!*\n",
      "\n",
      "'Twas fifteen years ago when me and me crew aboard the Salty Vengeance caught wind of a treasure most rare - the Sapphire of Poseidon, said to control the very tides themselves! The map came to me hands after a particularly spirited game o' cards with a one-eyed merchant who'd had far too much rum.\n",
      "\n",
      "We set sail under the cover of a moonless night, navigatin' by stars alone to reach the dreaded Crimson Cove - a place where the water turns red as blood when the sun sets, on account of the strange coral beneath the waves.\n",
      "\n",
      "Three days into our journey, the skies turned black as pitch! A storm like none I'd ever seen! Waves tall as mountains threatened to swallow us whole! \"HOLD FAST, YE MANGY DOGS!\" I bellowed over the howlin' winds.\n",
      "\n",
      "When we finally reached the cove, half me crew was convinced the treasure was cursed. Bah! Superstitious bilge rats! But I'll not be lyin' to ye... when we found that hidden cave behind the waterfall, and saw them skeletons arranged in a circle 'round an empty chest... well, even ME beard seemed to tremble of its own accord!\n",
      "\n",
      "The real treasure weren't no sapphire at all, but a map to somethin' far greater... somethin' I still be searchin' for to this very day!\n",
      "\n",
      "*Leans in closer, voice dropping to a whisper*\n",
      "\n",
      "And perhaps, if ye prove yerself worthy, I might be persuaded to let ye join the hunt! HARR HARR HARR!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"Tell me a story\"),\n",
    "]\n",
    "llm = Anthropic(model=\"claude-3-7-sonnet-latest\")\n",
    "resp = llm.chat(messages)\n",
    "\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87da7e",
   "metadata": {},
   "source": [
    "## Streaming Support\n",
    "\n",
    "Every method supports streaming through the `stream_` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49a2681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is a computer scientist, entrepreneur, venture capitalist, and essayist. He's best known for:\n",
      "\n",
      "1. Co-founding Viaweb (later sold to Yahoo and became Yahoo Store)\n",
      "2. Creating the programming language Arc\n",
      "3. Co-founding Y Combinator, an influential startup accelerator that has funded companies like Airbnb, Dropbox, and Stripe\n",
      "4. Writing influential essays on startups, programming, and technology that are published on his website\n",
      "5. His work on Lisp programming language\n",
      "\n",
      "Graham is widely respected in the tech and startup communities for his insights on building companies and technology development."
     ]
    }
   ],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "llm = Anthropic(model=\"claude-3-7-sonnet-latest\")\n",
    "\n",
    "resp = llm.stream_complete(\"Who is Paul Graham?\")\n",
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffe6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is a computer scientist, entrepreneur, venture capitalist, and essayist. He's best known for:\n",
      "\n",
      "1. Co-founding Viaweb (later sold to Yahoo and became Yahoo Store)\n",
      "2. Creating the programming language Arc\n",
      "3. Co-founding Y Combinator, an influential startup accelerator that has funded companies like Airbnb, Dropbox, and Stripe\n",
      "4. Writing influential essays on startups, programming, and technology that are published on his website\n",
      "5. His work on Lisp programming language\n",
      "\n",
      "Graham is widely respected in the tech and startup communities for his insights on building companies and technology development."
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"Who is Paul Graham?\"),\n",
    "]\n",
    "\n",
    "resp = llm.stream_chat(messages)\n",
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0624c7bd",
   "metadata": {},
   "source": [
    "## Async Usage\n",
    "\n",
    "Every synchronous method has an async counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f72965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is a computer scientist, entrepreneur, venture capitalist, and essayist. He's best known for:\n",
      "\n",
      "1. Co-founding Viaweb (later sold to Yahoo and became Yahoo Store)\n",
      "2. Creating the programming language Arc\n",
      "3. Co-founding Y Combinator, an influential startup accelerator that has funded companies like Airbnb, Dropbox, Stripe, and Reddit\n",
      "4. Writing influential essays on startups, programming, and technology that are published on his website\n",
      "5. His work on Lisp programming language\n",
      "\n",
      "Graham is widely respected in the tech and startup communities for his insights on building companies and technology development."
     ]
    }
   ],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "llm = Anthropic(model=\"claude-3-7-sonnet-latest\")\n",
    "\n",
    "resp = await llm.astream_complete(\"Who is Paul Graham?\")\n",
    "async for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04cfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Paul Graham is a computer scientist, entrepreneur, venture capitalist, and essayist. He's best known for:\n",
      "\n",
      "1. Co-founding Viaweb (later sold to Yahoo and became Yahoo Store)\n",
      "2. Creating the programming language Arc\n",
      "3. Co-founding Y Combinator, an influential startup accelerator that has funded companies like Airbnb, Dropbox, Stripe, and Reddit\n",
      "4. Writing influential essays on startups, programming, and technology that are published on his website\n",
      "5. His work on Lisp programming language\n",
      "\n",
      "Graham is widely respected in the tech and startup communities for his insights on building companies and technology development.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"Who is Paul Graham?\"),\n",
    "]\n",
    "\n",
    "resp = await llm.achat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ccf92",
   "metadata": {},
   "source": [
    "## Vertex AI Support\n",
    "\n",
    "By providing the `region` and `project_id` parameters (either through environment variables or directly), you can use an Anthropic model through Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ANTHROPIC_PROJECT_ID\"] = \"YOUR PROJECT ID HERE\"\n",
    "os.environ[\"ANTHROPIC_REGION\"] = \"YOUR PROJECT REGION HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed545c13",
   "metadata": {},
   "source": [
    "Do keep in mind that setting region and project_id here will make Anthropic use the Vertex AI client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8b6f49",
   "metadata": {},
   "source": [
    "## Bedrock Support\n",
    "\n",
    "LlamaIndex also supports Anthropic models through AWS Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df734a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "# Note: this assumes you have standard AWS credentials configured in your environment\n",
    "llm = Anthropic(\n",
    "    model=\"anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    aws_region=\"us-east-1\",\n",
    ")\n",
    "\n",
    "resp = llm.complete(\"Who is Paul Graham?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a35c7e",
   "metadata": {},
   "source": [
    "## Multi-Modal Support\n",
    "\n",
    "Using `ChatMessage` objects, you can pass in images and text to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36874ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://cdn.pixabay.com/photo/2021/12/12/20/00/play-6865967_640.jpg -O image.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e2e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: The image shows four wooden dice arranged on a dark blue or black textured surface. The dice appear to be made of light-colored wood with black dots representing the numbers. Each die shows a different face value, with various combinations of dots visible. The dice have a natural wooden finish and the classic cubic shape with rounded edges that's typical of gaming dice. This type of dice would commonly be used for board games, tabletop games, or various games of chance.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage, TextBlock, ImageBlock\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "llm = Anthropic(model=\"claude-3-7-sonnet-latest\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        blocks=[\n",
    "            ImageBlock(path=\"image.jpg\"),\n",
    "            TextBlock(text=\"What is in this image?\"),\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "resp = llm.chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aed884",
   "metadata": {},
   "source": [
    "## Prompt Caching\n",
    "\n",
    "Anthropic models support the idea of prompt cahcing -- wherein if a prompt is repeated multiple times, or the start of a prompt is repeated, the LLM can reuse pre-calculated attention results to speed up the response and lower costs.\n",
    "\n",
    "To enable prompt caching, you can set `cache_control` on your `ChatMessage` objects, or set `cache_idx` on the LLM to always cache the first X messages (with -1 being all messages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1027338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "llm = Anthropic(model=\"claude-3-7-latest\")\n",
    "\n",
    "# cache individual message(s)\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        content=\"<some very long prompt>\",\n",
    "        additional_kwargs={\"cache_control\": {\"type\": \"ephemeral\"}},\n",
    "    ),\n",
    "]\n",
    "\n",
    "resp = llm.chat(messages)\n",
    "\n",
    "# cache first X messages (with -1 being all messages)\n",
    "llm = Anthropic(model=\"claude-3-7-latest\", cache_idx=-1)\n",
    "\n",
    "resp = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33bfa1f-589e-475a-8eb3-fa37d95759a7",
   "metadata": {},
   "source": [
    "## Structured Prediction\n",
    "\n",
    "LlamaIndex provides an intuitive interface for converting any Anthropic LLMs into a structured LLM through `structured_predict` - simply define the target Pydantic class (can be nested), and given a prompt, we extract out the desired object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b622d1-0c91-4cde-ab4c-3f83e0127a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.bridge.pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class MenuItem(BaseModel):\n",
    "    \"\"\"A menu item in a restaurant.\"\"\"\n",
    "\n",
    "    course_name: str\n",
    "    is_vegetarian: bool\n",
    "\n",
    "\n",
    "class Restaurant(BaseModel):\n",
    "    \"\"\"A restaurant with name, city, and cuisine.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    city: str\n",
    "    cuisine: str\n",
    "    menu_items: List[MenuItem]\n",
    "\n",
    "\n",
    "llm = Anthropic(\"claude-3-5-sonnet-20240620\")\n",
    "prompt_tmpl = PromptTemplate(\n",
    "    \"Generate a restaurant in a given city {city_name}\"\n",
    ")\n",
    "\n",
    "# Option 1: Use `as_structured_llm`\n",
    "restaurant_obj = (\n",
    "    llm.as_structured_llm(Restaurant)\n",
    "    .complete(prompt_tmpl.format(city_name=\"Miami\"))\n",
    "    .raw\n",
    ")\n",
    "# Option 2: Use `structured_predict`\n",
    "# restaurant_obj = llm.structured_predict(Restaurant, prompt_tmpl, city_name=\"Miami\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec769384-d3fe-4761-99e3-bdddcb9c7e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Restaurant(name='Ocean Breeze Bistro', city='Miami', cuisine='Seafood', menu_items=[MenuItem(course_name='Grilled Mahi-Mahi', is_vegetarian=False), MenuItem(course_name='Coconut Shrimp', is_vegetarian=False), MenuItem(course_name='Key Lime Pie', is_vegetarian=True), MenuItem(course_name='Vegetable Paella', is_vegetarian=True)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fb270-3d7d-4b9c-9cda-aaa42004c7dd",
   "metadata": {},
   "source": [
    "#### Structured Prediction with Streaming\n",
    "\n",
    "Any LLM wrapped with `as_structured_llm` supports streaming through `stream_chat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e91261-e4ef-4706-8325-65d782f9a87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'city': 'San Francisco',\n",
      " 'cuisine': 'California Fusion',\n",
      " 'menu_items': [{'course_name': 'Sourdough Avocado Toast',\n",
      "                 'is_vegetarian': True},\n",
      "                {'course_name': 'Dungeness Crab Cioppino',\n",
      "                 'is_vegetarian': False},\n",
      "                {'course_name': 'Mission-style Veggie Burrito',\n",
      "                 'is_vegetarian': True},\n",
      "                {'course_name': 'Grilled Napa Valley Lamb Chops',\n",
      "                 'is_vegetarian': False},\n",
      "                {'course_name': 'Vegan Ghirardelli Chocolate Mousse',\n",
      "                 'is_vegetarian': True}],\n",
      " 'name': 'Golden Gate Grill'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Restaurant(name='Golden Gate Grill', city='San Francisco', cuisine='California Fusion', menu_items=[MenuItem(course_name='Sourdough Avocado Toast', is_vegetarian=True), MenuItem(course_name='Dungeness Crab Cioppino', is_vegetarian=False), MenuItem(course_name='Mission-style Veggie Burrito', is_vegetarian=True), MenuItem(course_name='Grilled Napa Valley Lamb Chops', is_vegetarian=False), MenuItem(course_name='Vegan Ghirardelli Chocolate Mousse', is_vegetarian=True)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from IPython.display import clear_output\n",
    "from pprint import pprint\n",
    "\n",
    "input_msg = ChatMessage.from_str(\"Generate a restaurant in San Francisco\")\n",
    "\n",
    "sllm = llm.as_structured_llm(Restaurant)\n",
    "stream_output = sllm.stream_chat([input_msg])\n",
    "for partial_output in stream_output:\n",
    "    clear_output(wait=True)\n",
    "    pprint(partial_output.raw.dict())\n",
    "    restaurant_obj = partial_output.raw\n",
    "\n",
    "restaurant_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc778794",
   "metadata": {},
   "source": [
    "## Model Thinking\n",
    "\n",
    "With `claude-3.7 Sonnet`, you can enable the model to \"think\" harder about a task, generating a chain-of-thought response before writing out the final answer.\n",
    "\n",
    "You can enable this by passing in the `thinking_dict` parameter to the constructor, specififying the amount of tokens to reserve for the thinking process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "llm = Anthropic(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    # max_tokens must be greater than budget_tokens\n",
    "    max_tokens=64000,\n",
    "    # temperature must be 1.0 for thinking to work\n",
    "    temperature=1.0,\n",
    "    thinking_dict={\"type\": \"enabled\", \"budget_tokens\": 1600},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94018d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Evaluating (1234 * 3421) / (231 + 2341)\n",
      "\n",
      "I'll solve this step by step.\n",
      "\n",
      "## Step 1: Calculate the numerator (1234 * 3421)\n",
      "1234 * 3421 = 4,221,514\n",
      "\n",
      "## Step 2: Calculate the denominator (231 + 2341)\n",
      "231 + 2341 = 2,572\n",
      "\n",
      "## Step 3: Divide the numerator by the denominator\n",
      "4,221,514 Ã· 2,572 = 1,641.335...\n",
      "\n",
      "Therefore:\n",
      "(1234 * 3421) / (231 + 2341) = 1,641.335...\n",
      "\n",
      "The exact answer is 1,641 + 862/2,572, which can be simplified to 1,641.335...\n",
      "# Evaluating (1234 * 3421) / (231 + 2341)\n",
      "\n",
      "I'll solve this step by step.\n",
      "\n",
      "## Step 1: Calculate the numerator (1234 * 3421)\n",
      "1234 * 3421 = 4,221,514\n",
      "\n",
      "## Step 2: Calculate the denominator (231 + 2341)\n",
      "231 + 2341 = 2,572\n",
      "\n",
      "## Step 3: Divide the numerator by the denominator\n",
      "4,221,514 Ã· 2,572 = 1,641.335...\n",
      "\n",
      "Therefore:\n",
      "(1234 * 3421) / (231 + 2341) = 1,641.335...\n",
      "\n",
      "The exact answer is 1,641 + 862/2,572, which can be simplified to 1,641.335...\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"(1234 * 3421) / (231 + 2341) = ?\")\n",
    "]\n",
    "\n",
    "resp_gen = llm.stream_chat(messages)\n",
    "\n",
    "for r in resp_gen:\n",
    "    print(r.delta, end=\"\")\n",
    "\n",
    "print()\n",
    "print(r.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573bbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ErUBCkYIARgCIkA2LmXlUq2Lmkrlw4yPTpMD2I688kow8bnUjgP8DaEg0jXSgnTBjx0MWOJGpxQJA6Y3RVT/fGFm/X8ZDa7JXC0jEgybB8Sb5YUDH8RsEKcaDAFQAYIlE+97QPbA8yIwUaJV4/6oPFzx6PHC8ZZn8P05tcGdcR/Vp1z4mlLmjfaikz3mHzAOvQp1wunx0sa0Kh0TIbmx80VaWeU/RgFk0yIIZmkKXtCVI27VFVu8nw==\n"
     ]
    }
   ],
   "source": [
    "print(r.message.additional_kwargs[\"thinking\"][\"signature\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc35e06",
   "metadata": {},
   "source": [
    "We can also expose the exact thinking process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac32910e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need to calculate (1234 * 3421) / (231 + 2341)\n",
      "\n",
      "Let's start by calculating the numerator: 1234 * 3421\n",
      "1234 * 3421 = (1234 * 3000) + (1234 * 400) + (1234 * 20) + (1234 * 1)\n",
      "= 3702000 + 493600 + 24680 + 1234\n",
      "= 4221514\n",
      "\n",
      "Now let's calculate the denominator: 231 + 2341\n",
      "231 + 2341 = 2572\n",
      "\n",
      "Finally, let's calculate the division: 4221514 / 2572\n",
      "\n",
      "Actually, let me just double-check my calculation of 1234 * 3421.\n",
      "1234 * 3421 = 1234 * 3421\n",
      "\n",
      "Let me do this calculation differently.\n",
      "   1234\n",
      "Ã—  3421\n",
      "------\n",
      "   1234\n",
      "  24680\n",
      " 493600\n",
      "3702000\n",
      "------\n",
      "4221514\n",
      "\n",
      "So the numerator is 4221514.\n",
      "\n",
      "Now let's calculate the denominator: 231 + 2341 = 2572\n",
      "\n",
      "Finally, let's calculate the division: 4221514 / 2572\n",
      "\n",
      "4221514 / 2572 = ?\n",
      "\n",
      "Let me try long division.\n",
      "4221514 / 2572 = 1640.94...\n",
      "\n",
      "Actually, let me verify this with another approach.\n",
      "\n",
      "4221514 / 2572 \n",
      "â‰ˆ 4200000 / 2600 \n",
      "â‰ˆ 1615.38...\n",
      "\n",
      "That's not matching my earlier calculation. Let me try the division again.\n",
      "\n",
      "4221514 / 2572 \n",
      "\n",
      "2572 goes into 4221 about 1.64 times, which is about 1 time.\n",
      "4221 - 2572 = 1649\n",
      "Bring down the 5: 16495\n",
      "2572 goes into 16495 about 6.41 times, which is about 6 times.\n",
      "16495 - (6 * 2572) = 16495 - 15432 = 1063\n",
      "Bring down the 1: 10631\n",
      "2572 goes into 10631 about 4.13 times, which is about 4 times.\n",
      "10631 - (4 * 2572) = 10631 - 10288 = 343\n",
      "Bring down the 4: 3434\n",
      "2572 goes into 3434 about 1.33 times, which is about 1 time.\n",
      "3434 - 2572 = 862\n",
      "\n",
      "Actually, I'm going to try one more approach. I'll use polynomial long division.\n",
      "4221514 / 2572 = (4221514/2572)\n",
      "\n",
      "Let me calculate this directly.\n",
      "4221514 / 2572 = 1641.3351...\n",
      "\n",
      "Let me double-check this by multiplying: 1641.3351 * 2572 â‰ˆ 4221514? Let's see. That's approximately 1641 * 2572 = 4,220,652.\n",
      "\n",
      "That seems close enough (1641 * 2572 is a bit less than 4221514, which makes sense since 1641 is a bit less than 1641.3351).\n",
      "\n",
      "So our answer is 4221514 / 2572 = 1641.3351...\n"
     ]
    }
   ],
   "source": [
    "print(resp.message.additional_kwargs[\"thinking\"][\"thinking\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b0bbcb",
   "metadata": {},
   "source": [
    "## Tool/Function Calling\n",
    "\n",
    "Anthropic supports direct tool/function calling through the API. Using LlamaIndex, we can implement some core agentic tool calling patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f360774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "from datetime import datetime\n",
    "\n",
    "llm = Anthropic(model=\"claude-3-7-sonnet-latest\")\n",
    "\n",
    "\n",
    "def get_current_time() -> dict:\n",
    "    \"\"\"Get the current time\"\"\"\n",
    "    return {\"time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "\n",
    "# uses the tool name, any type annotations, and docstring to describe the tool\n",
    "tool = FunctionTool.from_defaults(fn=get_current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306cf697",
   "metadata": {},
   "source": [
    "We can simply do a single pass to call the tool and get the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31eb615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': '2025-03-06 12:36:25'}\n"
     ]
    }
   ],
   "source": [
    "resp = llm.predict_and_call([tool], \"What is the current time?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7718d",
   "metadata": {},
   "source": [
    "We can also use lower-level APIs to implement an agentic tool-calling loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cb6fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling get_current_time with {}\n",
      "Tool output:  {'time': '2025-03-06 12:43:36'}\n",
      "Final response:  The current time is 12:43:36 PM on March 6, 2025.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [ChatMessage(role=\"user\", content=\"What is the current time?\")]\n",
    "tools_by_name = {t.metadata.name: t for t in [tool]}\n",
    "\n",
    "resp = llm.chat_with_tools([tool], chat_history=chat_history)\n",
    "tool_calls = llm.get_tool_calls_from_response(\n",
    "    resp, error_on_no_tool_call=False\n",
    ")\n",
    "\n",
    "if not tool_calls:\n",
    "    print(resp)\n",
    "else:\n",
    "    while tool_calls:\n",
    "        # add the LLM's response to the chat history\n",
    "        chat_history.append(resp.message)\n",
    "\n",
    "        for tool_call in tool_calls:\n",
    "            tool_name = tool_call.tool_name\n",
    "            tool_kwargs = tool_call.tool_kwargs\n",
    "\n",
    "            print(f\"Calling {tool_name} with {tool_kwargs}\")\n",
    "            tool_output = tool.call(**tool_kwargs)\n",
    "            print(\"Tool output: \", tool_output)\n",
    "            chat_history.append(\n",
    "                ChatMessage(\n",
    "                    role=\"tool\",\n",
    "                    content=str(tool_output),\n",
    "                    # most LLMs like Anthropic, OpenAI, etc. need to know the tool call id\n",
    "                    additional_kwargs={\"tool_call_id\": tool_call.tool_id},\n",
    "                )\n",
    "            )\n",
    "\n",
    "            resp = llm.chat_with_tools([tool], chat_history=chat_history)\n",
    "            tool_calls = llm.get_tool_calls_from_response(\n",
    "                resp, error_on_no_tool_call=False\n",
    "            )\n",
    "    print(\"Final response: \", resp.message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index-nfHyf6Wh-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
