{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6453d3d5",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/llm/anthropic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed6f61-28a7-4f90-8a45-e3f452f95dbd",
   "metadata": {},
   "source": [
    "# Anthropic\n",
    "\n",
    "Anthropic offers many state-of-the-art models from the haiku, sonnet, and opus families.\n",
    "\n",
    "Read on to learn how to use these models with LlamaIndex!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78b172f",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ü¶ô."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf8694-ad53-459a-84c1-64de2dadeaf5",
   "metadata": {},
   "source": [
    "#### Set Tokenizer\n",
    "\n",
    "First we want to set the tokenizer, which is slightly different than TikToken. This ensures that token counting is accurate throughout the library.\n",
    "\n",
    "**NOTE**: Anthropic recently updated their token counting API. Older models like claude-2.1 are no longer supported for token counting in the latest versions of the Anthropic python client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac37cb-b588-44c7-8fd9-8eab454900a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "from llama_index.core import Settings\n",
    "\n",
    "tokenizer = Anthropic().tokenizer\n",
    "Settings.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81a3ef6-2ee5-460d-9aa4-f73708774014",
   "metadata": {},
   "source": [
    "## Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fbba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-ant-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712ea8f4",
   "metadata": {},
   "source": [
    "You can call `complete` with a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b50ad-c55e-487e-8808-5905dfaa78b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "# To customize your API key, do this\n",
    "# otherwise it will lookup ANTHROPIC_API_KEY from your env variable\n",
    "# llm = Anthropic(api_key=\"<api_key>\")\n",
    "llm = Anthropic(model=\"claude-3-7-sonnet-latest\")\n",
    "\n",
    "resp = llm.complete(\"Who is Paul Graham?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfda925e-89c5-47a6-9311-16916ab08b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is a computer scientist, entrepreneur, venture capitalist, and essayist. He co-founded Viaweb (one of the first web application companies, later sold to Yahoo! and became Yahoo! Store), and later co-founded Y Combinator, an influential startup accelerator that has helped launch companies like Airbnb, Dropbox, Stripe, and Reddit. \n",
      "\n",
      "Graham is also well-known for his essays on technology, startups, and programming, which are published on his website. He created the Lisp dialect called Arc, and authored books including \"On Lisp,\" \"ANSI Common Lisp,\" and \"Hackers & Painters.\" He has a PhD in Computer Science from Harvard and studied painting at the Rhode Island School of Design and in Florence, Italy.\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c27c58",
   "metadata": {},
   "source": [
    "You can also call `chat` with a list of chat messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79dd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: # THE TREASURE OF CRIMSON COVE\n",
      "\n",
      "*Arrr, gather 'round, ye curious soul, for I be havin' a tale that'll chill yer very bones!*\n",
      "\n",
      "'Twas fifteen years ago when me and me crew aboard the Salty Vengeance caught wind of a treasure most rare - the Sapphire of Poseidon, said to control the very tides themselves! The map came to me hands after a particularly spirited game o' cards with a one-eyed merchant who'd had far too much rum.\n",
      "\n",
      "We set sail under the cover of a moonless night, navigatin' by stars alone to reach the dreaded Crimson Cove - a place where the water turns red as blood when the sun sets, on account of the strange coral beneath the waves.\n",
      "\n",
      "Three days into our journey, the skies turned black as pitch! A storm like none I'd ever seen! Waves tall as mountains threatened to swallow us whole! \"HOLD FAST, YE MANGY DOGS!\" I bellowed over the howlin' winds.\n",
      "\n",
      "When we finally reached the cove, half me crew was convinced the treasure was cursed. Bah! Superstitious bilge rats! But I'll not be lyin' to ye... when we found that hidden cave behind the waterfall, and saw them skeletons arranged in a circle 'round an empty chest... well, even ME beard seemed to tremble of its own accord!\n",
      "\n",
      "The real treasure weren't no sapphire at all, but a map to somethin' far greater... somethin' I still be searchin' for to this very day!\n",
      "\n",
      "*Leans in closer, voice dropping to a whisper*\n",
      "\n",
      "And perhaps, if ye prove yerself worthy, I might be persuaded to let ye join the hunt! HARR HARR HARR!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"Tell me a story\"),\n",
    "]\n",
    "llm = Anthropic(model=\"claude-3-7-sonnet-latest\")\n",
    "resp = llm.chat(messages)\n",
    "\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87da7e",
   "metadata": {},
   "source": [
    "## Streaming Support\n",
    "\n",
    "Every method supports streaming through the `stream_` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49a2681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is a computer scientist, entrepreneur, venture capitalist, and essayist. He's best known for:\n",
      "\n",
      "1. Co-founding Viaweb (later sold to Yahoo and became Yahoo Store)\n",
      "2. Creating the programming language Arc\n",
      "3. Co-founding Y Combinator, an influential startup accelerator that has funded companies like Airbnb, Dropbox, and Stripe\n",
      "4. Writing influential essays on startups, programming, and technology that are published on his website\n",
      "5. His work on Lisp programming language\n",
      "\n",
      "Graham is widely respected in the tech and startup communities for his insights on building companies and technology development."
     ]
    }
   ],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "llm = Anthropic(model=\"claude-3-7-sonnet-latest\")\n",
    "\n",
    "resp = llm.stream_complete(\"Who is Paul Graham?\")\n",
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffe6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is a computer scientist, entrepreneur, venture capitalist, and essayist. He's best known for:\n",
      "\n",
      "1. Co-founding Viaweb (later sold to Yahoo and became Yahoo Store)\n",
      "2. Creating the programming language Arc\n",
      "3. Co-founding Y Combinator, an influential startup accelerator that has funded companies like Airbnb, Dropbox, and Stripe\n",
      "4. Writing influential essays on startups, programming, and technology that are published on his website\n",
      "5. His work on Lisp programming language\n",
      "\n",
      "Graham is widely respected in the tech and startup communities for his insights on building companies and technology development."
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"Who is Paul Graham?\"),\n",
    "]\n",
    "\n",
    "resp = llm.stream_chat(messages)\n",
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0624c7bd",
   "metadata": {},
   "source": [
    "## Async Usage\n",
    "\n",
    "Every synchronous method has an async counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f72965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is a computer scientist, entrepreneur, venture capitalist, and essayist. He's best known for:\n",
      "\n",
      "1. Co-founding Viaweb (later sold to Yahoo and became Yahoo Store)\n",
      "2. Creating the programming language Arc\n",
      "3. Co-founding Y Combinator, an influential startup accelerator that has funded companies like Airbnb, Dropbox, Stripe, and Reddit\n",
      "4. Writing influential essays on startups, programming, and technology that are published on his website\n",
      "5. His work on Lisp programming language\n",
      "\n",
      "Graham is widely respected in the tech and startup communities for his insights on building companies and technology development."
     ]
    }
   ],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "llm = Anthropic(model=\"claude-3-7-sonnet-latest\")\n",
    "\n",
    "resp = await llm.astream_complete(\"Who is Paul Graham?\")\n",
    "async for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04cfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Paul Graham is a computer scientist, entrepreneur, venture capitalist, and essayist. He's best known for:\n",
      "\n",
      "1. Co-founding Viaweb (later sold to Yahoo and became Yahoo Store)\n",
      "2. Creating the programming language Arc\n",
      "3. Co-founding Y Combinator, an influential startup accelerator that has funded companies like Airbnb, Dropbox, Stripe, and Reddit\n",
      "4. Writing influential essays on startups, programming, and technology that are published on his website\n",
      "5. His work on Lisp programming language\n",
      "\n",
      "Graham is widely respected in the tech and startup communities for his insights on building companies and technology development.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"Who is Paul Graham?\"),\n",
    "]\n",
    "\n",
    "resp = await llm.achat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ccf92",
   "metadata": {},
   "source": [
    "## Vertex AI Support\n",
    "\n",
    "By providing the `region` and `project_id` parameters (either through environment variables or directly), you can use an Anthropic model through Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ANTHROPIC_PROJECT_ID\"] = \"YOUR PROJECT ID HERE\"\n",
    "os.environ[\"ANTHROPIC_REGION\"] = \"YOUR PROJECT REGION HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed545c13",
   "metadata": {},
   "source": [
    "Do keep in mind that setting region and project_id here will make Anthropic use the Vertex AI client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8b6f49",
   "metadata": {},
   "source": [
    "## Bedrock Support\n",
    "\n",
    "LlamaIndex also supports Anthropic models through AWS Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df734a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "# Note: this assumes you have standard AWS credentials configured in your environment\n",
    "llm = Anthropic(\n",
    "    model=\"anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    aws_region=\"us-east-1\",\n",
    ")\n",
    "\n",
    "resp = llm.complete(\"Who is Paul Graham?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a35c7e",
   "metadata": {},
   "source": [
    "## Multi-Modal Support\n",
    "\n",
    "Using `ChatMessage` objects, you can pass in images and text to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36874ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://cdn.pixabay.com/photo/2021/12/12/20/00/play-6865967_640.jpg -O image.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e2e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: The image shows four wooden dice arranged on a dark blue or black textured surface. The dice appear to be made of light-colored wood with black dots representing the numbers. Each die shows a different face value, with various combinations of dots visible. The dice have a natural wooden finish and the classic cubic shape with rounded edges that's typical of gaming dice. This type of dice would commonly be used for board games, tabletop games, or various games of chance.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage, TextBlock, ImageBlock\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "llm = Anthropic(model=\"claude-3-7-sonnet-latest\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        blocks=[\n",
    "            ImageBlock(path=\"image.jpg\"),\n",
    "            TextBlock(text=\"What is in this image?\"),\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "resp = llm.chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aed884",
   "metadata": {},
   "source": [
    "## Prompt Caching\n",
    "\n",
    "Anthropic models support the idea of prompt cahcing -- wherein if a prompt is repeated multiple times, or the start of a prompt is repeated, the LLM can reuse pre-calculated attention results to speed up the response and lower costs.\n",
    "\n",
    "To enable prompt caching, you can set `cache_control` on your `ChatMessage` objects, or set `cache_idx` on the LLM to always cache the first X messages (with -1 being all messages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1027338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "llm = Anthropic(model=\"claude-3-7-latest\")\n",
    "\n",
    "# cache individual message(s)\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        content=\"<some very long prompt>\",\n",
    "        additional_kwargs={\"cache_control\": {\"type\": \"ephemeral\"}},\n",
    "    ),\n",
    "]\n",
    "\n",
    "resp = llm.chat(messages)\n",
    "\n",
    "# cache first X messages (with -1 being all messages)\n",
    "llm = Anthropic(model=\"claude-3-7-latest\", cache_idx=-1)\n",
    "\n",
    "resp = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33bfa1f-589e-475a-8eb3-fa37d95759a7",
   "metadata": {},
   "source": [
    "## Structured Prediction\n",
    "\n",
    "LlamaIndex provides an intuitive interface for converting any Anthropic LLMs into a structured LLM through `structured_predict` - simply define the target Pydantic class (can be nested), and given a prompt, we extract out the desired object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b622d1-0c91-4cde-ab4c-3f83e0127a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.bridge.pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class MenuItem(BaseModel):\n",
    "    \"\"\"A menu item in a restaurant.\"\"\"\n",
    "\n",
    "    course_name: str\n",
    "    is_vegetarian: bool\n",
    "\n",
    "\n",
    "class Restaurant(BaseModel):\n",
    "    \"\"\"A restaurant with name, city, and cuisine.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    city: str\n",
    "    cuisine: str\n",
    "    menu_items: List[MenuItem]\n",
    "\n",
    "\n",
    "llm = Anthropic(\"claude-3-5-sonnet-20240620\")\n",
    "prompt_tmpl = PromptTemplate(\n",
    "    \"Generate a restaurant in a given city {city_name}\"\n",
    ")\n",
    "\n",
    "# Option 1: Use `as_structured_llm`\n",
    "restaurant_obj = (\n",
    "    llm.as_structured_llm(Restaurant)\n",
    "    .complete(prompt_tmpl.format(city_name=\"Miami\"))\n",
    "    .raw\n",
    ")\n",
    "# Option 2: Use `structured_predict`\n",
    "# restaurant_obj = llm.structured_predict(Restaurant, prompt_tmpl, city_name=\"Miami\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec769384-d3fe-4761-99e3-bdddcb9c7e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Restaurant(name='Ocean Breeze Bistro', city='Miami', cuisine='Seafood', menu_items=[MenuItem(course_name='Grilled Mahi-Mahi', is_vegetarian=False), MenuItem(course_name='Coconut Shrimp', is_vegetarian=False), MenuItem(course_name='Key Lime Pie', is_vegetarian=True), MenuItem(course_name='Vegetable Paella', is_vegetarian=True)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fb270-3d7d-4b9c-9cda-aaa42004c7dd",
   "metadata": {},
   "source": [
    "#### Structured Prediction with Streaming\n",
    "\n",
    "Any LLM wrapped with `as_structured_llm` supports streaming through `stream_chat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e91261-e4ef-4706-8325-65d782f9a87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'city': 'San Francisco',\n",
      " 'cuisine': 'California Fusion',\n",
      " 'menu_items': [{'course_name': 'Sourdough Avocado Toast',\n",
      "                 'is_vegetarian': True},\n",
      "                {'course_name': 'Dungeness Crab Cioppino',\n",
      "                 'is_vegetarian': False},\n",
      "                {'course_name': 'Mission-style Veggie Burrito',\n",
      "                 'is_vegetarian': True},\n",
      "                {'course_name': 'Grilled Napa Valley Lamb Chops',\n",
      "                 'is_vegetarian': False},\n",
      "                {'course_name': 'Vegan Ghirardelli Chocolate Mousse',\n",
      "                 'is_vegetarian': True}],\n",
      " 'name': 'Golden Gate Grill'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Restaurant(name='Golden Gate Grill', city='San Francisco', cuisine='California Fusion', menu_items=[MenuItem(course_name='Sourdough Avocado Toast', is_vegetarian=True), MenuItem(course_name='Dungeness Crab Cioppino', is_vegetarian=False), MenuItem(course_name='Mission-style Veggie Burrito', is_vegetarian=True), MenuItem(course_name='Grilled Napa Valley Lamb Chops', is_vegetarian=False), MenuItem(course_name='Vegan Ghirardelli Chocolate Mousse', is_vegetarian=True)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from IPython.display import clear_output\n",
    "from pprint import pprint\n",
    "\n",
    "input_msg = ChatMessage.from_str(\"Generate a restaurant in San Francisco\")\n",
    "\n",
    "sllm = llm.as_structured_llm(Restaurant)\n",
    "stream_output = sllm.stream_chat([input_msg])\n",
    "for partial_output in stream_output:\n",
    "    clear_output(wait=True)\n",
    "    pprint(partial_output.raw.dict())\n",
    "    restaurant_obj = partial_output.raw\n",
    "\n",
    "restaurant_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc778794",
   "metadata": {},
   "source": [
    "## Model Thinking\n",
    "\n",
    "With `claude-3.7 Sonnet`, you can enable the model to \"think\" harder about a task, generating a chain-of-thought response before writing out the final answer.\n",
    "\n",
    "You can enable this by passing in the `thinking_dict` parameter to the constructor, specififying the amount of tokens to reserve for the thinking process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "llm = Anthropic(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    # max_tokens must be greater than budget_tokens\n",
    "    max_tokens=64000,\n",
    "    # temperature must be 1.0 for thinking to work\n",
    "    temperature=1.0,\n",
    "    thinking_dict={\"type\": \"enabled\", \"budget_tokens\": 1600},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94018d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Evaluating (1234 * 3421) / (231 + 2341)\n",
      "\n",
      "I'll solve this step by step.\n",
      "\n",
      "## Step 1: Calculate the numerator (1234 * 3421)\n",
      "1234 * 3421 = 4,221,514\n",
      "\n",
      "## Step 2: Calculate the denominator (231 + 2341)\n",
      "231 + 2341 = 2,572\n",
      "\n",
      "## Step 3: Divide the numerator by the denominator\n",
      "4,221,514 √∑ 2,572 = 1,641.335...\n",
      "\n",
      "Therefore:\n",
      "(1234 * 3421) / (231 + 2341) = 1,641.335...\n",
      "\n",
      "The exact answer is 1,641 + 862/2,572, which can be simplified to 1,641.335...\n",
      "# Evaluating (1234 * 3421) / (231 + 2341)\n",
      "\n",
      "I'll solve this step by step.\n",
      "\n",
      "## Step 1: Calculate the numerator (1234 * 3421)\n",
      "1234 * 3421 = 4,221,514\n",
      "\n",
      "## Step 2: Calculate the denominator (231 + 2341)\n",
      "231 + 2341 = 2,572\n",
      "\n",
      "## Step 3: Divide the numerator by the denominator\n",
      "4,221,514 √∑ 2,572 = 1,641.335...\n",
      "\n",
      "Therefore:\n",
      "(1234 * 3421) / (231 + 2341) = 1,641.335...\n",
      "\n",
      "The exact answer is 1,641 + 862/2,572, which can be simplified to 1,641.335...\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"(1234 * 3421) / (231 + 2341) = ?\")\n",
    "]\n",
    "\n",
    "resp_gen = llm.stream_chat(messages)\n",
    "\n",
    "for r in resp_gen:\n",
    "    print(r.delta, end=\"\")\n",
    "\n",
    "print()\n",
    "print(r.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573bbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ErUBCkYIARgCIkA2LmXlUq2Lmkrlw4yPTpMD2I688kow8bnUjgP8DaEg0jXSgnTBjx0MWOJGpxQJA6Y3RVT/fGFm/X8ZDa7JXC0jEgybB8Sb5YUDH8RsEKcaDAFQAYIlE+97QPbA8yIwUaJV4/6oPFzx6PHC8ZZn8P05tcGdcR/Vp1z4mlLmjfaikz3mHzAOvQp1wunx0sa0Kh0TIbmx80VaWeU/RgFk0yIIZmkKXtCVI27VFVu8nw==\n"
     ]
    }
   ],
   "source": [
    "print(r.message.additional_kwargs[\"thinking\"][\"signature\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc35e06",
   "metadata": {},
   "source": [
    "We can also expose the exact thinking process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac32910e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need to calculate (1234 * 3421) / (231 + 2341)\n",
      "\n",
      "Let's start by calculating the numerator: 1234 * 3421\n",
      "1234 * 3421 = (1234 * 3000) + (1234 * 400) + (1234 * 20) + (1234 * 1)\n",
      "= 3702000 + 493600 + 24680 + 1234\n",
      "= 4221514\n",
      "\n",
      "Now let's calculate the denominator: 231 + 2341\n",
      "231 + 2341 = 2572\n",
      "\n",
      "Finally, let's calculate the division: 4221514 / 2572\n",
      "\n",
      "Actually, let me just double-check my calculation of 1234 * 3421.\n",
      "1234 * 3421 = 1234 * 3421\n",
      "\n",
      "Let me do this calculation differently.\n",
      "   1234\n",
      "√ó  3421\n",
      "------\n",
      "   1234\n",
      "  24680\n",
      " 493600\n",
      "3702000\n",
      "------\n",
      "4221514\n",
      "\n",
      "So the numerator is 4221514.\n",
      "\n",
      "Now let's calculate the denominator: 231 + 2341 = 2572\n",
      "\n",
      "Finally, let's calculate the division: 4221514 / 2572\n",
      "\n",
      "4221514 / 2572 = ?\n",
      "\n",
      "Let me try long division.\n",
      "4221514 / 2572 = 1640.94...\n",
      "\n",
      "Actually, let me verify this with another approach.\n",
      "\n",
      "4221514 / 2572 \n",
      "‚âà 4200000 / 2600 \n",
      "‚âà 1615.38...\n",
      "\n",
      "That's not matching my earlier calculation. Let me try the division again.\n",
      "\n",
      "4221514 / 2572 \n",
      "\n",
      "2572 goes into 4221 about 1.64 times, which is about 1 time.\n",
      "4221 - 2572 = 1649\n",
      "Bring down the 5: 16495\n",
      "2572 goes into 16495 about 6.41 times, which is about 6 times.\n",
      "16495 - (6 * 2572) = 16495 - 15432 = 1063\n",
      "Bring down the 1: 10631\n",
      "2572 goes into 10631 about 4.13 times, which is about 4 times.\n",
      "10631 - (4 * 2572) = 10631 - 10288 = 343\n",
      "Bring down the 4: 3434\n",
      "2572 goes into 3434 about 1.33 times, which is about 1 time.\n",
      "3434 - 2572 = 862\n",
      "\n",
      "Actually, I'm going to try one more approach. I'll use polynomial long division.\n",
      "4221514 / 2572 = (4221514/2572)\n",
      "\n",
      "Let me calculate this directly.\n",
      "4221514 / 2572 = 1641.3351...\n",
      "\n",
      "Let me double-check this by multiplying: 1641.3351 * 2572 ‚âà 4221514? Let's see. That's approximately 1641 * 2572 = 4,220,652.\n",
      "\n",
      "That seems close enough (1641 * 2572 is a bit less than 4221514, which makes sense since 1641 is a bit less than 1641.3351).\n",
      "\n",
      "So our answer is 4221514 / 2572 = 1641.3351...\n"
     ]
    }
   ],
   "source": [
    "print(resp.message.additional_kwargs[\"thinking\"][\"thinking\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b0bbcb",
   "metadata": {},
   "source": [
    "## Tool/Function Calling\n",
    "\n",
    "Anthropic supports direct tool/function calling through the API. Using LlamaIndex, we can implement some core agentic tool calling patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f360774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "from datetime import datetime\n",
    "\n",
    "llm = Anthropic(model=\"claude-3-7-sonnet-latest\")\n",
    "\n",
    "\n",
    "def get_current_time() -> dict:\n",
    "    \"\"\"Get the current time\"\"\"\n",
    "    return {\"time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "\n",
    "# uses the tool name, any type annotations, and docstring to describe the tool\n",
    "tool = FunctionTool.from_defaults(fn=get_current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306cf697",
   "metadata": {},
   "source": [
    "We can simply do a single pass to call the tool and get the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31eb615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': '2025-03-06 12:36:25'}\n"
     ]
    }
   ],
   "source": [
    "resp = llm.predict_and_call([tool], \"What is the current time?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7718d",
   "metadata": {},
   "source": [
    "We can also use lower-level APIs to implement an agentic tool-calling loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cb6fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling get_current_time with {}\n",
      "Tool output:  {'time': '2025-03-06 12:43:36'}\n",
      "Final response:  The current time is 12:43:36 PM on March 6, 2025.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [ChatMessage(role=\"user\", content=\"What is the current time?\")]\n",
    "tools_by_name = {t.metadata.name: t for t in [tool]}\n",
    "\n",
    "resp = llm.chat_with_tools([tool], chat_history=chat_history)\n",
    "tool_calls = llm.get_tool_calls_from_response(\n",
    "    resp, error_on_no_tool_call=False\n",
    ")\n",
    "\n",
    "if not tool_calls:\n",
    "    print(resp)\n",
    "else:\n",
    "    while tool_calls:\n",
    "        # add the LLM's response to the chat history\n",
    "        chat_history.append(resp.message)\n",
    "\n",
    "        for tool_call in tool_calls:\n",
    "            tool_name = tool_call.tool_name\n",
    "            tool_kwargs = tool_call.tool_kwargs\n",
    "\n",
    "            print(f\"Calling {tool_name} with {tool_kwargs}\")\n",
    "            tool_output = tool.call(**tool_kwargs)\n",
    "            print(\"Tool output: \", tool_output)\n",
    "            chat_history.append(\n",
    "                ChatMessage(\n",
    "                    role=\"tool\",\n",
    "                    content=str(tool_output),\n",
    "                    # most LLMs like Anthropic, OpenAI, etc. need to know the tool call id\n",
    "                    additional_kwargs={\"tool_call_id\": tool_call.tool_id},\n",
    "                )\n",
    "            )\n",
    "\n",
    "            resp = llm.chat_with_tools([tool], chat_history=chat_history)\n",
    "            tool_calls = llm.get_tool_calls_from_response(\n",
    "                resp, error_on_no_tool_call=False\n",
    "            )\n",
    "    print(\"Final response: \", resp.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e72094",
   "metadata": {},
   "source": [
    "## Server-Side Tool Calling\n",
    "\n",
    "Anthropic now also supports server-side tool calling in latest versions. \n",
    "\n",
    "Here's an example of how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2b257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll help you explore the latest AI research trends. Let me search for the most current information.\n",
      "\n",
      "Based on the search results, I can provide you with an overview of the latest AI research trends for 2025. Here are the key developments shaping the AI landscape:\n",
      "\n",
      "## 1. Agentic AI\n",
      "\n",
      "Autonomous agents are becoming a dominant trend in AI research. These are AI systems that can perform tasks independently without direct human involvement. While such systems have existed for some time, the development of Large Language Models (LLMs) with strong reasoning capabilities has accelerated autonomous agent research exponentially in recent years.\n",
      "\n",
      "By 2025, AI agents will have advanced capabilities, such as conversing with customers and planning subsequent actions‚Äîfor example, processing payments, checking for fraud, and completing shipping actions. Software companies are already embedding agentic AI capabilities into their core products.\n",
      "\n",
      "In 2025, AI will evolve from being merely a tool for work and home to becoming an integral part of both environments. AI-powered agents will operate with greater autonomy to help simplify life both at home and on the job. On the global stage, AI will help address major challenges, from climate change to healthcare access.\n",
      "\n",
      "## 2. AI Reasoning and Frontier Models\n",
      "\n",
      "Among the top trends in AI are advancements in AI reasoning and frontier models. The world's largest tech companies are competing to refine cutting-edge AI applications, including large language models' ability to reason like humans and frontier models that push boundaries in natural-language processing, image generation, and coding.\n",
      "\n",
      "Over the past year, AI models have become faster and more efficient. Today's large-scale \"frontier models\" can complete a broad range of tasks from writing to coding, while specialized models can be tailored for specific tasks or industries. In 2025, these models will do more and do it better. Models with advanced reasoning capabilities, like OpenAI o1, can already solve complex problems using logical steps similar to human thinking. These capabilities will continue to be valuable in fields such as science, coding, math, law, and medicine.\n",
      "\n",
      "## 3. Multimodal AI\n",
      "\n",
      "Multimodal AI, which integrates and processes information from multiple data sources like text, images, and audio, is expected to grow significantly. By 2025, this technology will allow systems to understand and respond to complex inputs more naturally, creating a seamless experience for users. For example, customer support chatbots will soon be able to handle both text and images.\n",
      "\n",
      "Contemporary autonomous agents often make use of generative AI in a central role, but there will be many more generative AI iterations and applications to come, including multimodal generative AI. Multimodal AI models process and generate various data types instead of focusing exclusively on just one‚Äîexamples include text-to-image and image-to-audio conversions. Advances in this technology will help systems interpret and generate content across different modalities, leading to applications in healthcare for diagnosis enhancement, autonomous vehicles, more robust content generation, and many other exciting uses. The rise of multimodal generative AI will be a prime catalyst of the continued AI industrial revolution in 2025.\n",
      "\n",
      "## 4. AI in Scientific Research\n",
      "\n",
      "One of the most exciting developments to watch in 2025 will be how AI's use in scientific research fuels progress in addressing some of the world's most pressing concerns. According to Ashley Llorens, corporate vice president at Microsoft Research, \"We'll start to see these tools having a measurable impact on the throughput of the people and institutions who are working on these huge problems, such as designing sustainable materials and accelerating development of life-saving drugs.\"\n",
      "\n",
      "This trend is expected to continue in 2025, with more data sets and models specifically aimed at scientific\n",
      "Source: https://machinelearningmastery.com/7-machine-learning-trends-2025/ - If you have been paying attention to the latest machine learning buzz terminology, you know that the autonomous agent, and discussion of them, is ever...\n",
      "Source: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/superagency-in-the-workplace-empowering-people-to-unlock-ais-full-potential-at-work - In 2025, an AI agent can converse with a customer and plan the actions it will take afterward‚Äîfor example, processing a payment, checking for fraud, a...\n",
      "Source: https://news.microsoft.com/source/features/ai/6-ai-trends-youll-see-more-of-in-2025/ - In 2025, AI will evolve from a tool for work and home to an integral part of both.  ¬∑ AI-powered agents will do more with greater autonomy and help si...\n",
      "Source: https://www.morganstanley.com/insights/articles/ai-trends-reasoning-frontier-models-2025-tmt - The top trends in new AI frontiers and the focus on enterprises include AI reasoning, custom silicon, cloud migrations, systems to measure AI efficacy...\n",
      "Source: https://www.morganstanley.com/insights/articles/ai-trends-reasoning-frontier-models-2025-tmt - The top trends in new AI frontiers and the focus on enterprises include AI reasoning, custom silicon, cloud migrations, systems to measure AI efficacy...\n",
      "Source: https://news.microsoft.com/source/features/ai/6-ai-trends-youll-see-more-of-in-2025/ - ¬∑ Over the past year, AI models became faster and more efficient. Today, large-scale ‚Äúfrontier models‚Äù can complete a broad range of tasks from writin...\n",
      "Source: https://www.hdwebsoft.com/blog/top-10-ai-and-machine-learning-trends-for-2025.html - Multimodal AI, which integrates and processes information from multiple data sources like text, images, and audio, will grow significantly. By 2025, t...\n",
      "Source: https://machinelearningmastery.com/7-machine-learning-trends-2025/ - Contemporary autonomous agents, mentioned above, make use of generative AI, often in a central role, but there will be many more generative AI iterati...\n",
      "Source: https://machinelearningmastery.com/7-machine-learning-trends-2025/ - Advances in multimodal AI technology will help systems interpret and generate content across different modalities, leading to many interesting applica...\n",
      "Source: https://news.microsoft.com/source/features/ai/6-ai-trends-youll-see-more-of-in-2025/ - One of the most exciting things to watch in 2025 will be how AI‚Äôs use in scientific research fuels progress in addressing some of the world‚Äôs most pre...\n",
      "Source: https://www.technologyreview.com/2025/01/08/1109188/whats-next-for-ai-in-2025/ - Expect this trend to continue next year, and to see more data sets and models that are aimed specifically at scientific discovery. Proteins were the p...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "llm = Anthropic(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=1024,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"web_search_20250305\",\n",
    "            \"name\": \"web_search\",\n",
    "            \"max_uses\": 3,  # Limit to 3 searches\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Get response with citations\n",
    "response = llm.complete(\"What are the latest AI research trends?\")\n",
    "\n",
    "# Access the main response content\n",
    "print(response.text)\n",
    "\n",
    "# Access citations if available\n",
    "for citation in response.citations:\n",
    "    print(f\"Source: {citation.get('url')} - {citation.get('cited_text')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index-nfHyf6Wh-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
