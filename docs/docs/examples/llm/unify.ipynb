{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify\n",
    "\n",
    "[Unify](https://unify.ai) is your centralized platform for LLM endpoints, enabling you to route your queries to the best LLM endpoints, benchmark performance, and seamlessly switch providers with a single API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's install LlamaIndex ü¶ô and the Unify integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-unify llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Make sure to set the `UNIFY_API_KEY` environment variable. You can get a key from the [Unify Console](https://console.unify.ai/login)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"UNIFY_API_KEY\"] = \"<YOUR API KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LlamaIndex with Unify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Usage \n",
    "\n",
    "Below we initialize and query a chat model using the `llama-3-70b-chat` endpoint from `together-ai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text=\"I'm not actually a llama, but I'm doing great, thanks for asking! I'm a large language model, so I don't have feelings like humans do, but I'm always happy to chat with you and help with any questions or topics you'd like to discuss. How about you? How's your day going?\", additional_kwargs={}, raw={'id': '88b5fcf02e259527-LHR', 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm not actually a llama, but I'm doing great, thanks for asking! I'm a large language model, so I don't have feelings like humans do, but I'm always happy to chat with you and help with any questions or topics you'd like to discuss. How about you? How's your day going?\", role='assistant', function_call=None, tool_calls=None))], 'created': 1716980504, 'model': 'llama-3-70b-chat@together-ai', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=67, prompt_tokens=17, total_tokens=84, cost=7.56e-05)}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.llms.unify import Unify\n",
    "\n",
    "llm = Unify(model=\"llama-3-70b-chat@together-ai\")\n",
    "llm.complete(\"How are you today, llama?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Sign-On\n",
    "\n",
    "You can use Unify's SSO to query endpoints in different providers without making accounts with all of them. For example, all of these are valid endpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Unify(model=\"llama-2-70b-chat@together-ai\")\n",
    "llm = Unify(model=\"gpt-3.5-turbo@openai\")\n",
    "llm = Unify(model=\"mixtral-8x7b-instruct-v0.1@mistral-ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows you to quickly switch and test different models and providers. You can look at all the available models/providers [here](https://unify.ai/hub)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime Dynamic Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evidenced by our [benchmarks](https://unify.ai/benchmarks), the optimal provider for each model varies by geographic location and time of day due to fluctuating API performances. To cirumvent this, we automatically direct your requests to the \"top performing provider\" at runtime. To enable this feature, simply replace your query's provider with one of the [available routing modes](https://unify.ai/docs/api/deploy_router.html#optimizing-a-metric). Let's look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Unify(\n",
    "    model=\"llama-2-70b-chat@input-cost\"\n",
    ")  # route to lowest input cost provider\n",
    "llm = Unify(\n",
    "    model=\"gpt-3.5-turbo@itl\"\n",
    ")  # route to provider with lowest inter token latency\n",
    "llm = Unify(\n",
    "    model=\"mixtral-8x7b-instruct-v0.1@ttft\"\n",
    ")  # route to provider with lowest time to first token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Routing\n",
    "Unify routes your queries to the best LLM on every prompt to consistently achieve better quality outputs than using a single, all-purpose, powerful model, at a fraction of the cost. This is achieved by using smaller models for simpler tasks, only using largers ones to handle complex queries.\n",
    "\n",
    "The router is benchmarked on various different data-sets such as `Open Hermes`, `GSM8K`, `HellaSwag`, `MMLU` and `MT-Bench` revealing that it can peform better than indivudal endpoints on average as explained [here](https://unify.ai/docs/concepts/routing.html#quality-routing). One can choose various different configurations of the router for a particular data-set from the [chat-interface](https://unify.ai/chat) as shown below:\n",
    "\n",
    "<img src = \"https://github.com/unifyai/demos/blob/edd6e0506891c288182331b3d9ea9b792276db88/LlamaIndex/BasicUsage/unify.gif?raw=true\" alt=\"Unify Router Selection WalkThrough\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Unify(model=\"router_2.58e-01_9.51e-04_3.91e-03@unify\")\n",
    "llm = Unify(model=\"router_2.12e-01_5.00e-04_2.78e-04@unify\")\n",
    "llm = Unify(model=\"router_2.12e-01_5.00e-04_2.78e-04@unify\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about quality routing, please refer to this [video](https://www.youtube.com/watch?v=ZpY6SIkBosE&feature=youtu.be)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming and optimizing for latency\n",
    "\n",
    "If you are building an application where responsiveness is key, you most likely want to get a streaming response. On top of that, ideally you would use the provider with the lowest Time to First Token, to reduce the time your users are waiting for a response. Using Unify this would look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Unify(model=\"mixtral-8x7b-instruct-v0.1@ttft\")\n",
    "\n",
    "response = llm.stream_complete(\n",
    "    \"Translate the following to German: \"\n",
    "    \"Hey, there's an emergency in translation street, \"\n",
    "    \"please send help asap!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and provider are : mixtral-8x7b-instruct-v0.1@mistral-ai\n",
      "\n",
      "Hallo, es gibt einen Notfall in der √úbersetzungsstra√üe, bitte senden Sie Hilfe so schnell wie m√∂glich!\n",
      "\n",
      "(Note: This is a loose translation and the phrase \"√úbersetzungsstra√üe\" does not literally exist, but I tried to convey the same meaning as the original message.)"
     ]
    }
   ],
   "source": [
    "show_provider = True\n",
    "for r in response:\n",
    "    if show_provider:\n",
    "        print(f\"Model and provider are : {r.raw['model']}\\n\")\n",
    "        show_provider = False\n",
    "    print(r.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async calls and Lowest Input Cost\n",
    "\n",
    "Last but not the least, you can also run multiple requests asynchronously. For tasks such as document summarization, optimizing for input costs is crucial. We can use the `input-cost` dynamic routing mode to route our queries to the cheapest provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and provider are : mixtral-8x7b-instruct-v0.1@deepinfra\n",
      "\n",
      " OpenAI: Pioneering 'safe' artificial general intelligence.\n"
     ]
    }
   ],
   "source": [
    "llm = Unify(model=\"mixtral-8x7b-instruct-v0.1@input-cost\")\n",
    "\n",
    "response = await llm.acomplete(\n",
    "    \"Summarize this in 10 words or less. OpenAI is a U.S. based artificial intelligence \"\n",
    "    \"(AI) research organization founded in December 2015, researching artificial intelligence \"\n",
    "    \"with the goal of developing 'safe and beneficial' artificial general intelligence, \"\n",
    "    \"which it defines as 'highly autonomous systems that outperform humans at most economically \"\n",
    "    \"valuable work'. As one of the leading organizations of the AI spring, it has developed \"\n",
    "    \"several large language models, advanced image generation models, and previously, released \"\n",
    "    \"open-source models. Its release of ChatGPT has been credited with starting the AI spring\"\n",
    ")\n",
    "\n",
    "print(f\"Model and provider are : {response.raw['model']}\\n\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
