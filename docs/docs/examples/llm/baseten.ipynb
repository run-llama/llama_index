{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd54a32",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/llm/openai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3a8796-edc8-43f2-94ad-fe4fb20d70ed",
   "metadata": {},
   "source": [
    "# Baseten Cookbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d07d2",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ü¶ô."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index llama-index-llms-baseten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007403c-6b7a-420c-92f1-4171d05ed9bb",
   "metadata": {},
   "source": [
    "## Model APIs vs. Dedicated Deployments\n",
    "\n",
    "Baseten offers two main ways to access LLMs. \n",
    "1. Model APIs is a public endpoint for popular open source models (Deepseek, Llama, etc) where you can directly use a frontier model via slug e.g.  `deepseek-ai/DeepSeek-V3-0324` and you will be charged on a per-token basis. \n",
    "2. Dedicated deployments are useful for serving custom models where you want to autoscale production workloads and have fine-grain configuration. You need to deploy a model in your Baseten dashboard provide the 8 character model id like `abcd1234`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23df96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the integration path explicitly\n",
    "integration_path = '/Users/alexker/code/llama_index/llama-index-integrations/llms/llama-index-llms-baseten'\n",
    "if integration_path not in sys.path:\n",
    "    sys.path.insert(0, integration_path)\n",
    "\n",
    "# Now try the import\n",
    "from llama_index.llms.baseten import Baseten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5ddd8b",
   "metadata": {},
   "source": [
    "#### Model APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1241728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ID: deepseek-ai/DeepSeek-V3-0324\n",
      "API Base URL: https://inference.baseten.co/v1/\n",
      "\n",
      "Direct request response:\n",
      "Status: 200\n",
      "Response: {'id': 'chatcmpl-5e014980abc94be7a1eac5a195732927', 'choices': [{'index': 0, 'message': {'content': 'The capital of France is **Paris**.  \\n\\nParis is known for its iconic landmarks such as the **Eiffel Tower**, **Louvre Museum', 'refusal': None, 'tool_calls': None, 'role': 'assistant', 'function_call': None, 'audio': None}, 'finish_reason': 'length', 'logprobs': None}], 'created': 1752177248, 'model': 'baseten/DeepSeek-V3-FP4', 'service_tier': None, 'system_fingerprint': None, 'object': 'chat.completion', 'usage': {'prompt_tokens': 13, 'completion_tokens': 30, 'total_tokens': 43, 'prompt_tokens_details': None, 'completion_tokens_details': None}}\n",
      "\n",
      "LLM response:\n",
      "The capital of France is **Paris**. It is one of the most famous cities in the world, known for its rich history, culture, and landmarks like the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.  \n",
      "\n",
      "Would you like information on anything specific about Paris? üòä\n",
      "assistant: Arrr, matey! I be known as Captain Crimsonbeard, the most fearsome yet flamboyant pirate to ever sail the Seven Seas! With me trusty parrot, Squawks McSparkles, and me ship, *The Gilded Kraken*, I roam the waves in search of treasure, adventure, and the finest silks to match me fabulous red beard! \n",
      "\n",
      "What be *your* name, or shall I dub ye something worthy of a legend‚Äîlike \"Salty McGlitterboots\" or \"Daring Daggerfloss\"? üòâüè¥‚Äç‚ò†Ô∏è‚ú®\n"
     ]
    }
   ],
   "source": [
    "# Create LLM instance with debug info for Model APIs\n",
    "llm = Baseten(\n",
    "    model_id=\"deepseek-ai/DeepSeek-V3-0324\",\n",
    "    api_key=\"IOiHoajg.eOuTpW5QkkeJgzyXejbYSo7PDurr05sV\",\n",
    "    model_apis=True,  # Default\n",
    ")\n",
    "\n",
    "# We are storing the model_id in the model field\n",
    "print(f\"Model ID: {llm.model}\")\n",
    "print(f\"API Base URL: {llm.api_base}\")\n",
    "\n",
    "# Try a direct request first to verify API key and endpoint\n",
    "import requests\n",
    "direct_response = requests.post(\n",
    "    f\"https://inference.baseten.co/v1/chat/completions\",\n",
    "    headers={\"Authorization\": f\"Api-Key {llm.api_key}\"},\n",
    "    json={\n",
    "        \"model\": llm.model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the capital of France?\"\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 32,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nDirect request response:\")\n",
    "print(f\"Status: {direct_response.status_code}\")\n",
    "print(f\"Response: {direct_response.json()}\")\n",
    "\n",
    "# Now try the LLM call\n",
    "try:\n",
    "    llm_response = llm.complete(\"What is the capital of France?\")\n",
    "    print(\"\\nLLM response:\")\n",
    "    print(llm_response.text)\n",
    "except Exception as e:\n",
    "    print(\"\\nError in LLM call:\")\n",
    "    print(f\"Error type: {type(e)}\")\n",
    "    print(f\"Error message: {str(e)}\")\n",
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = llm.chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a2e45",
   "metadata": {},
   "source": [
    "#### Call `complete` with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60be18ae-c957-4ac2-a58a-0652e18ee6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ID: yqvr2lxw\n",
      "API Base URL: https://model-yqvr2lxw.api.baseten.co/environments/production/sync/v1\n",
      "\n",
      "Direct request response:\n",
      "Status: 200\n",
      "Response: {'id': '161', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"Paul Graham is a significant figure in the tech industry, recognized as a venture capitalist, programmer, and technical writer. He founded Y Combinator, one of the world's most successful startup accelerators. Paul Graham is known for his insightful writings on the tech industry and startups, and he played a crucial role in fostering many successful companies in various technology domains.\", 'refusal': None, 'role': 'assistant', 'audio': None, 'function_call': None, 'tool_calls': None}}], 'created': 1743032053, 'model': '', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': None, 'usage': {'completion_tokens': 73, 'prompt_tokens': 32, 'total_tokens': 105, 'completion_tokens_details': None, 'prompt_tokens_details': None}}\n",
      "\n",
      "LLM response:\n",
      "Paul Graham is a well-known figure in the tech and startup communities. He is the founder of Y Combinator, a startup accelerator program, and the author of several influential books on entrepreneurship. Graham is also known for his views on the tech industry and has been a vocal critic of certain aspects of Silicon Valley culture and venture capital practices. He has written extensively on topics such as the future of work, the importance of building great products, and the role of technology in society.\n"
     ]
    }
   ],
   "source": [
    "# Create LLM instance with debug info\n",
    "# Need to delete before committing\n",
    "llm = Baseten(\n",
    "    model_id=\"6wg17egw\",\n",
    "    api_key=\"IOiHoajg.eOuTpW5QkkeJgzyXejbYSo7PDurr05sV\",\n",
    ")\n",
    "\n",
    "# We are storing the model_id in the model field\n",
    "print(f\"Model ID: {llm.model}\")\n",
    "print(f\"API Base URL: {llm.api_base}\")\n",
    "\n",
    "# Try a direct request first to verify API key and endpoint, making sure model is up\n",
    "# Your model must be in production\n",
    "import requests\n",
    "direct_response = requests.post(\n",
    "    f\"https://model-{llm.model}.api.baseten.co/environments/production/predict\",\n",
    "    headers={\"Authorization\": f\"Api-Key {llm.api_key}\"},\n",
    "    json={\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Paul Graham is\"}],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nDirect request response:\")\n",
    "print(f\"Status: {direct_response.status_code}\")\n",
    "print(f\"Response: {direct_response.json()}\")\n",
    "\n",
    "# Now try the LLM call\n",
    "try:\n",
    "    llm_response = llm.complete(\"Paul Graham is\")\n",
    "    print(\"\\nLLM response:\")\n",
    "    print(llm_response.text)\n",
    "except Exception as e:\n",
    "    print(\"\\nError in LLM call:\")\n",
    "    print(f\"Error type: {type(e)}\")\n",
    "    print(f\"Error message: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14831268-f90f-499d-9d86-925dbc88292b",
   "metadata": {},
   "source": [
    "#### Call `chat` with a list of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbe29574-4af1-48d5-9739-f60652b6ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cbd550a-0264-4a11-9b2c-a08d8723a5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Arrr, matey! I be known as Captain Crimsonbeard‚Äîthough me beard be more fiery red than crimson, truth be told! A pirate of legend, scourge of the seven memes, and connoisseur of questionable life choices. But ye can call me Cap‚Äôn if ye like, or \"That Weird Pirate Who Won‚Äôt Stop Talking About Pineapples.\" Now, what mischief brings ye to me ship today? üè¥‚Äç‚ò†Ô∏èüçç\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5e894-4597-4911-a623-591560f72b82",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb7986f-aaed-42e2-abdd-f274f6d4fc59",
   "metadata": {},
   "source": [
    "Using `stream_complete` endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d43f17a2-0aeb-464b-a7a7-732ba5e8ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm.stream_complete(\"Paul Graham is \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0214e911-cf0d-489c-bc48-9bb1d8bf65d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is a British-American entrepreneur, essayist, and venture capitalist, best known as a co-founder of **Y Combinator**, a highly influential startup accelerator that has helped launch companies like Airbnb, Dropbox, Stripe, and Reddit.  \n",
      "\n",
      "### Key Facts About Paul Graham:  \n",
      "1. **Early Career**: Originally a programmer, he developed **Viaweb**, one of the first web-based applications, which was acquired by Yahoo! in 1998 and became Yahoo! Store.  \n",
      "2. **Y Combinator**: In 2005, he co-founded Y Combinator with Jessica Livingston, Robert Morris, and Trevor Blackwell. It pioneered the \"seed accelerator\" model, providing funding and mentorship to early-stage startups.  \n",
      "3. **Essays**: Graham is known for his insightful essays on startups, technology, and life philosophy, available on his website ([paulgraham.com](http://www.paulgraham.com)). Popular ones include *\"How to Get Startup Ideas\"* and *\"Do Things That Don't Scale.\"*  \n",
      "4. **Investments**: Through YC, he has backed thousands of startups, shaping Silicon Valley‚Äôs tech landscape.  \n",
      "5. **Lisp Advocate**: A proponent of the Lisp programming language,"
     ]
    }
   ],
   "source": [
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d688cca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40350dd8-3f50-4a2f-8545-5723942039bb",
   "metadata": {},
   "source": [
    "Using `stream_chat` endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc636e65-a67b-4dcd-ac60-b25abc9d8dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = llm.stream_chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4475a6bc-1051-4287-abce-ba83324aeb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, me name be Captain Crimsonbeard! A fearsome and flamboyant pirate with a beard as red as the setting sun and a wardrobe brighter than a treasure chest full o‚Äô jewels! I sail the seven seas in search of adventure, gold, and the finest rum‚Äîalways with a dramatic flair and a twinkle in me eye. \n",
      "\n",
      "What be yer name, matey? Or shall I just call ye \"Lucky Crewmember\" for now? *winks and adjusts my feathered hat*"
     ]
    }
   ],
   "source": [
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0248c57",
   "metadata": {},
   "source": [
    "# Async\n",
    "\"Async operations are used for long-running inference tasks that may hit request timeouts, batch inference jobs, and prioritizing certain requests.\"\n",
    "\n",
    "(1) In the integation, `acomplete` async function is implemented using the aiohttp library, an asynchronous HTTP client in python. The function invokes the async_predict at the approriate Baseten model endpoint, then the user receives a response with the request_id if successful. The user can then check the status or cancel the async_predict request using the returned request_id.\n",
    "\n",
    "(2) Once the model finishes executing the request, the async result will be posted to the user provided webhook endpoint. The user's endpoint is responsible for validating the webhook signature for security, then processing and storing the output.\n",
    "\n",
    "* Testing was completed with the Baseten *async inference user guide*'s webhook endpoint running on Replit.\n",
    "\n",
    "* achat was not implemented, because chat does not make sense for async operations.\n",
    "\n",
    "* The OpenAI parent class's async methods cannot be used directly because Baseten's async_request endpoint returns a request_id immediately.\n",
    "\n",
    "OpenAI: Wait for completion ‚Üí return result\n",
    "\n",
    "Baseten: Get request_id ‚Üí result is posted to webhook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5e2e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35643965636d4c3da6f54b5c3b354aa0\n"
     ]
    }
   ],
   "source": [
    "async_llm = Baseten(\n",
    "    model_id=\"YOUR_MODEL_ID\",\n",
    "    api_key=\"YOUR_API_KEY\", \n",
    "    webhook_endpoint=\"YOUR_WEBHOOK_ENDPOINT\",\n",
    ")\n",
    "response = await async_llm.acomplete(\"Paul Graham is\")\n",
    "print(response) # This is the request id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb54a4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'request_id': '35643965636d4c3da6f54b5c3b354aa0', 'model_id': 'yqvr2lxw', 'deployment_id': '31kmg1w', 'status': 'SUCCEEDED', 'webhook_status': 'SUCCEEDED', 'created_at': '2025-03-27T00:17:51.578558Z', 'status_at': '2025-03-27T00:18:38.768572Z', 'errors': []}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This will return the status information of a request using an async_predict request's request_id and the model_id the async_predict request was made with.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "model_id = \"YOUR_MODEL_ID\"\n",
    "request_id = \"YOUR_REQUEST_ID\"\n",
    "# Read secrets from environment variables\n",
    "baseten_api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "resp = requests.get(\n",
    "    f\"https://model-{model_id}.api.baseten.co/async_request/{request_id}\",\n",
    "    headers={\"Authorization\": f\"Api-Key {baseten_api_key}\"})\n",
    "\n",
    "print(resp.json())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
