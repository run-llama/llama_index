{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd54a32",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/llm/openai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3a8796-edc8-43f2-94ad-fe4fb20d70ed",
   "metadata": {},
   "source": [
    "# Baseten Integration Cookbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d07d2",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f6f8702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in /Users/alexker/anaconda3/lib/python3.10/site-packages (0.12.25)\n",
      "Requirement already satisfied: llama-index-llms-baseten in /Users/alexker/anaconda3/lib/python3.10/site-packages (0.3.2)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index) (0.4.6)\n",
      "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.1 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index) (0.4.1)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.25 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index) (0.12.25)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index) (0.6.9)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index) (0.3.27)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index) (0.4.3)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index) (0.4.7)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: openai>=1.14.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.68.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (3.11.14)\n",
      "Requirement already satisfied: dataclasses-json in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.5.8)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.26.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (3.2.1)\n",
      "Requirement already satisfied: numpy in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.23.5)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (9.4.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2.10.6)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (8.2.2)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.25->llama-index) (1.14.1)\n",
      "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.16)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pandas in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.0.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.4.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.4.post1)\n",
      "Requirement already satisfied: click in /Users/alexker/anaconda3/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/alexker/anaconda3/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (2022.7.9)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (6.0.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.18.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.3.2.post1)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2024.8.30)\n",
      "Requirement already satisfied: anyio in /Users/alexker/anaconda3/lib/python3.10/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.25->llama-index) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/alexker/anaconda3/lib/python3.10/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.0.7)\n",
      "Requirement already satisfied: idna in /Users/alexker/anaconda3/lib/python3.10/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.25->llama-index) (3.6)\n",
      "Requirement already satisfied: sniffio in /Users/alexker/anaconda3/lib/python3.10/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.25->llama-index) (0.14.0)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.4 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.8)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.8.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.26.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.25->llama-index) (2.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.25->llama-index) (0.4.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.25->llama-index) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2023.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.25->llama-index) (1.2.0)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from llama-cloud-services>=0.6.4->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.25->llama-index) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/alexker/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index llama-index-llms-baseten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007403c-6b7a-420c-92f1-4171d05ed9bb",
   "metadata": {},
   "source": [
    "## Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83bfb4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.baseten import Baseten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead155e-b8bd-46f9-ab9b-28fc009361dd",
   "metadata": {},
   "source": [
    "#### Call `complete` with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60be18ae-c957-4ac2-a58a-0652e18ee6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ID: yqvr2lxw\n",
      "API Base URL: https://model-yqvr2lxw.api.baseten.co/environments/production/sync/v1\n",
      "\n",
      "Direct request response:\n",
      "Status: 200\n",
      "Response: {'id': '161', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"Paul Graham is a significant figure in the tech industry, recognized as a venture capitalist, programmer, and technical writer. He founded Y Combinator, one of the world's most successful startup accelerators. Paul Graham is known for his insightful writings on the tech industry and startups, and he played a crucial role in fostering many successful companies in various technology domains.\", 'refusal': None, 'role': 'assistant', 'audio': None, 'function_call': None, 'tool_calls': None}}], 'created': 1743032053, 'model': '', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': None, 'usage': {'completion_tokens': 73, 'prompt_tokens': 32, 'total_tokens': 105, 'completion_tokens_details': None, 'prompt_tokens_details': None}}\n",
      "\n",
      "LLM response:\n",
      "Paul Graham is a well-known figure in the tech and startup communities. He is the founder of Y Combinator, a startup accelerator program, and the author of several influential books on entrepreneurship. Graham is also known for his views on the tech industry and has been a vocal critic of certain aspects of Silicon Valley culture and venture capital practices. He has written extensively on topics such as the future of work, the importance of building great products, and the role of technology in society.\n"
     ]
    }
   ],
   "source": [
    "# Create LLM instance with debug info\n",
    "# Need to delete before committing\n",
    "llm = Baseten(\n",
    "    model_id=\"YOUR_MODEL_ID\",\n",
    "    api_key=\"YOUR_API_KEY\",\n",
    ")\n",
    "\n",
    "# We are storing the model_id in the model field\n",
    "print(f\"Model ID: {llm.model}\")\n",
    "print(f\"API Base URL: {llm.api_base}\")\n",
    "\n",
    "# Try a direct request first to verify API key and endpoint, making sure model is up\n",
    "# Your model must be in production\n",
    "import requests\n",
    "direct_response = requests.post(\n",
    "    f\"https://model-{llm.model}.api.baseten.co/environments/production/predict\",\n",
    "    headers={\"Authorization\": f\"Api-Key {llm.api_key}\"},\n",
    "    json={\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Paul Graham is\"}],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nDirect request response:\")\n",
    "print(f\"Status: {direct_response.status_code}\")\n",
    "print(f\"Response: {direct_response.json()}\")\n",
    "\n",
    "# Now try the LLM call\n",
    "try:\n",
    "    llm_response = llm.complete(\"Paul Graham is\")\n",
    "    print(\"\\nLLM response:\")\n",
    "    print(llm_response.text)\n",
    "except Exception as e:\n",
    "    print(\"\\nError in LLM call:\")\n",
    "    print(f\"Error type: {type(e)}\")\n",
    "    print(f\"Error message: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14831268-f90f-499d-9d86-925dbc88292b",
   "metadata": {},
   "source": [
    "#### Call `chat` with a list of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe29574-4af1-48d5-9739-f60652b6ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cbd550a-0264-4a11-9b2c-a08d8723a5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Ahoy there, matey! My name is Captain Blunderbuss, but you can call me Cap'n if you're feeling friendly. Now, what's a dandy thing we can do with a name like that?\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5e894-4597-4911-a623-591560f72b82",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb7986f-aaed-42e2-abdd-f274f6d4fc59",
   "metadata": {},
   "source": [
    "Using `stream_complete` endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d43f17a2-0aeb-464b-a7a7-732ba5e8ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm.stream_complete(\"Paul Graham is \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0214e911-cf0d-489c-bc48-9bb1d8bf65d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is an American entrepreneur, venture capitalist, and writer. He is best known as the founder of Y Combinator, a startup accelerator program, and as the author of several influential books on entrepreneurship, including \"Hackers & Painters\" and \"Real Software.\" Graham has been a significant figure in the tech industry and has provided valuable insights and advice to many startups and entrepreneurs."
     ]
    }
   ],
   "source": [
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40350dd8-3f50-4a2f-8545-5723942039bb",
   "metadata": {},
   "source": [
    "Using `stream_chat` endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc636e65-a67b-4dcd-ac60-b25abc9d8dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = llm.stream_chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4475a6bc-1051-4287-abce-ba83324aeb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there, matey! My name is Captain Blunderbuss, but you can call me Cap'n if you're feeling friendly. Now, what's a dandy thing we can do with a name like that?"
     ]
    }
   ],
   "source": [
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0248c57",
   "metadata": {},
   "source": [
    "# Async\n",
    "\"Async operations are used for long-running inference tasks that may hit request timeouts, batch inference jobs, and prioritizing certain requests.\"\n",
    "\n",
    "(1) In the integation, `acomplete` async function is implemented using the aiohttp library, an asynchronous HTTP client in python. The function invokes the async_predict at the approriate Baseten model endpoint, then the user receives a response with the request_id if successful. The user can then check the status or cancel the async_predict request using the returned request_id.\n",
    "\n",
    "(2) Once the model finishes executing the request, the async result will be posted to the user provided webhook endpoint. The user's endpoint is responsible for validating the webhook signature for security, then processing and storing the output.\n",
    "\n",
    "* Testing was completed with the Baseten *async inference user guide*'s webhook endpoint running on Replit.\n",
    "\n",
    "* achat was not implemented, because chat does not make sense for async operations.\n",
    "\n",
    "* The OpenAI parent class's async methods cannot be used directly because Baseten's async_request endpoint returns a request_id immediately.\n",
    "\n",
    "OpenAI: Wait for completion â†’ return result\n",
    "\n",
    "Baseten: Get request_id â†’ result is posted to webhook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5e2e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35643965636d4c3da6f54b5c3b354aa0\n"
     ]
    }
   ],
   "source": [
    "async_llm = Baseten(\n",
    "    model_id=\"YOUR_MODEL_ID\",\n",
    "    api_key=\"YOUR_API_KEY\", \n",
    "    webhook_endpoint=\"YOUR_WEBHOOK_ENDPOINT\",\n",
    ")\n",
    "response = await async_llm.acomplete(\"Paul Graham is\")\n",
    "print(response) # This is the request id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb54a4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'request_id': '35643965636d4c3da6f54b5c3b354aa0', 'model_id': 'yqvr2lxw', 'deployment_id': '31kmg1w', 'status': 'SUCCEEDED', 'webhook_status': 'SUCCEEDED', 'created_at': '2025-03-27T00:17:51.578558Z', 'status_at': '2025-03-27T00:18:38.768572Z', 'errors': []}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This will return the status information of a request using an async_predict request's request_id and the model_id the async_predict request was made with.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "model_id = \"YOUR_MODEL_ID\"\n",
    "request_id = \"YOUR_REQUEST_ID\"\n",
    "# Read secrets from environment variables\n",
    "baseten_api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "resp = requests.get(\n",
    "    f\"https://model-{model_id}.api.baseten.co/async_request/{request_id}\",\n",
    "    headers={\"Authorization\": f\"Api-Key {baseten_api_key}\"})\n",
    "\n",
    "print(resp.json())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
