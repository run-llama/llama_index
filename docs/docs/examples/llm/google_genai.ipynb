{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/llm/gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we show how to use the `google-genai` Python SDK with LlamaIndex to interact with Google GenAI models.\n",
    "\n",
    "If you're opening this Notebook on colab, you will need to install LlamaIndex ðŸ¦™ and the `google-genai` Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-google-genai llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "You will need to get an API key from [Google AI Studio](https://makersuite.google.com/app/apikey). Once you have one, you can either pass it explicity to the model, or use the `GOOGLE_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "You can call `complete` with a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "llm = GoogleGenAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    # api_key=\"some key\",  # uses GOOGLE_API_KEY env var by default\n",
    ")\n",
    "\n",
    "resp = llm.complete(\"Who is Paul Graham?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also call `chat` with a list of chat messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"Tell me a story\"),\n",
    "]\n",
    "llm = GoogleGenAI(model=\"gemini-2.5-flash\")\n",
    "resp = llm.chat(messages)\n",
    "\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Support\n",
    "\n",
    "Every method supports streaming through the `stream_` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "llm = GoogleGenAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "resp = llm.stream_complete(\"Who is Paul Graham?\")\n",
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"Who is Paul Graham?\"),\n",
    "]\n",
    "\n",
    "resp = llm.stream_chat(messages)\n",
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Usage\n",
    "\n",
    "Every synchronous method has an async counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "llm = GoogleGenAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "resp = await llm.astream_complete(\"Who is Paul Graham?\")\n",
    "async for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"Who is Paul Graham?\"),\n",
    "]\n",
    "\n",
    "resp = await llm.achat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertex AI Support\n",
    "\n",
    "By providing the `region` and `project_id` parameters (either through environment variables or directly), you can enable usage through Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "!export GOOGLE_GENAI_USE_VERTEXAI=true\n",
    "!export GOOGLE_CLOUD_PROJECT='your-project-id'\n",
    "!export GOOGLE_CLOUD_LOCATION='us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "# or set the parameters directly\n",
    "llm = GoogleGenAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    vertexai_config={\"project\": \"your-project-id\", \"location\": \"us-central1\"},\n",
    "    # you should set the context window to the max input tokens for the model\n",
    "    context_window=200000,\n",
    "    max_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cached Content Support\n",
    "\n",
    "Google GenAI supports cached content for improved performance and cost efficiency when reusing large contexts across multiple requests. This is particularly useful for RAG applications, document analysis, and multi-turn conversations with consistent context.\n",
    "\n",
    "#### Benefits\n",
    "\n",
    "- **Faster responses**\n",
    "- **Cost savings** through reduced input token usage\n",
    "- **Consistent context** across multiple queries\n",
    "- **Perfect for document analysis** with large files\n",
    "\n",
    "#### Creating Cached Content\n",
    "\n",
    "First, create cached content using the Google GenAI SDK:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai.types import CreateCachedContentConfig, Content, Part\n",
    "import time\n",
    "\n",
    "client = genai.Client(api_key=\"your-api-key\")\n",
    "\n",
    "# For VertexAI\n",
    "# client = genai.Client(\n",
    "#     http_options=HttpOptions(api_version=\"v1\"),\n",
    "#     project=\"your-project-id\",\n",
    "#     location=\"us-central1\",\n",
    "#     vertexai=\"True\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Upload Local Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload and process local PDF files\n",
    "pdf_file = client.files.upload(file=\"./your_document.pdf\")\n",
    "while pdf_file.state.name == \"PROCESSING\":\n",
    "    print(\"Waiting for PDF to be processed.\")\n",
    "    time.sleep(2)\n",
    "    pdf_file = client.files.get(name=pdf_file.name)\n",
    "\n",
    "# Create cache with uploaded file\n",
    "cache = client.caches.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        display_name=\"Document Analysis Cache\",\n",
    "        system_instruction=(\n",
    "            \"You are an expert document analyzer. Answer questions \"\n",
    "            \"based on the provided documents with accuracy and detail.\"\n",
    "        ),\n",
    "        contents=[pdf_file],  # Direct file reference\n",
    "        ttl=\"3600s\",  # Cache for 1 hour\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: Multiple Files with Content Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple files or Cloud Storage files with VertexAI\n",
    "contents = [\n",
    "    Content(\n",
    "        role=\"user\",\n",
    "        parts=[\n",
    "            Part.from_uri(\n",
    "                # file_uri=pdf_file.uri,    # you can use the uploaded file's URI too\n",
    "                file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "                mime_type=\"application/pdf\",\n",
    "            ),\n",
    "            Part.from_uri(\n",
    "                file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "                mime_type=\"application/pdf\",\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "cache = client.caches.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        display_name=\"Multi-Document Cache\",\n",
    "        system_instruction=(\n",
    "            \"You are an expert researcher. Analyze and compare \"\n",
    "            \"information across the provided documents.\"\n",
    "        ),\n",
    "        contents=contents,\n",
    "        ttl=\"3600s\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"Cache created: {cache.name}\")\n",
    "print(f\"Cached tokens: {cache.usage_metadata.total_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Cached Content with LlamaIndex\n",
    "\n",
    "Once you have created the cache, use it with LlamaIndex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "llm = GoogleGenAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=\"your-api-key\",\n",
    "    cached_content=cache.name,\n",
    ")\n",
    "\n",
    "# For VertexAI\n",
    "# llm = GoogleGenAI(\n",
    "#     model=\"gemini-2.5-flash\",\n",
    "#     vertexai_config={\"project\": \"your-project-id\", \"location\": \"us-central1\"},\n",
    "#     cached_content=cache.name\n",
    "# )\n",
    "\n",
    "# Use the cached content\n",
    "message = ChatMessage(\n",
    "    role=\"user\", content=\"Summarize the key findings from Chapter 4.\"\n",
    ")\n",
    "response = llm.chat([message])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Cached Content in Generation Config\n",
    "\n",
    "For request-level caching control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.genai.types as types\n",
    "\n",
    "# Specify cached content per request\n",
    "config = types.GenerateContentConfig(\n",
    "    cached_content=cache.name, temperature=0.1, max_output_tokens=1024\n",
    ")\n",
    "\n",
    "llm = GoogleGenAI(model=\"gemini-2.5-flash\", generation_config=config)\n",
    "\n",
    "response = llm.complete(\"List the first five chapters of the document\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all caches\n",
    "caches = client.caches.list()\n",
    "for cache_item in caches:\n",
    "    print(f\"Cache: {cache_item.display_name} ({cache_item.name})\")\n",
    "    print(f\"Tokens: {cache_item.usage_metadata.total_token_count}\")\n",
    "\n",
    "# Get cache details\n",
    "cache_info = client.caches.get(name=cache.name)\n",
    "print(f\"Created: {cache_info.create_time}\")\n",
    "print(f\"Expires: {cache_info.expire_time}\")\n",
    "\n",
    "# Delete cache when done\n",
    "client.caches.delete(name=cache.name)\n",
    "print(\"Cache deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Modal Support\n",
    "\n",
    "Using `ChatMessage` objects, you can pass in images and text to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://cdn.pixabay.com/photo/2021/12/12/20/00/play-6865967_640.jpg -O image.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage, TextBlock, ImageBlock\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "llm = GoogleGenAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        blocks=[\n",
    "            ImageBlock(path=\"image.jpg\"),\n",
    "            TextBlock(text=\"What is in this image?\"),\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "resp = llm.chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass in documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import DocumentBlock\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        blocks=[\n",
    "            DocumentBlock(\n",
    "                path=\"/path/to/your/test.pdf\", mime_type=\"application/pdf\"\n",
    "            ),\n",
    "            TextBlock(text=\"Describe the document in a sentence.\"),\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "resp = llm.chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass in videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import VideoBlock\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        blocks=[\n",
    "            VideoBlock(path=\"/path/to/your/video.mp4\", mime_type=\"video/mp4\"),\n",
    "            TextBlock(text=\"Describe the video in a sentence.\"),\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "resp = llm.chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Prediction\n",
    "\n",
    "LlamaIndex provides an intuitive interface for converting any LLM into a structured LLM through `structured_predict` - simply define the target Pydantic class (can be nested), and given a prompt, we extract out the desired object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.bridge.pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class MenuItem(BaseModel):\n",
    "    \"\"\"A menu item in a restaurant.\"\"\"\n",
    "\n",
    "    course_name: str\n",
    "    is_vegetarian: bool\n",
    "\n",
    "\n",
    "class Restaurant(BaseModel):\n",
    "    \"\"\"A restaurant with name, city, and cuisine.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    city: str\n",
    "    cuisine: str\n",
    "    menu_items: List[MenuItem]\n",
    "\n",
    "\n",
    "llm = GoogleGenAI(model=\"gemini-2.5-flash\")\n",
    "prompt_tmpl = PromptTemplate(\n",
    "    \"Generate a restaurant in a given city {city_name}\"\n",
    ")\n",
    "\n",
    "# Option 1: Use `as_structured_llm`\n",
    "restaurant_obj = (\n",
    "    llm.as_structured_llm(Restaurant)\n",
    "    .complete(prompt_tmpl.format(city_name=\"Miami\"))\n",
    "    .raw\n",
    ")\n",
    "# Option 2: Use `structured_predict`\n",
    "# restaurant_obj = llm.structured_predict(Restaurant, prompt_tmpl, city_name=\"Miami\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(restaurant_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured Prediction with Streaming\n",
    "\n",
    "Any LLM wrapped with `as_structured_llm` supports streaming through `stream_chat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from IPython.display import clear_output\n",
    "from pprint import pprint\n",
    "\n",
    "input_msg = ChatMessage.from_str(\"Generate a restaurant in San Francisco\")\n",
    "\n",
    "sllm = llm.as_structured_llm(Restaurant)\n",
    "stream_output = sllm.stream_chat([input_msg])\n",
    "for partial_output in stream_output:\n",
    "    clear_output(wait=True)\n",
    "    pprint(partial_output.raw.dict())\n",
    "    restaurant_obj = partial_output.raw\n",
    "\n",
    "restaurant_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool/Function Calling\n",
    "\n",
    "Google GenAI supports direct tool/function calling through the API. Using LlamaIndex, we can implement some core agentic tool calling patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from datetime import datetime\n",
    "\n",
    "llm = GoogleGenAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "\n",
    "def get_current_time(timezone: str) -> dict:\n",
    "    \"\"\"Get the current time\"\"\"\n",
    "    return {\n",
    "        \"time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"timezone\": timezone,\n",
    "    }\n",
    "\n",
    "\n",
    "# uses the tool name, any type annotations, and docstring to describe the tool\n",
    "tool = FunctionTool.from_defaults(fn=get_current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply do a single pass to call the tool and get the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm.predict_and_call([tool], \"What is the current time in New York?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use lower-level APIs to implement an agentic tool-calling loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [\n",
    "    ChatMessage(role=\"user\", content=\"What is the current time in New York?\")\n",
    "]\n",
    "tools_by_name = {t.metadata.name: t for t in [tool]}\n",
    "\n",
    "resp = llm.chat_with_tools([tool], chat_history=chat_history)\n",
    "tool_calls = llm.get_tool_calls_from_response(\n",
    "    resp, error_on_no_tool_call=False\n",
    ")\n",
    "\n",
    "if not tool_calls:\n",
    "    print(resp)\n",
    "else:\n",
    "    while tool_calls:\n",
    "        # add the LLM's response to the chat history\n",
    "        chat_history.append(resp.message)\n",
    "\n",
    "        for tool_call in tool_calls:\n",
    "            tool_name = tool_call.tool_name\n",
    "            tool_kwargs = tool_call.tool_kwargs\n",
    "\n",
    "            print(f\"Calling {tool_name} with {tool_kwargs}\")\n",
    "            tool_output = tool.call(**tool_kwargs)\n",
    "            print(\"Tool output: \", tool_output)\n",
    "            chat_history.append(\n",
    "                ChatMessage(\n",
    "                    role=\"tool\",\n",
    "                    content=str(tool_output),\n",
    "                    # most LLMs like Gemini, Anthropic, OpenAI, etc. need to know the tool call id\n",
    "                    additional_kwargs={\"tool_call_id\": tool_call.tool_id},\n",
    "                )\n",
    "            )\n",
    "\n",
    "            resp = llm.chat_with_tools([tool], chat_history=chat_history)\n",
    "            tool_calls = llm.get_tool_calls_from_response(\n",
    "                resp, error_on_no_tool_call=False\n",
    "            )\n",
    "    print(\"Final response: \", resp.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also call multiple tools simultaneously in a single request, making it efficient for complex queries that require different types of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define another tool for temperature\n",
    "def get_temperature(city: str) -> dict:\n",
    "    \"\"\"Get the current temperature for a city\"\"\"\n",
    "    return {\n",
    "        \"city\": city,\n",
    "        \"temperature\": \"25Â°C\",\n",
    "    }\n",
    "\n",
    "\n",
    "# Create tools from functions\n",
    "tool1 = FunctionTool.from_defaults(fn=get_current_time)\n",
    "tool2 = FunctionTool.from_defaults(fn=get_temperature)\n",
    "\n",
    "# Ask a question that requires both tools\n",
    "chat_history = [\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        content=\"What is the current time and temperature in New York?\",\n",
    "    )\n",
    "]\n",
    "\n",
    "# The model will intelligently decide which tools to call\n",
    "resp = llm.chat_with_tools([tool1, tool2], chat_history=chat_history)\n",
    "tool_calls = llm.get_tool_calls_from_response(\n",
    "    resp, error_on_no_tool_call=False\n",
    ")\n",
    "\n",
    "print(f\"Model made {len(tool_calls)} tool calls:\")\n",
    "for i, tool_call in enumerate(tool_calls, 1):\n",
    "    print(f\"{i}. {tool_call.tool_name} with args: {tool_call.tool_kwargs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Search Grounding\n",
    "\n",
    "Google Gemini 2.0 and 2.5 models support Google Search grounding, which allows the model to search for real-time information and ground its responses with web search results. This is particularly useful for getting up-to-date information.\n",
    "\n",
    "The `built_in_tool` parameter accepts Google Search tools that enable the model to ground its responses with real-world data from Google Search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from google.genai import types\n",
    "\n",
    "# Create Google Search grounding tool\n",
    "grounding_tool = types.Tool(google_search=types.GoogleSearch())\n",
    "\n",
    "llm = GoogleGenAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    built_in_tool=grounding_tool,\n",
    ")\n",
    "\n",
    "resp = llm.complete(\"When is the next total solar eclipse in the US?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Google Search grounding tool provides several benefits:\n",
    "\n",
    "- **Real-time information**: Access to current events and up-to-date data\n",
    "- **Factual accuracy**: Responses grounded in actual search results\n",
    "- **Source attribution**: Grounding metadata includes search sources\n",
    "- **Automatic search decisions**: The model determines when to search based on the query\n",
    "\n",
    "You can also use the grounding tool with chat messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Google Search with chat messages\n",
    "messages = [ChatMessage(role=\"user\", content=\"Who won the Euro 2024?\")]\n",
    "\n",
    "resp = llm.chat(messages)\n",
    "print(resp)\n",
    "\n",
    "# You can access grounding metadata from the raw response\n",
    "if hasattr(resp, \"raw\") and \"grounding_metadata\" in resp.raw:\n",
    "    print(resp.raw[\"grounding_metadata\"])\n",
    "else:\n",
    "    print(\"\\nNo grounding metadata in this response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Execution\n",
    "\n",
    "The `built_in_tool` parameter also accepts code execution tools that enable the model to write and execute Python code to solve problems, perform calculations, and analyze data. This is particularly useful for mathematical computations, data analysis, and generating visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from google.genai import types\n",
    "\n",
    "# Create code execution tool\n",
    "code_execution_tool = types.Tool(code_execution=types.ToolCodeExecution())\n",
    "\n",
    "llm = GoogleGenAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    built_in_tool=code_execution_tool,\n",
    ")\n",
    "\n",
    "resp = llm.complete(\"Calculate 20th fibonacci number.\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Code Execution Details\n",
    "\n",
    "When the model uses code execution, you can access the executed code, results, and other metadata through the raw response. This includes:\n",
    "\n",
    "- **executable_code**: The actual Python code that was executed\n",
    "- **code_execution_result**: The output from running the code\n",
    "- **text**: The model's explanation and commentary\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request a calculation that will likely use code execution\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"user\", content=\"What is the sum of the first 50 prime numbers?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "resp = llm.chat(messages)\n",
    "\n",
    "# Access the raw response to see code execution details\n",
    "if hasattr(resp, \"raw\") and \"content\" in resp.raw:\n",
    "    parts = resp.raw[\"content\"].get(\"parts\", [])\n",
    "\n",
    "    for i, part in enumerate(parts):\n",
    "        print(f\"Part {i+1}:\")\n",
    "\n",
    "        if \"text\" in part and part[\"text\"]:\n",
    "            print(f\"  Text: {part['text'][:100]}\", end=\"\")\n",
    "            print(\" ...\" if len(part[\"text\"]) > 100 else \"\")\n",
    "\n",
    "        if \"executable_code\" in part and part[\"executable_code\"]:\n",
    "            print(f\"  Executable Code: {part['executable_code']}\")\n",
    "\n",
    "        if \"code_execution_result\" in part and part[\"code_execution_result\"]:\n",
    "            print(f\"  Code Result: {part['code_execution_result']}\")\n",
    "else:\n",
    "    print(\"No detailed parts found in raw response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation\n",
    "\n",
    "Select models also support image outputs, as well as image inputs. Using the `response_modalities` config, we can generate and edit images with a Gemini model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "import google.genai.types as types\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    temperature=0.1, response_modalities=[\"Text\", \"Image\"]\n",
    ")\n",
    "\n",
    "llm = GoogleGenAI(\n",
    "    model=\"models/gemini-2.5-flash-exp\", generation_config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage, TextBlock, ImageBlock\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"Please generate an image of a cute dog\")\n",
    "]\n",
    "\n",
    "resp = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "for block in resp.message.blocks:\n",
    "    if isinstance(block, ImageBlock):\n",
    "        image = Image.open(block.resolve_image())\n",
    "        display(image)\n",
    "    elif isinstance(block, TextBlock):\n",
    "        print(block.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also edit the image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(resp.message)\n",
    "messages.append(\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        content=\"Please edit the image to make the dog a mini-schnauzer, but keep the same overall pose, framing, background, and art style.\",\n",
    "    )\n",
    ")\n",
    "\n",
    "resp = llm.chat(messages)\n",
    "\n",
    "for block in resp.message.blocks:\n",
    "    if isinstance(block, ImageBlock):\n",
    "        image = Image.open(block.resolve_image())\n",
    "        display(image)\n",
    "    elif isinstance(block, TextBlock):\n",
    "        print(block.text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "gemini.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
