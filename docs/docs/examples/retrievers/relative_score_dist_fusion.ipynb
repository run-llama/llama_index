{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/retrievers/relative_score_dist_fusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative Score Fusion and Distribution-Based Score Fusion\n",
    "\n",
    "In this example, we demonstrate using QueryFusionRetriever with two methods which aim to improve on Reciprocal Rank Fusion:\n",
    "1. Relative Score Fusion ([Weaviate](https://weaviate.io/blog/hybrid-search-fusion-algorithms))\n",
    "2. Distribution-Based Score Fusion ([Mazzecchi: blog post](https://medium.com/plain-simple-software/distribution-based-score-fusion-dbsf-a-new-approach-to-vector-search-ranking-f87c37488b18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-openai\n",
    "%pip install llama-index-retrievers-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p 'data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will setup a vector index over the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.55it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 504/504 [00:03<00:00, 128.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=256)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, transformations=[splitter], show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Hybrid Fusion Retriever using Relative Score Fusion\n",
    "\n",
    "In this step, we fuse our index with a BM25 based retriever. This will enable us to capture both semantic relations and keywords in our input queries.\n",
    "\n",
    "Since both of these retrievers calculate a score, we can use the `QueryFusionRetriever` to re-sort our nodes without using an additional models or excessive computation.\n",
    "\n",
    "The following example uses the [Relative Score Fusion](https://weaviate.io/blog/hybrid-search-fusion-algorithms) algorithm from Weaviate, which applies a MinMax scaler to each result set, then makes a weighted sum. Here, we'll give the vector retriever slightly more weight than BM25 (0.6 vs. 0.4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create our retrievers. Each will retrieve the top-10 most similar nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "vector_retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create our fusion retriever, which well return the top-10 most similar nodes from the 20 returned nodes from the retrievers.\n",
    "\n",
    "Note that the vector and BM25 retrievers may have returned all the same nodes, only in different orders; in this case, it simply acts as a re-ranker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    retriever_weights=[0.6, 0.4],\n",
    "    similarity_top_k=10,\n",
    "    num_queries=1,  # set this to 1 to disable query generation\n",
    "    mode=\"relative_score\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply nested async to run in a notebook\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_with_scores = retriever.retrieve(\n",
    "    \"What happened at Interleafe and Viaweb?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.60 - You wouldn't need versions, or ports, or any of that crap. At Interleaf there had been a whole group...\n",
      "-----\n",
      "Score: 0.59 - The UI was horrible, but it proved you could build a whole store through the browser, without any cl...\n",
      "-----\n",
      "Score: 0.40 - We were determined to be the Microsoft Word, not the Interleaf. Which meant being easy to use and in...\n",
      "-----\n",
      "Score: 0.36 - In its time, the editor was one of the best general-purpose site builders. I kept the code tight and...\n",
      "-----\n",
      "Score: 0.25 - I kept the code tight and didn't have to integrate with any other software except Robert's and Trevo...\n",
      "-----\n",
      "Score: 0.25 - If all I'd had to do was work on this software, the next 3 years would have been the easiest of my l...\n",
      "-----\n",
      "Score: 0.21 - To find out, we decided to try making a version of our store builder that you could control through ...\n",
      "-----\n",
      "Score: 0.11 - But the most important thing I learned, and which I used in both Viaweb and Y Combinator, is that th...\n",
      "-----\n",
      "Score: 0.11 - The next year, from the summer of 1998 to the summer of 1999, must have been the least productive of...\n",
      "-----\n",
      "Score: 0.07 - The point is that it was really cheap, less than half market price.\n",
      "\n",
      "[8] Most software you can launc...\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for node in nodes_with_scores:\n",
    "    print(f\"Score: {node.score:.2f} - {node.text[:100]}...\\n-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution-Based Score Fusion\n",
    "\n",
    "A variant on Relative Score Fusion, [Distribution-Based Score Fusion](https://medium.com/plain-simple-software/distribution-based-score-fusion-dbsf-a-new-approach-to-vector-search-ranking-f87c37488b18) scales the scores a bit differently - based on the mean and standard deviation of the scores for each result set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.42 - You wouldn't need versions, or ports, or any of that crap. At Interleaf there had been a whole group...\n",
      "-----\n",
      "Score: 0.41 - The UI was horrible, but it proved you could build a whole store through the browser, without any cl...\n",
      "-----\n",
      "Score: 0.32 - We were determined to be the Microsoft Word, not the Interleaf. Which meant being easy to use and in...\n",
      "-----\n",
      "Score: 0.30 - In its time, the editor was one of the best general-purpose site builders. I kept the code tight and...\n",
      "-----\n",
      "Score: 0.27 - To find out, we decided to try making a version of our store builder that you could control through ...\n",
      "-----\n",
      "Score: 0.24 - I kept the code tight and didn't have to integrate with any other software except Robert's and Trevo...\n",
      "-----\n",
      "Score: 0.24 - If all I'd had to do was work on this software, the next 3 years would have been the easiest of my l...\n",
      "-----\n",
      "Score: 0.20 - Now we felt like we were really onto something. I had visions of a whole new generation of software ...\n",
      "-----\n",
      "Score: 0.20 - Users wouldn't need anything more than a browser.\n",
      "\n",
      "This kind of software, known as a web app, is com...\n",
      "-----\n",
      "Score: 0.18 - But the most important thing I learned, and which I used in both Viaweb and Y Combinator, is that th...\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    retriever_weights=[0.6, 0.4],\n",
    "    similarity_top_k=10,\n",
    "    num_queries=1,  # set this to 1 to disable query generation\n",
    "    mode=\"dist_based_score\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "nodes_with_scores = retriever.retrieve(\n",
    "    \"What happened at Interleafe and Viaweb?\"\n",
    ")\n",
    "\n",
    "for node in nodes_with_scores:\n",
    "    print(f\"Score: {node.score:.2f} - {node.text[:100]}...\\n-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use in a Query Engine!\n",
    "\n",
    "Now, we can plug our retriever into a query engine to synthesize natural language responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What happened at Interleafe and Viaweb?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** At Interleaf, there was a group called Release Engineering that was as large as the group writing the software. They had to deal with versions, ports, and other complexities. In contrast, at Viaweb, the software could be updated directly on the server, simplifying the process. Viaweb was founded with $10,000 in seed funding, and the software allowed building a whole store through the browser without the need for client software or command line inputs on the server. The company aimed to be easy to use and inexpensive, offering low monthly prices for their services."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_response\n",
    "\n",
    "display_response(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
