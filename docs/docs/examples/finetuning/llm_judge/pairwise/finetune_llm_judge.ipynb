{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "722c13cc-a78a-4037-859e-48c538d00d9b",
   "metadata": {},
   "source": [
    "# Knowledge Distillation For Fine-Tuning A GPT-3.5 Judge (Pairwise)\n",
    "\n",
    "There has been recent research that demonstrated GPT-4's ability to closely align to human judges when evaluating LLM generated texts (e.g., see [[1]](https://arxiv.org/abs/2306.05685), [[2]](https://arxiv.org/abs/2303.16634)). In this notebook, we demonstrate how to use the `llama_index` library to distill knowledge from GPT-4 to GPT-3.5 so that a smaller GPT-3.5 becomes closer to GPT-4 performance; and by proxy, closer to human judges.\n",
    "\n",
    "To do so, we will perform the following high level steps:\n",
    "\n",
    "1. Generate datasets: `train_dataset` and `test_dataset`\n",
    "2. Perform knowledge distillation (using `train_dataset`)\n",
    "3. Evaluate the distilled model  on `test_dataset`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1863cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-readers-wikipedia\n",
    "%pip install llama-index-finetuning\n",
    "%pip install llama-index-llms-openai\n",
    "%pip install llama-index-llms-mistralai\n",
    "%pip install llama-index-llms-huggingface-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e24ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this notebook makes several API calls to generate text with OpenAI GPT\n",
    "# models as well as models hosted on HuggingFace. If you prefer not to wait for\n",
    "# these generations, then the data for this notebook can be obtained with the\n",
    "# `wget` command provided below.\n",
    "\n",
    "# !wget \"https://www.dropbox.com/scl/fo/m7skpjdbpb0g3p76y6epe/h?rlkey=omh2ysgh9qqqztf81qvjlivu2&dl=1\" -O pairwise.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d01ad-d2c3-46a5-876b-7461dd593147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f7df9-a048-43aa-b862-290e832ea631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# we will be using models on HuggingFace as our LLM answer generators\n",
    "HUGGING_FACE_TOKEN = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
    "\n",
    "# we will use GPT-4 and GPT-3.5 + OpenAI Fine-Tuning\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec7031c-9da8-4116-969a-b1180a0fc118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# define jupyter display function\n",
    "def display_eval_df(question, source, answer_a, answer_b, result) -> None:\n",
    "    \"\"\"Pretty print question/answer + gpt-4 judgement dataset.\"\"\"\n",
    "    eval_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Question\": question,\n",
    "            \"Source\": source,\n",
    "            \"Model A\": answer_a[\"model\"],\n",
    "            \"Answer A\": answer_a[\"text\"],\n",
    "            \"Model B\": answer_b[\"model\"],\n",
    "            \"Answer B\": answer_b[\"text\"],\n",
    "            \"Score\": result.score,\n",
    "            \"Judgement\": result.feedback,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    eval_df = eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"300px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Answer A\", \"Answer B\"]\n",
    "    )\n",
    "    display(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c89edf1-b359-4370-8b1e-fad279508c68",
   "metadata": {},
   "source": [
    "## Step 1 Generate datasets: `train_dataset` and `test_dataset`\n",
    "\n",
    "For our dataset on which we will generate questions and prompt various LLMs to answer, we're going to use the `WikipediaReader` to read \"History of <city>\" for several cities. We're going to split up our cities into two lists: one to be used for `train_dataset` and the other for `test_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88777f6b-a746-4df9-80c4-e6296d4e7a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff11314-315d-465b-983a-f17d657436ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia pages\n",
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "\n",
    "train_cities = [\n",
    "    \"San Francisco\",\n",
    "    \"Toronto\",\n",
    "    \"New York City\",\n",
    "    \"Vancouver\",\n",
    "    \"Montreal\",\n",
    "    \"Boston\",\n",
    "]\n",
    "\n",
    "test_cities = [\n",
    "    \"Tokyo\",\n",
    "    \"Singapore\",\n",
    "    \"Paris\",\n",
    "]\n",
    "\n",
    "train_documents = WikipediaReader().load_data(\n",
    "    pages=[f\"History of {x}\" for x in train_cities]\n",
    ")\n",
    "test_documents = WikipediaReader().load_data(\n",
    "    pages=[f\"History of {x}\" for x in test_cities]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66486ab-38cf-4ed6-bef4-6fe9deee0590",
   "metadata": {},
   "source": [
    "### Use a `DatasetGenerator` to build `train_dataset` and `test_dataset`\n",
    "\n",
    "Now that we have our train and test set of `Document`'s, the next step is to generate the questions. For this we will use the `DatasetGenerator`, which uses an LLM to generate questions from given set of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef531ed-8e97-4d8f-8cc3-6f7e0c6ca141",
   "metadata": {},
   "source": [
    "#### Generate Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99afd212-38b0-492c-91ea-a810e126ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_GEN_PROMPT = (\n",
    "    \"You are a Teacher/ Professor. Your task is to setup \"\n",
    "    \"a quiz/examination. Using the provided context, formulate \"\n",
    "    \"a single question that captures an important fact from the \"\n",
    "    \"context. Restrict the question to the context information provided.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc60af-2ef8-43b6-8b24-f46adc223d03",
   "metadata": {},
   "source": [
    "With all that out of the way, let's spring into action. First, we will download the reference pdf document and create the set of questions against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e10603-a0e0-4c87-a4d5-fdf8f1ca0303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate questions against chunks\n",
    "from llama_index.core.evaluation import DatasetGenerator\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "\n",
    "\n",
    "# instantiate DatasetGenerator's for train and test\n",
    "train_dataset_generator = DatasetGenerator.from_documents(\n",
    "    train_documents,\n",
    "    question_gen_query=QUESTION_GEN_PROMPT,\n",
    "    llm=llm,\n",
    "    show_progress=True,\n",
    "    num_questions_per_chunk=25,\n",
    ")\n",
    "\n",
    "test_dataset_generator = DatasetGenerator.from_documents(\n",
    "    test_documents,\n",
    "    question_gen_query=QUESTION_GEN_PROMPT,\n",
    "    llm=llm,\n",
    "    show_progress=True,\n",
    "    num_questions_per_chunk=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e2276-92dc-48c3-8cd0-583655ab7ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [00:02<00:00, 36.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# use DatasetGenerator to create questions from nodes\n",
    "train_questions = train_dataset_generator.generate_questions_from_nodes(\n",
    "    num=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0a873-e22d-4eb7-97f9-336ca33346fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [00:02<00:00, 29.98it/s]\n"
     ]
    }
   ],
   "source": [
    "test_questions = test_dataset_generator.generate_questions_from_nodes(num=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84e1a9a-3d3f-48c9-a8e2-c5ce124c0250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_questions), len(test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f3dd3-62ab-4aac-a995-cf16d3306f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What event in 1906 caused significant damage to San Francisco but was followed by a quick rebuild?',\n",
       " 'What was the name of the first significant homestead established outside the immediate vicinity of Mission Dolores in San Francisco?',\n",
       " \"What event in 1855 led to the establishment of San Francisco's first county hospital and the development of California's system of county hospitals for the poor?\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's take a look at a few of these\n",
    "train_questions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52556d80-0aef-4e2e-a4c8-2ba3a48ffe2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Question: What was the name of the oldest Buddhist temple in Tokyo, founded in 628?',\n",
       " 'What event marked the end of the samurai system and feudal class divisions in Tokyo?',\n",
       " 'Question: What role did the Tokyo Imperial University play in the Meiji Era?']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_questions[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b201d9cb-4746-4c71-8728-55e56cb8b76f",
   "metadata": {},
   "source": [
    "#### Generate Answers To The Questions\n",
    "\n",
    "The next step is to generate answers using LLMs. Just a reminder, that the point is to judge these generated answers. So later on, we will use GPT models to judge these answers.\n",
    "\n",
    "But for the generation of the answers to the questions, we will use two other LLMs, namely: Llama-2 and Mistral. In order to do this, we first a create a vector store for our documents and an associated retriever, which both of the LLM answer-generators will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973d108-a350-4afc-8add-69969c59c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "# Create vector index\n",
    "train_index = VectorStoreIndex.from_documents(documents=train_documents)\n",
    "\n",
    "# Create the retriver on this index\n",
    "train_retriever = VectorIndexRetriever(\n",
    "    index=train_index,\n",
    "    similarity_top_k=2,\n",
    ")\n",
    "\n",
    "# Create vector index for test to be used later\n",
    "test_index = VectorStoreIndex.from_documents(documents=test_documents)\n",
    "\n",
    "# Create the retriver for test to be used later\n",
    "test_retriever = VectorIndexRetriever(\n",
    "    index=test_index,\n",
    "    similarity_top_k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65125845-b96c-4f84-b45c-7ddaf6910d92",
   "metadata": {},
   "source": [
    "From here we will build `RetrieverQueryEngine`'s that will take in our queries (i.e. questions) for processing. Note that we use `HuggingFaceInferenceAPI` for our LLM answer-generators, and that Llama-2 requires permissions. If you haven't yet gain accessed to these models, then feel free to swap out Llama-2 with another model of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b687ddf-c8fa-4073-9aa9-0b9cfec23f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "\n",
    "def create_query_engine(\n",
    "    hf_name: str, retriever: VectorIndexRetriever, hf_llm_generators: dict\n",
    ") -> RetrieverQueryEngine:\n",
    "    \"\"\"Create a RetrieverQueryEngine using the HuggingFaceInferenceAPI LLM\"\"\"\n",
    "    if hf_name not in hf_llm_generators:\n",
    "        raise KeyError(\"model not listed in hf_llm_generators\")\n",
    "    llm = HuggingFaceInferenceAPI(\n",
    "        model_name=hf_llm_generators[hf_name],\n",
    "        context_window=2048,  # to use refine\n",
    "        token=HUGGING_FACE_TOKEN,\n",
    "    )\n",
    "    return RetrieverQueryEngine.from_args(retriever=retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84331853-ac29-49ca-85c1-e874d26e5f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our llm-generators (query_engines)\n",
    "hf_llm_generators = {\n",
    "    \"mistral-7b-instruct\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"llama2-7b-chat\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "}\n",
    "\n",
    "train_query_engines = {\n",
    "    mdl: create_query_engine(mdl, train_retriever, hf_llm_generators)\n",
    "    for mdl in hf_llm_generators.keys()\n",
    "}\n",
    "\n",
    "test_query_engines = {\n",
    "    mdl: create_query_engine(mdl, test_retriever, hf_llm_generators)\n",
    "    for mdl in hf_llm_generators.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760a173-675a-4db0-8f66-0b396c2a34d7",
   "metadata": {},
   "source": [
    "We're ready to now to produce the answers from the various LLMs. We'll do this now for the `train_dataset` and hold off on doing this for `test_dataset` until the time comes for us to use it.\n",
    "\n",
    "NOTE: this will take some time to generate. If you'd rather not wait, you have the option of loading the `train_qa.jsonl` that contains Llama-2 and Mistral answers per question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1d126f-8fcc-42ea-b143-4533638763a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [07:40<00:00,  6.14s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import random\n",
    "\n",
    "train_dataset = []\n",
    "for q in tqdm.tqdm(train_questions):\n",
    "    # randomly select two LLMs to generate answers to this q\n",
    "    model_versus = random.sample(list(train_query_engines.items()), 2)\n",
    "\n",
    "    # data for this q\n",
    "    data_entry = {\"question\": q}\n",
    "    responses = []\n",
    "    source = None\n",
    "\n",
    "    # generate answers\n",
    "    for name, engine in model_versus:\n",
    "        response = engine.query(q)\n",
    "        response_struct = {}\n",
    "        response_struct[\"model\"] = name\n",
    "        response_struct[\"text\"] = str(response)\n",
    "        if source is not None:\n",
    "            assert source == response.source_nodes[0].node.text[:1000] + \"...\"\n",
    "        else:\n",
    "            source = response.source_nodes[0].node.text[:1000] + \"...\"\n",
    "        responses.append(response_struct)\n",
    "\n",
    "    data_entry[\"answers\"] = responses\n",
    "    data_entry[\"source\"] = source\n",
    "    train_dataset.append(data_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e653515-75c4-4987-87e4-c0a0b17a0bdf",
   "metadata": {},
   "source": [
    "### Get GPT-4 Evaluations On The Mistral and LLama-2 Answers \n",
    "\n",
    "As mentioned a couple of times before, the point of this guide is fine-tune an LLM judge from a GPT-4 judge. So, in order to complete our `train_dataset` we now need to instantiate our GPT-4 judge and have it evaluate the answers that were provided by the other LLMs: Llama-2 and Mistral. To do this, we will use the `PairwiseComparisonEvaluator` class. What this judge will do then is it will compare the two answers and provide a verdict as to whether Llama-2's answer is better, Mistral's answer is better, or if it's a tie.\n",
    "\n",
    "There is a bit of added nuance here since with pairwise evaluations, we have to be mindful of the potential for \"position-bias\". This is when the judge favours the first answer that was presented to it (within the prompt/context). To account for this position-bias, we invoke the GPT-4 judge to perform to evaluations per sample, where in the second evaluation, we switch the order of presentation of the two answers (i.e., first evaluation: Llama-2 then Mistral, second evaluation: Mistral then Llama-2).\n",
    "\n",
    "Finally, we also use the `OpenAIFineTuningHandler` which will collect all the chat histories that we will eventually need to fine-tune GPT-3.5.\n",
    "\n",
    "NOTE: this will take some time to generate the judgements. Again, you have the option to load the `train_qa.jsonl` as `train_dataset`. Moreover, we also stored the JSONL files that we passed to OpenAI to fine-tune GPT-3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe8958f-62fd-41a8-8d21-3055cd80de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the gpt-4 judge\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.finetuning.callbacks import OpenAIFineTuningHandler\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from llama_index.core.evaluation import PairwiseComparisonEvaluator\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# NOTE: this finetuning_handler will collect 2x chat_histories for\n",
    "# each query: one for original, and another for flipped\n",
    "main_finetuning_handler = OpenAIFineTuningHandler()\n",
    "callback_manager = CallbackManager([main_finetuning_handler])\n",
    "Settings.callback_manager = callback_manager\n",
    "\n",
    "llm_4 = OpenAI(temperature=0, model=\"gpt-4\", callback_manager=callback_manager)\n",
    "\n",
    "gpt4_judge = PairwiseComparisonEvaluator(llm=llm_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0a4d4-e55f-4189-8e64-132c128bd5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [48:04<00:00, 38.46s/it]\n"
     ]
    }
   ],
   "source": [
    "for data_entry in tqdm.tqdm(train_dataset):\n",
    "    final_eval_result = await gpt4_judge.aevaluate(\n",
    "        query=data_entry[\"question\"],\n",
    "        response=data_entry[\"answers\"][0][\"text\"],\n",
    "        second_response=data_entry[\"answers\"][1][\"text\"],\n",
    "        reference=data_entry[\"source\"],\n",
    "    )\n",
    "\n",
    "    # save final result\n",
    "    judgement = {}\n",
    "    judgement[\"llm\"] = \"gpt_4\"\n",
    "    judgement[\"score\"] = final_eval_result.score\n",
    "    judgement[\"text\"] = final_eval_result.response\n",
    "    judgement[\"source\"] = final_eval_result.pairwise_source\n",
    "    data_entry[\"evaluations\"] = [judgement]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faae6f0-bb84-4762-8716-5594bf8f25a7",
   "metadata": {},
   "source": [
    "Let's see how one of these GPT-4 evaluations looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfd4601-289e-4fba-bc74-b9a0cfcce09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_47a77_row0_col3, #T_47a77_row0_col5 {\n",
       "  inline-size: 300px;\n",
       "  overflow-wrap: break-word;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_47a77\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_47a77_level0_col0\" class=\"col_heading level0 col0\" >Question</th>\n",
       "      <th id=\"T_47a77_level0_col1\" class=\"col_heading level0 col1\" >Source</th>\n",
       "      <th id=\"T_47a77_level0_col2\" class=\"col_heading level0 col2\" >Model A</th>\n",
       "      <th id=\"T_47a77_level0_col3\" class=\"col_heading level0 col3\" >Answer A</th>\n",
       "      <th id=\"T_47a77_level0_col4\" class=\"col_heading level0 col4\" >Model B</th>\n",
       "      <th id=\"T_47a77_level0_col5\" class=\"col_heading level0 col5\" >Answer B</th>\n",
       "      <th id=\"T_47a77_level0_col6\" class=\"col_heading level0 col6\" >Score</th>\n",
       "      <th id=\"T_47a77_level0_col7\" class=\"col_heading level0 col7\" >Judgement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_47a77_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_47a77_row0_col0\" class=\"data row0 col0\" >What resources are available in Boston for studying and preserving historical artifacts and remains?</td>\n",
       "      <td id=\"T_47a77_row0_col1\" class=\"data row0 col1\" >In 1963, Boston Mayor John F. Collins and Boston Redevelopment Authority (BRA) executive Edward J. Logue organized a consortium of savings banks, cooperatives, and federal and state savings and loan associations in the city called the Boston Banks Urban Renewal Group (B-BURG) that would reverse redline parts of Dorchester, Roxbury, and Mattapan along Blue Hill Avenue. Despite the passage of legislation by the 156th Massachusetts General Court banning racial discrimination or segregation in housing in 1950, as well as the issuance of Executive Order 11063 by President John F. Kennedy in 1962 requiring all federal agencies to prevent racial discrimination in all federally-funded subsidized housing in the United States, the Boston Housing Authority (BHA) Board actively segregated the public housing developments in the city during the Collins administration as well, with BHA departments engaging in bureaucratic resistance against integration through at least 1966 and the Board retaining co...</td>\n",
       "      <td id=\"T_47a77_row0_col2\" class=\"data row0 col2\" >llama2-7b-chat</td>\n",
       "      <td id=\"T_47a77_row0_col3\" class=\"data row0 col3\" >\n",
       "There are several resources available in Boston for studying and preserving historical artifacts and remains. Some of these include:\n",
       "\n",
       "1. The Boston Historical Society: This organization provides access to historical artifacts, photographs, and documents related to Boston's history.\n",
       "2. Boston Mapjunction: This website provides over 200 historical maps of Boston dating back to 1630, as well as aerial photos compared with maps of today.\n",
       "3. City of Boston Archaeology Program and Lab: The City of Boston has a City Archaeologist on staff to oversee any lots of land to be developed for historical artifacts and significance, and to manage the archaeological remains located on public land in Boston. The program also has an Archaeology Laboratory and Education and Curation Center.\n",
       "4. The Freedom House Photographs Collection: This collection contains over 2,000 images of Roxbury people, places, and events from 1950 to 1975.\n",
       "5. Vital Records of Boston: This resource provides access to historical records related to births, marriages, and deaths in Boston.\n",
       "6. Reading and Everyday Life in</td>\n",
       "      <td id=\"T_47a77_row0_col4\" class=\"data row0 col4\" >mistral-7b-instruct</td>\n",
       "      <td id=\"T_47a77_row0_col5\" class=\"data row0 col5\" >\n",
       "The City of Boston has a City Archaeologist on staff to oversee any lots of land to be developed for historical artifacts and significance, and to manage the archaeological remains located on public land in Boston, and also has a City Archaeology Program and an Archaeology Laboratory, Education and Curation Center. The Freedom House Photographs Collection contains over 2,000 images of Roxbury people, places and events, 1950–1975 (Archives and Special Collections of the Northeastern University Libraries in Boston, MA).</td>\n",
       "      <td id=\"T_47a77_row0_col6\" class=\"data row0 col6\" >1.000000</td>\n",
       "      <td id=\"T_47a77_row0_col7\" class=\"data row0 col7\" >Assistant A provides a more comprehensive answer, listing several resources available in Boston for studying and preserving historical artifacts and remains. These include the Boston Historical Society, Boston Mapjunction, the City of Boston Archaeology Program and Lab, the Freedom House Photographs Collection, and Vital Records of Boston. This answer is more detailed and provides a wider range of resources for the user to explore.\n",
       "\n",
       "Assistant B, on the other hand, only mentions the City of Boston Archaeology Program and Lab and the Freedom House Photographs Collection. While these are relevant resources, the answer lacks the depth and variety of Assistant A's response.\n",
       "\n",
       "Therefore, based on the depth, variety, and level of detail in the responses, Assistant A's answer is superior.\n",
       "\n",
       "Final Verdict: [[A]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2c6687610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's see the last one\n",
    "display_eval_df(\n",
    "    question=data_entry[\"question\"],\n",
    "    source=data_entry[\"source\"],\n",
    "    answer_a=data_entry[\"answers\"][0],\n",
    "    answer_b=data_entry[\"answers\"][1],\n",
    "    result=final_eval_result,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35956034-42aa-4bb7-8107-f979cd82c7e3",
   "metadata": {},
   "source": [
    "#### Special Care To The Fine-Tuning JSONL\n",
    "\n",
    "Since there are two evaluations (one for original order of presentation of the LLM answers and another for a flipped ordering), we need to be careful to choose the correct one to keep in our fine-tuning dataset. What this means is that we need to pick off the correct events that were collected by our `OpenAIFineTuningHandler` and then only use those to prepare the JSONL which we will pass to OpenAI's fine-tuning API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d6e45-b6c6-4694-8aee-fbba853d57a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 150 examples to pairwise_finetuning_events.jsonl\n"
     ]
    }
   ],
   "source": [
    "main_finetuning_handler.save_finetuning_events(\n",
    "    \"pairwise_finetuning_events.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e05e278-0f94-4aa1-a745-6ad02f7ee99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Get the fine_tuning_examples master dataset\n",
    "with open(\"pairwise_finetuning_events.jsonl\") as f:\n",
    "    combined_finetuning_events = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209391b1-a858-47e3-b2a6-7513c147c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_events = (\n",
    "    []\n",
    ")  # for storing events using original order of presentation\n",
    "flipped_finetuning_events = (\n",
    "    []\n",
    ")  # for storing events using flipped order of presentation\n",
    "\n",
    "for ix, event in enumerate(combined_finetuning_events):\n",
    "    if ix % 2 == 0:  # we always do original ordering first\n",
    "        finetuning_events += [event]\n",
    "    else:  # then we flip order and have GPT-4 make another judgement\n",
    "        flipped_finetuning_events += [event]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd58a37-ca96-41f5-9c07-9db11e791f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(finetuning_events) == len(flipped_finetuning_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec6e0a5-a8a3-4270-a514-a80f1e2aaa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to pick which of the chat_histories to keep\n",
    "resolved_finetuning_events = []\n",
    "for ix, data_entry in enumerate(train_dataset):\n",
    "    if data_entry[\"evaluations\"][0][\"source\"] == \"original\":\n",
    "        resolved_finetuning_events += [finetuning_events[ix]]\n",
    "    elif data_entry[\"evaluations\"][0][\"source\"] == \"flipped\":\n",
    "        resolved_finetuning_events += [flipped_finetuning_events[ix]]\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e56e30-9676-418d-bb57-a0ed90909b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"resolved_pairwise_finetuning_events.jsonl\", \"w\") as outfile:\n",
    "    for entry in resolved_finetuning_events:\n",
    "        print(json.dumps(entry), file=outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb16d0-15b1-45f3-96b0-3b51574d1626",
   "metadata": {},
   "source": [
    "## Step 2 Perform knowledge distillation\n",
    "\n",
    "Okay, it's now time to distill some knowledge from GPT-4 to GPT-3.5 To do this, we will make use of the `OpenAIFinetuneEngine` class as well as the `resolved_pairwise_finetuning_events.jsonl` file that we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab328da-c235-4ae6-a7ff-4315fa0b07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning import OpenAIFinetuneEngine\n",
    "\n",
    "finetune_engine = OpenAIFinetuneEngine(\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"resolved_pairwise_finetuning_events.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd14ca3-97c5-4a1c-a257-6ed183b09968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 72\n",
      "First example:\n",
      "{'role': 'system', 'content': \"Please act as an impartial judge and evaluate the quality of the responses provided by two AI question-answering assistants to the user question perhaps with added reference which are displayed below. You should choose the assistant that follows the user’s instructions and answers the user’s question better using the provided context. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: '[[A]]' if assistant A is better, '[[B]]' if assistant B is better, and '[[C]]' for a tie.\\n\"}\n",
      "{'role': 'user', 'content': \"[User Question]\\nWhat event in 1906 caused significant damage to San Francisco but was followed by a quick rebuild?\\n\\n[The Start of Reference]\\n=== Reconstruction ===\\nAlmost immediately after the quake re-planning and reconstruction plans were hatched to quickly rebuild the city. One of the more famous and ambitious plans, proposed before the fire, came from famed urban planner, Daniel Burnham. His bold plan called for Haussmann style avenues, boulevards, and arterial thoroughfares that radiated across the city, a massive civic center complex with classical structures, what would have been the largest urban park in the world, stretching from Twin Peaks to Lake Merced with a large athenaeum at its peak, and various other proposals. This plan was dismissed by critics (both at the time and now), as impractical and unrealistic to municipal supply and demand. Property owners and the Real Estate industry were against the idea as well due to the amounts of their land the city would have to purchase to realize such proposals. While the original street grid was restored, many of Burnham's proposals eventually saw the light of day such ...\\n[The End of Reference]\\n\\n[The Start of Assistant A’s Answer]\\n\\n\\nThe 1906 earthquake and fire caused significant damage to San Francisco, with over 80% of the city destroyed, including almost all of the downtown core. The earthquake, which had a magnitude of 7.8 on the Richter scale, resulted from the rupture of over 270 miles of the San Andreas Fault, centered immediately offshore of San Francisco. Water mains ruptured throughout the city, and the fires that followed burned out of control for days, trapping many residents and causing widespread destruction. Despite the devastation, the city quickly rebuilt, with many of the buildings constructed after the disaster being more earthquake-resistant than those that had come before. The official death toll from this event was initially reported as 478, but was later revised to over 3,000.\\n[The End of Assistant A’s Answer]\\n\\n[The Start of Assistant B’s Answer]\\n1906 earthquake and fire.\\n[The End of Assistant B’s Answer]\"}\n",
      "{'role': 'assistant', 'content': \"Assistant A provides a detailed and comprehensive response to the user's question, explaining the event that caused significant damage to San Francisco in 1906, which was the earthquake and fire. It also provides additional information about the earthquake's magnitude, the extent of the damage, and the city's quick rebuilding efforts. On the other hand, Assistant B's response is very brief and only mentions the earthquake and fire, without providing any additional information or context. Therefore, Assistant A's response is more helpful, relevant, accurate, and detailed. \\n\\nFinal Verdict: [[A]]\"}\n",
      "No errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 579, 1198\n",
      "mean / median: 818.9305555555555, 772.0\n",
      "p5 / p95: 625.9, 1076.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 66, 248\n",
      "mean / median: 129.26388888888889, 117.5\n",
      "p5 / p95: 81.0, 193.9\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~58963 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~176889 tokens\n",
      "As of August 22, 2023, fine-tuning gpt-3.5-turbo is $0.008 / 1K Tokens.\n",
      "This means your total cost for training will be $0.471704 per epoch.\n"
     ]
    }
   ],
   "source": [
    "finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882690c-b072-406c-a3d2-91ffca762a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-jLxZggQbHz2F98IlhQEI9KIw at 0x2e6b91170> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-jLxZggQbHz2F98IlhQEI9KIw\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1698817329,\n",
       "  \"finished_at\": 1698817949,\n",
       "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:llamaindex::8FyRSSOl\",\n",
       "  \"organization_id\": \"org-1ZDAvajC6v2ZtAP9hLEIsXRz\",\n",
       "  \"result_files\": [\n",
       "    \"file-qLTnxGSZX2rHP0Q7wJIDDNWX\"\n",
       "  ],\n",
       "  \"status\": \"succeeded\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-xsAaOBjQ949ti0qk1xHHLOiF\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 3\n",
       "  },\n",
       "  \"trained_tokens\": 176457,\n",
       "  \"error\": null\n",
       "}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can check the status of our current job as follows\n",
    "# This may take some time ...\n",
    "finetune_engine.get_current_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e631ccf-b4f5-478d-b112-52dc88ffed1e",
   "metadata": {},
   "source": [
    "## 3 Evaluate The Fine-Tuned GPT-3.5 Judge On The Test Dataset\n",
    "\n",
    "Now that we have our fine-tuned GPT-3.5, let's see how well it performs on a test set. But first, remember that we said we'd hold off on creating the `test_dataset` until the time comes that we need it? Well, that time is now. So we will repeat the process of creating the `train_dataset` here but instead now for the `test_dataset`.\n",
    "\n",
    "NOTE: generating these answers and evaluations will take some time. You have the option of loading `test_qa_complete.jsonl` which has all the evaluations from the three considered LLM judges. You can load that as `test_dataset` and run the code found in the Metrics subsection below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155fba3-a4bd-4822-b2ba-93c8eec428b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [28:23<00:00, 26.62s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Use Llama-2 and Mistral LLMs to generate the answers to the test queries\n",
    "test_dataset = []\n",
    "for q in tqdm.tqdm(test_questions):\n",
    "    # randomly select two LLMs to generate answers to this q\n",
    "    model_versus = random.sample(list(test_query_engines.items()), 2)\n",
    "\n",
    "    # data for this q\n",
    "    data_entry = {\"question\": q}\n",
    "    responses = []\n",
    "    source = None\n",
    "\n",
    "    # generate answers\n",
    "    for name, engine in model_versus:\n",
    "        response = engine.query(q)\n",
    "        response_struct = {}\n",
    "        response_struct[\"model\"] = name\n",
    "        response_struct[\"text\"] = str(response)\n",
    "        if source is not None:\n",
    "            assert source == response.source_nodes[0].node.text[:1000] + \"...\"\n",
    "        else:\n",
    "            source = response.source_nodes[0].node.text[:1000] + \"...\"\n",
    "        responses.append(response_struct)\n",
    "\n",
    "    data_entry[\"answers\"] = responses\n",
    "    data_entry[\"source\"] = source\n",
    "    test_dataset.append(data_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb666db-5f57-49b4-9e96-18c0de17beac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [43:21<00:00, 40.66s/it]\n"
     ]
    }
   ],
   "source": [
    "# get the gpt-4 judgments on the Mistal and Llama-2 answers\n",
    "for data_entry in tqdm.tqdm(test_dataset):\n",
    "    final_eval_result = await gpt4_judge.aevaluate(\n",
    "        query=data_entry[\"question\"],\n",
    "        response=data_entry[\"answers\"][0][\"text\"],\n",
    "        second_response=data_entry[\"answers\"][1][\"text\"],\n",
    "        reference=data_entry[\"source\"],\n",
    "    )\n",
    "\n",
    "    # save final result\n",
    "    judgement = {}\n",
    "    judgement[\"llm\"] = \"gpt_4\"\n",
    "    judgement[\"score\"] = final_eval_result.score\n",
    "    judgement[\"text\"] = final_eval_result.response\n",
    "    judgement[\"source\"] = final_eval_result.pairwise_source\n",
    "    data_entry[\"evaluations\"] = [judgement]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a888e-865c-48de-a5c0-b98daa992904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [04:08<00:00,  3.88s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import EvaluationResult\n",
    "\n",
    "# use our fine-tuned GPT-3.5 to evaluate the answers\n",
    "ft_llm = finetune_engine.get_finetuned_model()\n",
    "\n",
    "\n",
    "ft_gpt_3p5_judge = PairwiseComparisonEvaluator(llm=ft_llm)\n",
    "\n",
    "for data_entry in tqdm.tqdm(test_dataset):\n",
    "    try:\n",
    "        final_eval_result = await ft_gpt_3p5_judge.aevaluate(\n",
    "            query=data_entry[\"question\"],\n",
    "            response=data_entry[\"answers\"][0][\"text\"],\n",
    "            second_response=data_entry[\"answers\"][1][\"text\"],\n",
    "            reference=data_entry[\"source\"],\n",
    "        )\n",
    "    except:\n",
    "        final_eval_result = EvaluationResult(\n",
    "            query=data_entry[\"question\"],\n",
    "            response=\"\",\n",
    "            passing=None,\n",
    "            score=0.5,\n",
    "            feedback=\"\",\n",
    "            pairwise_source=\"output-cannot-be-parsed\",\n",
    "        )\n",
    "\n",
    "    # save final result\n",
    "    judgement = {}\n",
    "    judgement[\"llm\"] = \"ft_gpt_3p5\"\n",
    "    judgement[\"score\"] = final_eval_result.score\n",
    "    judgement[\"text\"] = final_eval_result.response\n",
    "    judgement[\"source\"] = final_eval_result.pairwise_source\n",
    "    data_entry[\"evaluations\"] += [judgement]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2caba7-6e8e-4daa-9145-807f9670a6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [09:32<00:00,  8.95s/it]\n"
     ]
    }
   ],
   "source": [
    "# Similarly, use a non-fine-tuned judge to evaluate the answers\n",
    "gpt_3p5_llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "gpt_3p5_judge = PairwiseComparisonEvaluator(llm=gpt_3p5_llm)\n",
    "\n",
    "for data_entry in tqdm.tqdm(test_dataset):\n",
    "    try:\n",
    "        final_eval_result = await gpt_3p5_judge.aevaluate(\n",
    "            query=data_entry[\"question\"],\n",
    "            response=data_entry[\"answers\"][0][\"text\"],\n",
    "            second_response=data_entry[\"answers\"][1][\"text\"],\n",
    "            reference=data_entry[\"source\"],\n",
    "        )\n",
    "    except:\n",
    "        final_eval_result = EvaluationResult(\n",
    "            query=data_entry[\"question\"],\n",
    "            response=\"\",\n",
    "            passing=None,\n",
    "            score=0.5,\n",
    "            feedback=\"\",\n",
    "            pairwise_source=\"output-cannot-be-parsed\",\n",
    "        )\n",
    "\n",
    "    # save final result\n",
    "    judgement = {}\n",
    "    judgement[\"llm\"] = \"gpt_3p5\"\n",
    "    judgement[\"score\"] = final_eval_result.score\n",
    "    judgement[\"text\"] = final_eval_result.response\n",
    "    judgement[\"source\"] = final_eval_result.pairwise_source\n",
    "    data_entry[\"evaluations\"] += [judgement]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35120c06-3218-4317-ad40-b94df1d6ca14",
   "metadata": {},
   "source": [
    "### The Metrics\n",
    "\n",
    "Phew! Now that we have generated all of the LLM judges evaluations of the Llama-2/Mistral answers on the test queries. Let's now get a quantitative view on how close fine-tuned GPT-3.5 is to GPT-4.\n",
    "\n",
    "For this, we report several metrics, namely:\n",
    "- Agreement Rate with GPT-4 evaluations\n",
    "- Correlation to GPT-4 evaluations\n",
    "- Jaccard Similarity to GPT-4 evaluations\n",
    "\n",
    "We also report the \"inconclusive\" counts, which is when the LLM judge switches its decision after being presented with the flipped order of presentation of Llama-2 and Mistral answers. Higher inconclusive counts is an indication of the LLM judge being susceptible to position bias, which is no good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd3d10-087b-4055-9386-43dfbb3f968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63636af6-326c-41de-abdb-d54e7b59fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# store the scores and inconclusive booleans for each sample per LLM judge\n",
    "scores = {\"gpt_4\": [], \"gpt_3p5\": [], \"ft_gpt_3p5\": []}\n",
    "inconclusives = {\"gpt_4\": [], \"gpt_3p5\": [], \"ft_gpt_3p5\": []}\n",
    "\n",
    "for ix, d in enumerate(test_dataset):\n",
    "    for e in d[\"evaluations\"]:\n",
    "        scores[e[\"llm\"]].append(e[\"score\"])\n",
    "        inconclusives[e[\"llm\"]].append(\n",
    "            e[\"source\"] not in [\"original\", \"flipped\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b9267a-d427-4959-9f99-0c7cdf31e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORT_FMT_STR = (\n",
    "    \"{model}\\n\"\n",
    "    \"-----------------\\n\"\n",
    "    \"Number of inconclusives: {inconclusive}\\n\"\n",
    "    \"Number of agreements with GPT-4: {agreement} out of {total}\\n\"\n",
    "    \"Agreement rate: {agreement_rate}\\n\"\n",
    "    \"Correlation: {corr}\\n\"\n",
    "    \"Jaccard: {jacc}\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e99836-adc6-46c5-a69d-2fef4bfde4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3.5 w/ fine-tuning\n",
      "-----------------\n",
      "Number of inconclusives: 15\n",
      "Number of agreements with GPT-4: 41 out of 47\n",
      "Agreement rate: 0.8723404255319149\n",
      "Correlation: 0.765365523658036\n",
      "Jaccard: 0.773126734505088\n",
      "\n",
      "\n",
      "GPT-3.5 w/out fine-tuning\n",
      "-----------------\n",
      "Number of inconclusives: 24\n",
      "Number of agreements with GPT-4: 32 out of 38\n",
      "Agreement rate: 0.8421052631578947\n",
      "Correlation: 0.671929323262293\n",
      "Jaccard: 0.7308712958867757\n",
      "\n",
      "\n",
      "GPT-4\n",
      "-----------------\n",
      "Inconclusive Count: 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "# numpy conversion\n",
    "np_scores_gpt_4 = np.array(scores[\"gpt_4\"])\n",
    "np_scores_gpt_3p5 = np.array(scores[\"gpt_3p5\"])\n",
    "np_scores_ft_gpt_3p5 = np.array(scores[\"ft_gpt_3p5\"])\n",
    "\n",
    "# can only compare when both judges have non inconclusive results\n",
    "ft_mask = ~np.array(inconclusives[\"gpt_4\"]) * ~np.array(\n",
    "    inconclusives[\"ft_gpt_3p5\"]\n",
    ")\n",
    "no_ft_mask = ~np.array(inconclusives[\"gpt_4\"]) * ~np.array(\n",
    "    inconclusives[\"gpt_3p5\"]\n",
    ")\n",
    "\n",
    "# agreement rates\n",
    "agreement_ft = sum(np_scores_gpt_4[ft_mask] == np_scores_ft_gpt_3p5[ft_mask])\n",
    "agreement_rate_ft = agreement_ft / sum(ft_mask)\n",
    "agreement_no_ft = sum(\n",
    "    np_scores_gpt_4[no_ft_mask] == np_scores_gpt_3p5[no_ft_mask]\n",
    ")\n",
    "agreement_rate_no_ft = agreement_no_ft / sum(no_ft_mask)\n",
    "\n",
    "# correlations\n",
    "corr_ft = np.corrcoef(np_scores_gpt_4[ft_mask], np_scores_ft_gpt_3p5[ft_mask])[\n",
    "    0, 1\n",
    "]\n",
    "corr_no_ft = np.corrcoef(\n",
    "    np_scores_gpt_4[no_ft_mask], np_scores_gpt_3p5[no_ft_mask]\n",
    ")[0, 1]\n",
    "\n",
    "# jaccard\n",
    "jaccard_ft = jaccard_score(\n",
    "    np_scores_gpt_4[ft_mask].astype(str),\n",
    "    np_scores_ft_gpt_3p5[ft_mask].astype(str),\n",
    "    average=\"weighted\",\n",
    ")\n",
    "jaccard_no_ft = jaccard_score(\n",
    "    np_scores_gpt_4[no_ft_mask].astype(str),\n",
    "    np_scores_gpt_3p5[no_ft_mask].astype(str),\n",
    "    average=\"weighted\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    REPORT_FMT_STR.format(\n",
    "        model=\"GPT-3.5 w/ fine-tuning\",\n",
    "        inconclusive=sum(inconclusives[\"ft_gpt_3p5\"]),\n",
    "        agreement=agreement_ft,\n",
    "        total=sum(ft_mask),\n",
    "        agreement_rate=agreement_rate_ft,\n",
    "        corr=corr_ft,\n",
    "        jacc=jaccard_ft,\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    REPORT_FMT_STR.format(\n",
    "        model=\"GPT-3.5 w/out fine-tuning\",\n",
    "        inconclusive=sum(inconclusives[\"gpt_3p5\"]),\n",
    "        agreement=agreement_no_ft,\n",
    "        total=sum(no_ft_mask),\n",
    "        agreement_rate=agreement_rate_no_ft,\n",
    "        corr=corr_no_ft,\n",
    "        jacc=jaccard_no_ft,\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    f\"GPT-4\\n-----------------\\nInconclusive Count: {sum(inconclusives['gpt_4'])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dece60-e813-4002-986f-ad3f95f7aa7e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "From the above numbers we see that fine-tuning a GPT-3.5 judge yields higher agreement scores, correlation, and jaccard similarity than a non-fine-tuned GPT-3.5 judge. What's more is that we see the inconclusive counts go down after fine-tuning as well. Overall, we see that fine-tuning here has helped us to get a GPT-3.5 judge that is closer to a GPT-4 judge (and thus by proxy, closer to human judgements) and at the same time helped remedy the position bias that a non-fine-tuned GPT-3.5 would have otherwise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_3.10",
   "language": "python",
   "name": "llama_index_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
