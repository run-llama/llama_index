{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24103c51",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/agent/openai_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cea58c-48bc-4af6-8358-df9695659983",
   "metadata": {},
   "source": [
    "# Function Calling Mistral Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673df1fe-eb6c-46ea-9a73-a96e7ae7942e",
   "metadata": {},
   "source": [
    "This notebook shows you how to use our Mistral agent, powered by function calling capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b7bc2e-606f-411a-9490-fcfab9236dfc",
   "metadata": {},
   "source": [
    "## Initial Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e80e5b-aaee-4f23-b338-7ae62b08141f",
   "metadata": {},
   "source": [
    "Let's start by importing some simple building blocks.  \n",
    "\n",
    "The main thing we need is:\n",
    "1. the OpenAI API (using our own `llama_index` LLM class)\n",
    "2. a place to keep conversation history \n",
    "3. a definition for tools that our agent can use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41101795",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ü¶ô.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4985c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-mistralai\n",
    "%pip install llama-index-embeddings-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61c873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d47283b-025e-4874-88ed-76245b22f82e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Sequence, List\n",
    "\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe08eb1-e638-4c00-9103-5c305bfacccf",
   "metadata": {},
   "source": [
    "Let's define some very simple calculator tools for our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dd3c4a6-f3e0-46f9-ad3b-7ba57d1bc992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfcfb78b-7d4f-48d9-8d4c-ffcded23e7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4becf171-6632-42e5-bdec-918a00934696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = MistralAI(model=\"mistral-large-latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d30b8-6405-4187-a9ed-6146dcc42167",
   "metadata": {},
   "source": [
    "## Initialize Mistral Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ca3fd-6711-4c0c-a853-d868dd14b484",
   "metadata": {},
   "source": [
    "Here we initialize a simple Mistral agent with calculator functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38ab3938-1138-43ea-b085-f430b42f5377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools([multiply_tool, add_tool], llm=llm, verbose=True)\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de28d3e1-c48d-4d9a-8fca-a5c0863cb1ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is (121 * 4) + 3?\n",
      "MESSAGES: [ChatMessage(role='user', content='What is (121 * 4) + 3?', name=None, tool_calls=None)]\n",
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\"a\": 121, \"b\": 4}\n",
      "MESSAGES: [ChatMessage(role='user', content='What is (121 * 4) + 3?', name=None, tool_calls=None), ChatMessage(role='assistant', content='', name=None, tool_calls=[ToolCall(id='null', type=<ToolType.function: 'function'>, function=FunctionCall(name='multiply', arguments='{\"a\": 121, \"b\": 4}')), ToolCall(id='null', type=<ToolType.function: 'function'>, function=FunctionCall(name='add', arguments='{\"a\": 484, \"b\": 3}'))]), ChatMessage(role='tool', content='484', name=None, tool_calls=None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is (121 * 4) + 3?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500cbee4",
   "metadata": {},
   "source": [
    "### Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fd1cad5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is (121 * 3) + 5?\n",
      "MESSAGES: [ChatMessage(role='user', content='What is (121 * 3) + 5?', name=None, tool_calls=None)]\n",
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\"a\": 121, \"b\": 3}\n",
      "MESSAGES: [ChatMessage(role='user', content='What is (121 * 3) + 5?', name=None, tool_calls=None), ChatMessage(role='assistant', content='', name=None, tool_calls=[ToolCall(id='null', type=<ToolType.function: 'function'>, function=FunctionCall(name='multiply', arguments='{\"a\": 121, \"b\": 3}')), ToolCall(id='null', type=<ToolType.function: 'function'>, function=FunctionCall(name='add', arguments='{\"a\": 363, \"b\": 5}'))]), ChatMessage(role='tool', content='363', name=None, tool_calls=None)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMistralAPIStatusException\u001b[0m                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/mistralai/client.py:131\u001b[0m, in \u001b[0;36mMistralClient._request\u001b[0;34m(self, method, json, path, stream, attempt)\u001b[0m\n\u001b[1;32m    124\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    125\u001b[0m             method,\n\u001b[1;32m    126\u001b[0m             url,\n\u001b[1;32m    127\u001b[0m             headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    128\u001b[0m             json\u001b[38;5;241m=\u001b[39mjson,\n\u001b[1;32m    129\u001b[0m         )\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/mistralai/client.py:72\u001b[0m, in \u001b[0;36mMistralClient._check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, response: Response) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_response_status_codes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     json_response: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/mistralai/client.py:50\u001b[0m, in \u001b[0;36mMistralClient._check_response_status_codes\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01min\u001b[39;00m RETRY_STATUS_CODES:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MistralAPIStatusException\u001b[38;5;241m.\u001b[39mfrom_response(\n\u001b[1;32m     51\u001b[0m         response,\n\u001b[1;32m     52\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n",
      "\u001b[0;31mMistralAPIStatusException\u001b[0m: Status: 500. Message: {\"object\":\"error\",\"message\":\"Service unavailable.\",\"type\":\"internal_server_error\",\"param\":null,\"code\":\"1000\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMistralAPIStatusException\u001b[0m                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/mistralai/client.py:131\u001b[0m, in \u001b[0;36mMistralClient._request\u001b[0;34m(self, method, json, path, stream, attempt)\u001b[0m\n\u001b[1;32m    124\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    125\u001b[0m             method,\n\u001b[1;32m    126\u001b[0m             url,\n\u001b[1;32m    127\u001b[0m             headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    128\u001b[0m             json\u001b[38;5;241m=\u001b[39mjson,\n\u001b[1;32m    129\u001b[0m         )\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/mistralai/client.py:72\u001b[0m, in \u001b[0;36mMistralClient._check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, response: Response) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_response_status_codes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     json_response: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/mistralai/client.py:50\u001b[0m, in \u001b[0;36mMistralClient._check_response_status_codes\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01min\u001b[39;00m RETRY_STATUS_CODES:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MistralAPIStatusException\u001b[38;5;241m.\u001b[39mfrom_response(\n\u001b[1;32m     51\u001b[0m         response,\n\u001b[1;32m     52\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n",
      "\u001b[0;31mMistralAPIStatusException\u001b[0m: Status: 500. Message: {\"object\":\"error\",\"message\":\"Service unavailable.\",\"type\":\"internal_server_error\",\"param\":null,\"code\":\"1000\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is (121 * 3) + 5?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(response))\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama-index-core/llama_index/core/callbacks/utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama-index-core/llama_index/core/agent/runner/base.py:596\u001b[0m, in \u001b[0;36mAgentRunner.chat\u001b[0;34m(self, message, chat_history, tool_choice)\u001b[0m\n\u001b[1;32m    591\u001b[0m     tool_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_tool_choice\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    593\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mAGENT_STEP,\n\u001b[1;32m    594\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mMESSAGES: [message]},\n\u001b[1;32m    595\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 596\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatResponseMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWAIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_response, AgentChatResponse)\n\u001b[1;32m    603\u001b[0m     e\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: chat_response})\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama-index-core/llama_index/core/instrumentation/dispatcher.py:100\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_drop(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m, err\u001b[38;5;241m=\u001b[39me, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama-index-core/llama_index/core/agent/runner/base.py:533\u001b[0m, in \u001b[0;36mAgentRunner._chat\u001b[0;34m(self, message, chat_history, tool_choice, mode)\u001b[0m\n\u001b[1;32m    530\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(AgentChatWithStepStartEvent())\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# pass step queue in as argument, assume step executor is stateless\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cur_step_output\u001b[38;5;241m.\u001b[39mis_last:\n\u001b[1;32m    538\u001b[0m         result_output \u001b[38;5;241m=\u001b[39m cur_step_output\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama-index-core/llama_index/core/instrumentation/dispatcher.py:100\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_drop(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m, err\u001b[38;5;241m=\u001b[39me, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama-index-core/llama_index/core/agent/runner/base.py:382\u001b[0m, in \u001b[0;36mAgentRunner._run_step\u001b[0;34m(self, task_id, step, input, mode, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# TODO: figure out if you can dynamically swap in different step executors\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# not clear when you would do that by theoretically possible\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mWAIT:\n\u001b[0;32m--> 382\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mSTREAM:\n\u001b[1;32m    384\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_worker\u001b[38;5;241m.\u001b[39mstream_step(step, task, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama-index-core/llama_index/core/callbacks/utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama-index-core/llama_index/core/agent/function_calling/step.py:248\u001b[0m, in \u001b[0;36mFunctionCallingAgentWorker.run_step\u001b[0;34m(self, step, task, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m tools \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tools(task\u001b[38;5;241m.\u001b[39minput)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# get response and tool call (if exists)\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_with_tool\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_all_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m tool_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39m_get_tool_call_from_response(\n\u001b[1;32m    255\u001b[0m     response, error_on_no_tool_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    256\u001b[0m )\n\u001b[1;32m    257\u001b[0m task\u001b[38;5;241m.\u001b[39mextra_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mput(response\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama-index-integrations/llms/llama-index-llms-mistralai/llama_index/llms/mistralai/base.py:465\u001b[0m, in \u001b[0;36mMistralAI.chat_with_tool\u001b[0;34m(self, tools, user_msg, chat_history, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_msg:\n\u001b[1;32m    463\u001b[0m     messages\u001b[38;5;241m.\u001b[39mappend(user_msg)\n\u001b[0;32m--> 465\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama-index-core/llama_index/core/llms/callbacks.py:130\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    116\u001b[0m     LLMChatStartEvent(\n\u001b[1;32m    117\u001b[0m         model_dict\u001b[38;5;241m=\u001b[39m_self\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m     )\n\u001b[1;32m    121\u001b[0m )\n\u001b[1;32m    122\u001b[0m event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m    123\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m    124\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m     },\n\u001b[1;32m    129\u001b[0m )\n\u001b[0;32m--> 130\u001b[0m f_return_val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_gen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResponseGen:\n",
      "File \u001b[0;32m~/Programming/gpt_index/llama-index-integrations/llms/llama-index-llms-mistralai/llama_index/llms/mistralai/base.py:226\u001b[0m, in \u001b[0;36mMistralAI.chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# messages = [\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m#     mistral_chatmessage(role=x.role, content=x.content) for x in messages\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[1;32m    225\u001b[0m all_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_all_kwargs(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 226\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m tool_calls \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mtool_calls\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ChatResponse(\n\u001b[1;32m    231\u001b[0m     message\u001b[38;5;241m=\u001b[39mChatMessage(\n\u001b[1;32m    232\u001b[0m         role\u001b[38;5;241m=\u001b[39mMessageRole\u001b[38;5;241m.\u001b[39mASSISTANT,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m     raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(response),\n\u001b[1;32m    239\u001b[0m )\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/mistralai/client.py:201\u001b[0m, in \u001b[0;36mMistralClient.chat\u001b[0;34m(self, messages, model, tools, temperature, max_tokens, top_p, random_seed, safe_mode, safe_prompt, tool_choice, response_format)\u001b[0m\n\u001b[1;32m    185\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_chat_request(\n\u001b[1;32m    186\u001b[0m     messages,\n\u001b[1;32m    187\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m     response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[1;32m    197\u001b[0m )\n\u001b[1;32m    199\u001b[0m single_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, request, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m single_response:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatCompletionResponse(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m MistralException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo response received\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/mistralai/client.py:150\u001b[0m, in \u001b[0;36mMistralClient._request\u001b[0;34m(self, method, json, path, stream, attempt)\u001b[0m\n\u001b[1;32m    147\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(backoff)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Retry as a generator\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(method, json, path, stream\u001b[38;5;241m=\u001b[39mstream, attempt\u001b[38;5;241m=\u001b[39mattempt):\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m r\n",
      "File \u001b[0;32m~/Programming/gpt_index/.venv/lib/python3.10/site-packages/mistralai/client.py:147\u001b[0m, in \u001b[0;36mMistralClient._request\u001b[0;34m(self, method, json, path, stream, attempt)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MistralAPIStatusException\u001b[38;5;241m.\u001b[39mfrom_response(response, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    146\u001b[0m backoff \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mattempt  \u001b[38;5;66;03m# exponential backoff\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackoff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Retry as a generator\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(method, json, path, stream\u001b[38;5;241m=\u001b[39mstream, attempt\u001b[38;5;241m=\u001b[39mattempt):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is (121 * 3) + 5?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538bf32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ToolOutput(content='363', tool_name='multiply', raw_input={'args': (), 'kwargs': {'a': 121, 'b': 3}}, raw_output=363), ToolOutput(content='405', tool_name='add', raw_input={'args': (), 'kwargs': {'a': 363, 'b': 42}}, raw_output=405)]\n"
     ]
    }
   ],
   "source": [
    "# inspect sources\n",
    "print(response.sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33983c",
   "metadata": {},
   "source": [
    "### Async Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1fc974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TURN 1\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\n",
      "  \"a\": 121,\n",
      "  \"b\": 3\n",
      "}\n",
      "Got output: 363\n",
      "========================\n",
      "\n",
      "STARTING TURN 2\n",
      "---------------\n",
      "\n",
      "121 multiplied by 3 is equal to 363.\n"
     ]
    }
   ],
   "source": [
    "response = await agent.achat(\"What is 121 * 3?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabfdf01-8d63-43ff-b06e-a3059ede2ddf",
   "metadata": {},
   "source": [
    "## Mistral Agent over RAG Pipeline\n",
    "\n",
    "Build a Mistral agent over a simple 10K document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48120dd4-7f50-426f-bc7e-a903e090d32e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-23 11:13:41--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8002::154, 2606:50c0:8001::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1880483 (1.8M) [application/octet-stream]\n",
      "Saving to: ‚Äòdata/10k/uber_2021.pdf‚Äô\n",
      "\n",
      "data/10k/uber_2021. 100%[===================>]   1.79M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2024-03-23 11:13:41 (19.3 MB/s) - ‚Äòdata/10k/uber_2021.pdf‚Äô saved [1880483/1880483]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/10k/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48c0cf98-3f10-4599-8437-d88dc89cefad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "\n",
    "embed_model = MistralAIEmbedding()\n",
    "query_llm = MistralAI(model=\"mistral-medium\")\n",
    "\n",
    "# load data\n",
    "uber_docs = SimpleDirectoryReader(input_files=[\"./data/10k/uber_2021.pdf\"]).load_data()\n",
    "# build index\n",
    "uber_index = VectorStoreIndex.from_documents(uber_docs, embed_model=embed_model)\n",
    "uber_engine = uber_index.as_query_engine(similarity_top_k=3, llm=query_llm)\n",
    "query_engine_tool = QueryEngineTool(\n",
    "    query_engine=uber_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"uber_10k\",\n",
    "        description=(\n",
    "            \"Provides information about Uber financials for year 2021. \"\n",
    "            \"Use a detailed plain text question as input to the tool.\"\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebfdaf80-e5e1-4c60-b556-20558da3d5e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools([query_engine_tool], llm=llm, verbose=True)\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58c53f2a-0a3f-4abe-b8b6-97a974ec7546",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me the risk factors for Uber?\n",
      "MESSAGES: [ChatMessage(role='user', content='Tell me the risk factors for Lyft?', name=None, tool_calls=None), ChatMessage(role='assistant', content=\"I'm sorry for the confusion, but the function provided is related to Uber's financials for the year 2021. I don't have information about Lyft's risk factors. If you have any questions about Uber's financials, I'd be happy to help with that.\", name=None, tool_calls=None), ChatMessage(role='user', content='Tell me the risk factors for Uber?', name=None, tool_calls=None)]\n",
      "=== Calling Function ===\n",
      "Calling function: uber_10k with args: {\"input\": \"What are the risk factors for Uber?\"}\n",
      "MESSAGES: [ChatMessage(role='system', content=\"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\", name=None, tool_calls=None), ChatMessage(role='user', content='Context information is below.\\n---------------------\\npage_label: 14\\nfile_path: data/10k/uber_2021.pdf\\n\\n‚Ä¢We may fail to offer autonomous vehicle technologies on our platform, fail to offer such technologies on our platform before our competitors, or suchtechnologies\\n may fail to perform as expected, may be inferior to those offered by our competitors, or may be perceived as less safe than those offered bycompetitors or non-autonomous v\\nehicles.‚Ä¢\\nOur  business  depends  on  retaining  and  attracting  high-quality  personnel,  and  continued  attrition,  future  attrition,  or  unsuccessful  succession  planningcould adversely affec\\nt our business.‚Ä¢\\nWe may experience security or data privacy breaches or other unauthorized or improper access to, use of, alteration of or destruction of our proprietary orconfidential data, employee \\ndata, or platform user data.‚Ä¢\\nCyberattacks,  including  computer  malware,  ransomware,  viruses,  spamming,  and  phishing  attacks  could  harm  our  reputation,  business,  and  operatingresults.\\n‚Ä¢\\nWe are subject to climate change risks, including physical and transitional risks, and if we are unable to manage such risks, our business may be adverselyimpacted.\\n‚Ä¢\\nWe have made climate related commitments that require us to invest significant effort, resources, and management time and circumstances may arise,including those bey\\nond our control, that may require us to revise the contemplated timeframes for implementing these commitments.‚Ä¢\\nWe  rely  on  third  parties  maintaining  open  marketplaces  to  distribute  our  platform  and  to  provide  the  software  we  use  in  certain  of  our  products  andofferings. If such third pa\\nrties interfere with the distribution of our products or offerings or with our use of such software, our business would be adverselyaffected.\\n‚Ä¢\\nWe will require additional capital to support the growth of our business, and this capital might not be available on reasonable terms or at all.‚Ä¢\\nIf  we  are  unable  to  successfully  identify,  acquire  and  integrate  suitable  businesses,  our  operating  results  and  prospects  could  be  harmed,  and  anybusinesses we acq\\nuire may not perform as expected or be effectively integrated.‚Ä¢\\nWe may continue to be blocked from or limited in providing or operating our products and offerings in certain jurisdictions, and may be required tomodify our business model in those \\njurisdictions as a result.‚Ä¢\\nOur business is subject to numerous legal and regulatory risks that could have an adverse impact on our business and future prospects.‚Ä¢\\nOur business is subject to extensive government regulation and oversight relating to the provision of payment and financial services.‚Ä¢\\nWe face risks related to our collection, use, transfer, disclosure, and other processing of data, which could result in investigations, inquiries, litigation,fines, legislative and \\nregulatory action, and negative press about our privacy and data protection practices.‚Ä¢\\nIf we are unable to protect our intellectual property, or if third parties are successful in claiming that we are misappropriating the intellectual property ofothers, we may incur signifi\\ncant expense and our business may be adversely affected.‚Ä¢\\nThe  market  price  of  our  common  stock  has  been,  and  may  continue  to  be,  volatile  or  may  decline  steeply  or  suddenly  regardless  of  our  operatingperformance,\\n and we may not be able to meet investor or analyst expectations. You may not be able to resell your shares at or above the price you paidand may lose all or part o\\nf your investment.Risks Related to Our Business\\nGeneral Economic Risks\\nThe\\n coronavirus (‚ÄúCOVID-19‚Äù) pandemic and the impact of actions to mitigate the pandemic have adversely impacted and could continue to adversely impactour business, financial condition and results of operations.\\nIn\\n March 2020, the World Health Organization declared the outbreak of COVID-19 a pandemic. Since then, in an attempt to limit the spread of the virus,various\\n governments  around  the  world  have  implemented,  lifted,  and  in  some  regions  reinstated  travel  restrictions,  business  restrictions,  school  closures,limitations on social o\\nr public gatherings, and other measures that have, and may continue to have, an adverse impact on our business and operations, including, forexample,\\n by reducing the demand for our Mobility offerings globally, and affecting travel behavior and demand. Even as such restrictions are being lifted andmany regions around\\n the world are making progress in their recovery from the pandemic, end-user behavior and demand for our Mobility offering may not recoverto\\n pre-pandemic levels.\\n\\npage_label: 15\\nfile_path: data/10k/uber_2021.pdf\\n\\nMoreover,  even  after  shelter  at  home  orders  and  travel  advisories  are  lifted,demand\\n for our Mobility offering may remain weak for a significant length of time and we cannot predict when and if our Mobility offering will return to pre-COVID-19 demand levels.\\nIn\\n addition, we cannot predict  the impact  the COVID-19 pandemic  will have on our business partners  and third-party  vendors, and we may be adverselyimpacted as\\n a result of the adverse impact our business partners and third-party vendors suffer. Additionally, concerns over the economic impact of the COVID-19pandemic\\n have caused extreme volatility in financial markets, which has and may continue to adversely impact our stock price and our ability to access capitalmarkets.\\n To the extent the COVID-19 pandemic adversely affects our business and financial results, it may also have the effect of heightening many of the otherrisks\\n described in this ‚ÄúRisk Factors‚Äù section. Any of the foregoing factors, or other cascading effects of the pandemic that are not currently foreseeable, couldadversely impact our busin\\ness, financial performance and condition, and results of operations.Operational Risks\\nOur business would be adversely affected if Driv\\ners were classified as employees, workers or quasi-employees.The\\n classification  of  Drivers  is  currently  being  challenged  in  courts,  by  legislators  and  by  government  agencies  in  the  United  States  and  abroad.  We  areinvolved\\n in numerous legal proceedings globally, including putative class and collective class action lawsuits, demands for arbitration, charges and claims beforeadministrative\\n agencies, and investigations or audits by labor, social security, and tax authorities that claim that Drivers should be treated as our employees (or asworkers\\n or  quasi-employees  where  those  statuses  exist),  rather  than  as  independent  contractors.  We  believe  that  Drivers  are  independent  contractors  because,among other\\n things, they can choose whether, when, and where to provide services on our platform, are free to provide services on our competitors‚Äô platforms, andprovide a vehicle to per\\nform services on our platform. Nevertheless, we may not be successful in defending the classification of Drivers in some or all jurisdictions.Furthermore, the costs associated with\\n defending, settling, or resolving pending and future lawsuits (including demands for arbitration) relating to the classificationof Drivers have been and may con\\ntinue to be material to our business.In addition, more than 150,000 Drive\\nrs in the United States who have entered into arbitration agreements with us have filed (or13\\n\\npage_label: 13\\nfile_path: data/10k/uber_2021.pdf\\n\\nWe have previously received significant media coverage andnegative\\n publicity  regarding  our  brand  and  reputation,  and  while  we  have  taken  significant  steps  to  rehabilitate  our  brand  and  reputation,  failure  tomaintain and enhance our b\\nrand and reputation will cause our business to suffer.‚Ä¢\\nOur historical workplace culture and forward-leaning approach created operational, compliance, and cultural challenges and our efforts to address thesechallenges may not be succ\\nessful.‚Ä¢\\nIf  we  are  unable  to  optimize  our  organizational  structure  or  effectively  manage  our  growth,  our  financial  performance  and  future  prospects  will  beadversely affected.\\n‚Ä¢\\nPlatform users may engage in, or be subject to, criminal, violent, inappropriate, or dangerous activity that results in major safety incidents, which mayharm our ability to at\\ntract and retain Drivers, consumers, merchants, shippers, and carriers.‚Ä¢\\nWe  are  making  substantial  investments  in  new  offerings  and  technologies,  and  may  increase  such  investments  in  the  future.  These  new  ventures  areinherently risky, and we may nev\\ner realize any expected benefits from them.‚Ä¢\\nWe generate a significant percentage of our Gross Bookings from trips in large metropolitan areas, and these operations may be negatively affected byeconomic, social, weathe\\nr, and regulatory conditions or other circumstances, including COVID-19.11\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: What are the risk factors for Uber?\\nAnswer: ', name=None, tool_calls=None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Tell me the risk factors for Uber?\") \n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b6703-a1a7-4e58-a6c1-3ba452b8df94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v3",
   "language": "python",
   "name": "llama_index_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
