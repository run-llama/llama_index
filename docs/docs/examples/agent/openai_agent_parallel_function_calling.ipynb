{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81206313-0516-4c5c-a294-f1a4825fabe1",
   "metadata": {},
   "source": [
    "# Single-Turn Multi-Function Calling OpenAI Agents\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/agent/openai_agent_parallel_function_calling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ce8589-7b10-40be-807b-b39e66317649",
   "metadata": {},
   "source": [
    "With the latest OpenAI API (v. 1.1.0+), users can now execute multiple function calls within a single turn of `User` and `Agent` dialogue. We've updated our library to enable this new feature as well, and in this notebook we'll show you how it all works!\n",
    "\n",
    "NOTE: OpenAI refers to this as \"Parallel\" function calling, but the current implementation doesn't invoke parallel computations of the multiple function calls. So, it's \"parallelizable\" function calling in terms of our current implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-agent-openai\n",
    "%pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b839f-da37-401a-9815-53d791dce805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.tools import BaseTool, FunctionTool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e7ad14-a1a9-4e7a-91ca-85351398084a",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "If you've seen any of our previous notebooks on OpenAI Agents, then you're already familiar with the cookbook recipe that we have to follow here. But if not, or if you fancy a refresher then all we need to do (at a high level) are the following steps:\n",
    "\n",
    "1. Define a set of tools (we'll use `FunctionTool`) since Agents work with tools\n",
    "2. Define the `LLM` for the Agent\n",
    "3. Define a `OpenAIAgent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a905b412-8d9b-4187-8fba-6a9a64374ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc05ce0-e520-427a-9fcc-6eb3aefd300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560776f1-d48b-4b1b-b98a-16f3e3c1d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    [multiply_tool, add_tool], llm=llm, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1757ca-92f9-4a95-aa78-6f6910de75d7",
   "metadata": {},
   "source": [
    "### Sync mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aaf5cc-1482-4377-a88d-e10fbf521c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TURN 1\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\"a\": 121, \"b\": 3}\n",
      "Got output: 363\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: add with args: {\"a\": 363, \"b\": 42}\n",
      "Got output: 405\n",
      "========================\n",
      "\n",
      "STARTING TURN 2\n",
      "---------------\n",
      "\n",
      "The result of (121 * 3) + 42 is 405.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is (121 * 3) + 42?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1fcee-9f4c-4d2b-82b6-aa41388de062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TURN 1\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: add with args: {\"a\":363,\"b\":42}\n",
      "Got output: 405\n",
      "========================\n",
      "\n",
      "STARTING TURN 2\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = agent.stream_chat(\"What is (121 * 3) + 42?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0471ad80-0f9c-4be0-87a7-65eff1293d30",
   "metadata": {},
   "source": [
    "### Async mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee3ead7-4507-4886-81b5-4924bb422690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01344bdb-c06b-4f56-ab75-6b06eb7e7af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TURN 1\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: add with args: {\"a\":363,\"b\":42}\n",
      "Got output: 405\n",
      "========================\n",
      "\n",
      "STARTING TURN 2\n",
      "---------------\n",
      "\n",
      "The result of (121 * 3) + 42 is 405.\n"
     ]
    }
   ],
   "source": [
    "response = await agent.achat(\"What is (121 * 3) + 42?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe479ae-21fc-4e7b-95af-204985730d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TURN 1\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\"a\": 121, \"b\": 3}\n",
      "Got output: 363\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: add with args: {\"a\": 363, \"b\": 42}\n",
      "Got output: 405\n",
      "========================\n",
      "\n",
      "STARTING TURN 2\n",
      "---------------\n",
      "\n",
      "The result of (121 * 3) + 42 is 405."
     ]
    }
   ],
   "source": [
    "response = await agent.astream_chat(\"What is (121 * 3) + 42?\")\n",
    "\n",
    "response_gen = response.response_gen\n",
    "\n",
    "async for token in response.async_response_gen():\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2172bcb4-08b6-4cfc-b208-e38ce3a588bf",
   "metadata": {},
   "source": [
    "### Example from OpenAI docs\n",
    "\n",
    "Here's an example straight from the OpenAI [docs](https://platform.openai.com/docs/guides/function-calling/parallel-function-calling) on Parallel function calling. (Their example gets this done in 76 lines of code, whereas with the `llama_index` library you can get that down to about 18 lines.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6853d-8638-4a62-9dec-4f32fa22e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    if \"tokyo\" in location.lower():\n",
    "        return json.dumps(\n",
    "            {\"location\": location, \"temperature\": \"10\", \"unit\": \"celsius\"}\n",
    "        )\n",
    "    elif \"san francisco\" in location.lower():\n",
    "        return json.dumps(\n",
    "            {\"location\": location, \"temperature\": \"72\", \"unit\": \"fahrenheit\"}\n",
    "        )\n",
    "    else:\n",
    "        return json.dumps(\n",
    "            {\"location\": location, \"temperature\": \"22\", \"unit\": \"celsius\"}\n",
    "        )\n",
    "\n",
    "\n",
    "weather_tool = FunctionTool.from_defaults(fn=get_current_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a529de65-6360-444c-8f43-d9ebbb71b184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TURN 1\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: get_current_weather with args: {\"location\": \"San Francisco\", \"unit\": \"fahrenheit\"}\n",
      "Got output: {\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"}\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: get_current_weather with args: {\"location\": \"Tokyo\", \"unit\": \"fahrenheit\"}\n",
      "Got output: {\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"}\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: get_current_weather with args: {\"location\": \"Paris\", \"unit\": \"fahrenheit\"}\n",
      "Got output: {\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"}\n",
      "========================\n",
      "\n",
      "STARTING TURN 2\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "agent = OpenAIAgent.from_tools([weather_tool], llm=llm, verbose=True)\n",
    "response = agent.chat(\n",
    "    \"What's the weather like in San Francisco, Tokyo, and Paris?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549701a6-ed24-4aa1-b339-5ac7acc2b8a4",
   "metadata": {},
   "source": [
    "All of the above function calls that the Agent has done above were in a single turn of dialogue between the `Assistant` and the `User`. What's interesting is that an older version of GPT-3.5 is not quite advanced enough compared to is successor â€” it will do the above task in 3 separate turns. For the sake of demonstration, here it is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de05a75-93fd-42c9-b197-f31821aec121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TURN 1\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: get_current_weather with args: {\n",
      "  \"location\": \"San Francisco\"\n",
      "}\n",
      "Got output: {\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"}\n",
      "========================\n",
      "\n",
      "STARTING TURN 2\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: get_current_weather with args: {\n",
      "  \"location\": \"Tokyo\"\n",
      "}\n",
      "Got output: {\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"}\n",
      "========================\n",
      "\n",
      "STARTING TURN 3\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: get_current_weather with args: {\n",
      "  \"location\": \"Paris\"\n",
      "}\n",
      "Got output: {\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"}\n",
      "========================\n",
      "\n",
      "STARTING TURN 4\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "agent = OpenAIAgent.from_tools([weather_tool], llm=llm, verbose=True)\n",
    "response = agent.chat(\n",
    "    \"What's the weather like in San Francisco, Tokyo, and Paris?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b59b5-a21b-4197-a241-83e87415a25a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "And so, as you can see the `llama_index` library can handle multiple function calls (as well as a single function call) within a single turn of dialogue between the user and the OpenAI agent!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_3.10",
   "language": "python",
   "name": "llama_index_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
