{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91c998a5",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/agent/multi_document_agents-v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43497beb-817d-4366-9156-f4d7f0d44942",
   "metadata": {},
   "source": [
    "# Multi-Document Agents (V1)\n",
    "\n",
    "In this guide, you learn towards setting up a multi-document agent over the LlamaIndex documentation.\n",
    "\n",
    "This is an extension of V0 multi-document agents with the additional features:\n",
    "- Reranking during document (tool) retrieval\n",
    "- Query planning tool that the agent can use to plan \n",
    "\n",
    "\n",
    "We do this with the following architecture:\n",
    "\n",
    "- setup a \"document agent\" over each Document: each doc agent can do QA/summarization within its doc\n",
    "- setup a top-level agent over this set of document agents. Do tool retrieval and then do CoT over the set of tools to answer a question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77ac7184",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034a7661",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-agent-openai\n",
    "%pip install llama-index-readers-file\n",
    "%pip install llama-index-postprocessor-cohere-rerank\n",
    "%pip install llama-index-llms-openai\n",
    "%pip install llama-index-embeddings-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index llama-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e47ac-ec6d-48eb-93a3-0e1fcab22112",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be00aba-b6c5-4940-9825-81c5d2cd2f0b",
   "metadata": {},
   "source": [
    "## Setup and Download Data\n",
    "\n",
    "In this section, we'll load in the LlamaIndex documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49893d69-c106-4169-92c3-6b5b751066e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"docs.llamaindex.ai\"\n",
    "docs_url = \"https://docs.llamaindex.ai/en/latest/\"\n",
    "!wget -e robots=off --recursive --no-clobber --page-requisites --html-extension --convert-links --restrict-file-names=windows --domains {domain} --no-parent {docs_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c661cb62-1e18-410c-bc2e-e707b66596a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import UnstructuredReader\n",
    "\n",
    "reader = UnstructuredReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44feebd5-0430-4d73-9cb1-a3de73c1f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "all_files_gen = Path(\"./docs.llamaindex.ai/\").rglob(\"*\")\n",
    "all_files = [f.resolve() for f in all_files_gen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d837b4b-130c-493c-b62e-6662904c20ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_html_files = [f for f in all_files if f.suffix.lower() == \".html\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cddf0f5-3c5f-4d42-868d-54bedb12d02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "638"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_html_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1dd0cf-5da2-4ac0-bfd1-8f48921518c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "# TODO: set to higher value if you want more docs\n",
    "doc_limit = 100\n",
    "\n",
    "docs = []\n",
    "for idx, f in enumerate(all_html_files):\n",
    "    if idx > doc_limit:\n",
    "        break\n",
    "    print(f\"Idx {idx}/{len(all_html_files)}\")\n",
    "    loaded_docs = reader.load_data(file=f, split_documents=True)\n",
    "    # Hardcoded Index. Everything before this is ToC for all pages\n",
    "    start_idx = 72\n",
    "    loaded_doc = Document(\n",
    "        text=\"\\n\\n\".join([d.get_content() for d in loaded_docs[72:]]),\n",
    "        metadata={\"path\": str(f)},\n",
    "    )\n",
    "    print(loaded_doc.metadata[\"path\"])\n",
    "    docs.append(loaded_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189aaf4-2eb7-40bc-9e83-79ce4f221b4b",
   "metadata": {},
   "source": [
    "Define Global LLM + Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e56afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6e5e48-91b9-4701-a85d-d98c92323350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeef31a-fc25-4367-a5ba-945f81d04cf9",
   "metadata": {},
   "source": [
    "## Building Multi-Document Agents\n",
    "\n",
    "In this section we show you how to construct the multi-document agent. We first build a document agent for each document, and then define the top-level parent agent with an object index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976cd798-2e8d-474c-922a-51b12c5c6f36",
   "metadata": {},
   "source": [
    "### Build Document Agent for each Document\n",
    "\n",
    "In this section we define \"document agents\" for each document.\n",
    "\n",
    "We define both a vector index (for semantic search) and summary index (for summarization) for each document. The two query engines are then converted into tools that are passed to an OpenAI function calling agent.\n",
    "\n",
    "This document agent can dynamically choose to perform semantic search or summarization within a given document.\n",
    "\n",
    "We create a separate document agent for each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacdf3a7-cfe3-4c2b-9037-b28a065ed148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core import (\n",
    "    load_index_from_storage,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "async def build_agent_per_doc(nodes, file_base):\n",
    "    print(file_base)\n",
    "\n",
    "    vi_out_path = f\"./data/llamaindex_docs/{file_base}\"\n",
    "    summary_out_path = f\"./data/llamaindex_docs/{file_base}_summary.pkl\"\n",
    "    if not os.path.exists(vi_out_path):\n",
    "        Path(\"./data/llamaindex_docs/\").mkdir(parents=True, exist_ok=True)\n",
    "        # build vector index\n",
    "        vector_index = VectorStoreIndex(nodes)\n",
    "        vector_index.storage_context.persist(persist_dir=vi_out_path)\n",
    "    else:\n",
    "        vector_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=vi_out_path),\n",
    "        )\n",
    "\n",
    "    # build summary index\n",
    "    summary_index = SummaryIndex(nodes)\n",
    "\n",
    "    # define query engines\n",
    "    vector_query_engine = vector_index.as_query_engine(llm=llm)\n",
    "    summary_query_engine = summary_index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\", llm=llm\n",
    "    )\n",
    "\n",
    "    # extract a summary\n",
    "    if not os.path.exists(summary_out_path):\n",
    "        Path(summary_out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        summary = str(\n",
    "            await summary_query_engine.aquery(\n",
    "                \"Extract a concise 1-2 line summary of this document\"\n",
    "            )\n",
    "        )\n",
    "        pickle.dump(summary, open(summary_out_path, \"wb\"))\n",
    "    else:\n",
    "        summary = pickle.load(open(summary_out_path, \"rb\"))\n",
    "\n",
    "    # define tools\n",
    "    query_engine_tools = [\n",
    "        QueryEngineTool(\n",
    "            query_engine=vector_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=f\"vector_tool_{file_base}\",\n",
    "                description=f\"Useful for questions related to specific facts\",\n",
    "            ),\n",
    "        ),\n",
    "        QueryEngineTool(\n",
    "            query_engine=summary_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=f\"summary_tool_{file_base}\",\n",
    "                description=f\"Useful for summarization questions\",\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # build agent\n",
    "    function_llm = OpenAI(model=\"gpt-4\")\n",
    "    agent = OpenAIAgent.from_tools(\n",
    "        query_engine_tools,\n",
    "        llm=function_llm,\n",
    "        verbose=True,\n",
    "        system_prompt=f\"\"\"\\\n",
    "You are a specialized agent designed to answer queries about the `{file_base}.html` part of the LlamaIndex docs.\n",
    "You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\\n",
    "\"\"\",\n",
    "    )\n",
    "\n",
    "    return agent, summary\n",
    "\n",
    "\n",
    "async def build_agents(docs):\n",
    "    node_parser = SentenceSplitter()\n",
    "\n",
    "    # Build agents dictionary\n",
    "    agents_dict = {}\n",
    "    extra_info_dict = {}\n",
    "\n",
    "    # # this is for the baseline\n",
    "    # all_nodes = []\n",
    "\n",
    "    for idx, doc in enumerate(tqdm(docs)):\n",
    "        nodes = node_parser.get_nodes_from_documents([doc])\n",
    "        # all_nodes.extend(nodes)\n",
    "\n",
    "        # ID will be base + parent\n",
    "        file_path = Path(doc.metadata[\"path\"])\n",
    "        file_base = str(file_path.parent.stem) + \"_\" + str(file_path.stem)\n",
    "        agent, summary = await build_agent_per_doc(nodes, file_base)\n",
    "\n",
    "        agents_dict[file_base] = agent\n",
    "        extra_info_dict[file_base] = {\"summary\": summary, \"nodes\": nodes}\n",
    "\n",
    "    return agents_dict, extra_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44748b46-dd6b-4d4f-bc70-7022ae96413f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5dca576251400083b450c6e1559c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest_search\n",
      "latest_genindex\n",
      "latest_index\n",
      "community_frequently_asked_questions\n",
      "community_integrations\n",
      "community_full_stack_projects\n",
      "integrations_tonicvalidate\n",
      "integrations_using_with_langchain\n",
      "integrations_trulens\n",
      "integrations_deepeval\n",
      "integrations_managed_indices\n",
      "integrations_chatgpt_plugins\n",
      "integrations_graphsignal\n",
      "integrations_lmformatenforcer\n",
      "integrations_graph_stores\n",
      "integrations_vector_stores\n",
      "integrations_fleet_libraries_context\n",
      "llama_packs_root\n",
      "faq_vector_database\n",
      "faq_query_engines\n",
      "faq_embeddings\n",
      "faq_chat_engines\n",
      "faq_llms\n",
      "getting_started_reading\n",
      "getting_started_discover_llamaindex\n",
      "getting_started_starter_example\n",
      "getting_started_customization\n",
      "getting_started_installation\n",
      "getting_started_concepts\n",
      "api_reference_storage\n",
      "api_reference_multi_modal\n",
      "api_reference_response\n",
      "api_reference_callbacks\n",
      "api_reference_node\n",
      "api_reference_readers\n",
      "api_reference_agents\n",
      "api_reference_query\n",
      "api_reference_example_notebooks\n",
      "api_reference_node_postprocessor\n",
      "api_reference_composability\n",
      "api_reference_llm_predictor\n",
      "api_reference_service_context\n",
      "api_reference_prompts\n",
      "api_reference_struct_store\n",
      "api_reference_indices\n",
      "api_reference_evaluation\n",
      "api_reference_index\n",
      "api_reference_memory\n",
      "api_reference_llms\n",
      "langchain_integrations_base\n",
      "storage_kv_store\n",
      "storage_vector_store\n",
      "storage_index_store\n",
      "storage_docstore\n",
      "storage_indices_save_load\n",
      "multi_modal_openai\n",
      "query_query_transform\n",
      "query_query_bundle\n",
      "query_retrievers\n",
      "query_response_synthesizer\n",
      "query_query_engines\n",
      "retrievers_vector_store\n",
      "retrievers_empty\n",
      "retrievers_transform\n",
      "retrievers_tree\n",
      "retrievers_kg\n",
      "retrievers_table\n",
      "query_engines_graph_query_engine\n",
      "query_engines_citation_query_engine\n",
      "query_engines_pandas_query_engine\n",
      "query_engines_retriever_query_engine\n",
      "query_engines_knowledge_graph_query_engine\n",
      "query_engines_retriever_router_query_engine\n",
      "query_engines_sub_question_query_engine\n",
      "query_engines_flare_query_engine\n",
      "query_engines_sql_query_engine\n",
      "query_engines_transform_query_engine\n",
      "query_engines_sql_join_query_engine\n",
      "query_engines_router_query_engine\n",
      "chat_engines_condense_question_chat_engine\n",
      "chat_engines_simple_chat_engine\n",
      "chat_engines_condense_plus_context_chat_engine\n",
      "indices_vector_store\n",
      "indices_tree\n",
      "indices_kg\n",
      "indices_list\n",
      "indices_table\n",
      "indices_struct_store\n",
      "service_context_embeddings\n",
      "service_context_prompt_helper\n",
      "llms_xinference\n",
      "llms_langchain\n",
      "llms_replicate\n",
      "llms_llama_cpp\n",
      "llms_azure_openai\n",
      "llms_openai\n",
      "llms_predibase\n",
      "llms_gradient_model_adapter\n",
      "llms_anthropic\n",
      "llms_openllm\n",
      "llms_huggingface\n"
     ]
    }
   ],
   "source": [
    "agents_dict, extra_info_dict = await build_agents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899ca55b-0c02-429b-a765-8e4f806d503f",
   "metadata": {},
   "source": [
    "### Build Retriever-Enabled OpenAI Agent\n",
    "\n",
    "We build a top-level agent that can orchestrate across the different document agents to answer any user query.\n",
    "\n",
    "This `RetrieverOpenAIAgent` performs tool retrieval before tool use (unlike a default agent that tries to put all tools in the prompt).\n",
    "\n",
    "**Improvements from V0**: We make the following improvements compared to the \"base\" version in V0.\n",
    "\n",
    "- Adding in reranking: we use Cohere reranker to better filter the candidate set of documents.\n",
    "- Adding in a query planning tool: we add an explicit query planning tool that's dynamically created based on the set of retrieved tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6884ff15-bf40-4bdd-a1e3-58cbd056a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tool for each document agent\n",
    "all_tools = []\n",
    "for file_base, agent in agents_dict.items():\n",
    "    summary = extra_info_dict[file_base][\"summary\"]\n",
    "    doc_tool = QueryEngineTool(\n",
    "        query_engine=agent,\n",
    "        metadata=ToolMetadata(\n",
    "            name=f\"tool_{file_base}\",\n",
    "            description=summary,\n",
    "        ),\n",
    "    )\n",
    "    all_tools.append(doc_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ed0e1-b96f-446b-a768-4f11a9a1a7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolMetadata(description='This document provides examples and instructions for searching within the Llama Index documentation.', name='tool_latest_search', fn_schema=<class 'llama_index.tools.types.DefaultToolFnSchema'>)\n"
     ]
    }
   ],
   "source": [
    "print(all_tools[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b266ad43-c3fd-41cb-9e3b-4cb2bb2c2e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import (\n",
    "    ObjectIndex,\n",
    "    SimpleToolNodeMapping,\n",
    "    ObjectRetriever,\n",
    ")\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-4-0613\")\n",
    "\n",
    "tool_mapping = SimpleToolNodeMapping.from_objects(all_tools)\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    tool_mapping,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "vector_node_retriever = obj_index.as_node_retriever(similarity_top_k=10)\n",
    "\n",
    "\n",
    "# define a custom retriever with reranking\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    def __init__(self, vector_retriever, postprocessor=None):\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._postprocessor = postprocessor or CohereRerank(top_n=5)\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle):\n",
    "        retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        filtered_nodes = self._postprocessor.postprocess_nodes(\n",
    "            retrieved_nodes, query_bundle=query_bundle\n",
    "        )\n",
    "\n",
    "        return filtered_nodes\n",
    "\n",
    "\n",
    "# define a custom object retriever that adds in a query planning tool\n",
    "class CustomObjectRetriever(ObjectRetriever):\n",
    "    def __init__(self, retriever, object_node_mapping, all_tools, llm=None):\n",
    "        self._retriever = retriever\n",
    "        self._object_node_mapping = object_node_mapping\n",
    "        self._llm = llm or OpenAI(\"gpt-4-0613\")\n",
    "\n",
    "    def retrieve(self, query_bundle):\n",
    "        nodes = self._retriever.retrieve(query_bundle)\n",
    "        tools = [self._object_node_mapping.from_node(n.node) for n in nodes]\n",
    "\n",
    "        sub_question_engine = SubQuestionQueryEngine.from_defaults(\n",
    "            query_engine_tools=tools, llm=self._llm\n",
    "        )\n",
    "        sub_question_description = f\"\"\"\\\n",
    "Useful for any queries that involve comparing multiple documents. ALWAYS use this tool for comparison queries - make sure to call this \\\n",
    "tool with the original query. Do NOT use the other tools for any queries involving multiple documents.\n",
    "\"\"\"\n",
    "        sub_question_tool = QueryEngineTool(\n",
    "            query_engine=sub_question_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"compare_tool\", description=sub_question_description\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return tools + [sub_question_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba0d1a6-e324-4faa-b72b-d340904e65b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_node_retriever = CustomRetriever(vector_node_retriever)\n",
    "\n",
    "# wrap it with ObjectRetriever to return objects\n",
    "custom_obj_retriever = CustomObjectRetriever(\n",
    "    custom_node_retriever, tool_mapping, all_tools, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8654ce2a-cce7-44fc-8445-8bbcfdf7ee91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "tmps = custom_obj_retriever.retrieve(\"hello\")\n",
    "print(len(tmps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed38942-1e37-4c61-89fa-d2ef41151831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai_legacy import FnRetrieverOpenAIAgent\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "top_agent = FnRetrieverOpenAIAgent.from_retriever(\n",
    "    custom_obj_retriever,\n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries about the documentation.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# top_agent = ReActAgent.from_tools(\n",
    "#     tool_retriever=custom_obj_retriever,\n",
    "#     system_prompt=\"\"\" \\\n",
    "# You are an agent designed to answer queries about the documentation.\n",
    "# Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "# \"\"\",\n",
    "#     llm=llm,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa32b97c-6779-4b60-823d-6ca3be6f358a",
   "metadata": {},
   "source": [
    "### Define Baseline Vector Store Index\n",
    "\n",
    "As a point of comparison, we define a \"naive\" RAG pipeline which dumps all docs into a single vector index collection.\n",
    "\n",
    "We set the top_k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f54834-1597-46ce-b0d3-0456bfa0d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = [\n",
    "    n for extra_info in extra_info_dict.values() for n in extra_info[\"nodes\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dfc88f-6f47-4ef2-9ae6-74abde06a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_index = VectorStoreIndex(all_nodes)\n",
    "base_query_engine = base_index.as_query_engine(similarity_top_k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedb927-a992-4f21-a0fb-4ce4361adcb3",
   "metadata": {},
   "source": [
    "## Running Example Queries\n",
    "\n",
    "Let's run some example queries, ranging from QA / summaries over a single document to QA / summarization over multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e743c62-7dd8-4ac9-85a5-f1cbc112a79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: tool_api_reference_evaluation with args: {\n",
      "  \"input\": \"types of evaluation\"\n",
      "}\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_api_reference_evaluation with args: {\n",
      "  \"input\": \"types of evaluation\"\n",
      "}\n",
      "Got output: The types of evaluation can include correctness evaluation, faithfulness evaluation, guideline evaluation, hit rate evaluation, MRR (Mean Reciprocal Rank) evaluation, pairwise comparison evaluation, relevancy evaluation, and response evaluation.\n",
      "========================\n",
      "Got output: The types of evaluation mentioned in the `api_reference_evaluation.html` part of the LlamaIndex docs include:\n",
      "\n",
      "1. Correctness Evaluation\n",
      "2. Faithfulness Evaluation\n",
      "3. Guideline Evaluation\n",
      "4. Hit Rate Evaluation\n",
      "5. MRR (Mean Reciprocal Rank) Evaluation\n",
      "6. Pairwise Comparison Evaluation\n",
      "7. Relevancy Evaluation\n",
      "8. Response Evaluation\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "response = top_agent.query(\n",
    "    \"Tell me about the different types of evaluation in LlamaIndex\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce2a76-5779-4acf-9337-69109dae7fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several types of evaluation in LlamaIndex:\n",
      "\n",
      "1. Correctness Evaluation: This type of evaluation measures the accuracy of the retrieval results. It checks if the retrieved documents are correct and relevant to the query.\n",
      "\n",
      "2. Faithfulness Evaluation: Faithfulness evaluation measures how faithfully the retrieved documents represent the original data. It checks if the retrieved documents accurately reflect the information in the original documents.\n",
      "\n",
      "3. Guideline Evaluation: Guideline evaluation involves comparing the retrieval results against a set of guidelines or ground truth. It checks if the retrieval results align with the expected or desired outcomes.\n",
      "\n",
      "4. Hit Rate Evaluation: Hit rate evaluation measures the percentage of queries that return at least one relevant document. It is a binary evaluation metric that indicates the effectiveness of the retrieval system in finding relevant documents.\n",
      "\n",
      "5. MRR (Mean Reciprocal Rank) Evaluation: MRR evaluation measures the average rank of the first relevant document in the retrieval results. It provides a single value that represents the effectiveness of the retrieval system in ranking relevant documents.\n",
      "\n",
      "6. Pairwise Comparison Evaluation: Pairwise comparison evaluation involves comparing the retrieval results of different systems or algorithms. It helps determine which system performs better in terms of retrieval accuracy and relevance.\n",
      "\n",
      "7. Relevancy Evaluation: Relevancy evaluation measures the relevance of the retrieved documents to the query. It can be done using various metrics such as precision, recall, and F1 score.\n",
      "\n",
      "8. Response Evaluation: Response evaluation measures the quality of the response generated by the retrieval system. It checks if the response is informative, accurate, and helpful to the user.\n",
      "\n",
      "These evaluation types help assess the performance and effectiveness of the retrieval system in LlamaIndex.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28b422-fb73-4b59-9e77-3ba3afa87795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex utilizes various types of evaluation methods to assess its performance and effectiveness. These evaluation methods include RelevancyEvaluator, RetrieverEvaluator, SemanticSimilarityEvaluator, PairwiseComparisonEvaluator, CorrectnessEvaluator, FaithfulnessEvaluator, and GuidelineEvaluator. Each of these evaluators serves a specific purpose in evaluating different aspects of the LlamaIndex system.\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "response = base_query_engine.query(\n",
    "    \"Tell me about the different types of evaluation in LlamaIndex\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ef20c-3ccc-46c3-ad87-667138d78d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: compare_tool with args: {\n",
      "  \"input\": \"content in the contributions page vs. index page\"\n",
      "}\n",
      "Generated 2 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[tool_development_contributing] Q: What is the content of the contributions page?\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[tool_latest_index] Q: What is the content of the index page?\n",
      "\u001b[0m=== Calling Function ===\n",
      "Calling function: summary_tool_development_contributing with args: {\n",
      "  \"input\": \"development_contributing.html\"\n",
      "}\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_latest_index with args: {\n",
      "  \"input\": \"content of the index page\"\n",
      "}\n",
      "Got output: The development_contributing.html file provides information on how to contribute to LlamaIndex. It includes guidelines on what to work on, such as extending core modules, fixing bugs, adding usage examples, adding experimental features, and improving code quality and documentation. The file also provides details on each module, including data loaders, node parsers, text splitters, document/index/KV stores, managed index, vector stores, retrievers, query engines, query transforms, token usage optimizers, node postprocessors, and output parsers. Additionally, the file includes a development guideline section that covers environment setup, validating changes, formatting/linting, testing, creating example notebooks, and creating a pull request.\n",
      "========================\n",
      "Got output: The content of the index page provides information about LlamaIndex, a data framework for LLM applications. It explains why LlamaIndex is useful for augmenting LLM models with private or domain-specific data that may be distributed across different applications and data stores. LlamaIndex offers tools such as data connectors, data indexes, engines, and data agents to ingest, structure, and access data. It is designed for beginners as well as advanced users who can customize and extend its modules. The page also provides installation instructions, tutorials, and links to the LlamaIndex ecosystem and associated projects.\n",
      "========================\n",
      "\u001b[1;3;38;2;90;149;237m[tool_latest_index] A: The content of the `latest_index.html` page provides comprehensive information about LlamaIndex, a data framework for LLM applications. It explains the utility of LlamaIndex in augmenting LLM models with private or domain-specific data that may be distributed across different applications and data stores. \n",
      "\n",
      "The page details the tools offered by LlamaIndex, such as data connectors, data indexes, engines, and data agents, which are used to ingest, structure, and access data. It is designed to cater to both beginners and advanced users, with the flexibility to customize and extend its modules.\n",
      "\n",
      "Additionally, the page provides installation instructions and tutorials for users. It also includes links to the LlamaIndex ecosystem and associated projects for further exploration and understanding.\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[tool_development_contributing] A: The `development_contributing.html` page of the LlamaIndex docs provides comprehensive information on how to contribute to the project. It includes guidelines on the areas to focus on, such as extending core modules, fixing bugs, adding usage examples, adding experimental features, and improving code quality and documentation.\n",
      "\n",
      "The page also provides detailed information on each module, including data loaders, node parsers, text splitters, document/index/KV stores, managed index, vector stores, retrievers, query engines, query transforms, token usage optimizers, node postprocessors, and output parsers.\n",
      "\n",
      "In addition, there is a development guideline section that covers various aspects of the development process, including environment setup, validating changes, formatting/linting, testing, creating example notebooks, and creating a pull request.\n",
      "\u001b[0mGot output: The content in the contributions page of the LlamaIndex documentation provides comprehensive information on how to contribute to the project, including guidelines on areas to focus on and detailed information on each module. It also covers various aspects of the development process. \n",
      "\n",
      "On the other hand, the content in the index page of the LlamaIndex documentation provides comprehensive information about LlamaIndex itself, explaining its utility in augmenting LLM models with private or domain-specific data. It details the tools offered by LlamaIndex and provides installation instructions, tutorials, and links to the LlamaIndex ecosystem and associated projects.\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "response = top_agent.query(\n",
    "    \"Compare the content in the contributions page vs. index page.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe1dd4c-8bfd-43d0-99bc-ca60861dc418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contributions page of the LlamaIndex documentation provides guidelines for contributing to LlamaIndex, including extending core modules, fixing bugs, adding usage examples, adding experimental features, and improving code quality and documentation. It also includes information on the environment setup, validating changes, formatting and linting, testing, creating example notebooks, and creating a pull request.\n",
      "\n",
      "On the other hand, the index page of the LlamaIndex documentation provides information about LlamaIndex itself. It explains that LlamaIndex is a data framework that allows LLM applications to ingest, structure, and access private or domain-specific data. It provides tools such as data connectors, data indexes, engines, data agents, and application integrations. The index page also mentions that LlamaIndex is designed for beginners, advanced users, and everyone in between, and offers both high-level and lower-level APIs for customization. It provides installation instructions, links to the GitHub and PyPi repositories, and information about the LlamaIndex community on Twitter and Discord.\n",
      "\n",
      "In summary, the contributions page focuses on contributing to LlamaIndex, while the index page provides an overview of LlamaIndex and its features.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d97266-8e22-43a8-adfe-b9a7f833c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = top_agent.query(\n",
    "    \"Can you compare the tree index and list index at a very high-level?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7401a80c-3cc7-4c72-9c45-82ffc1bd6816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At a high level, the Tree Index and List Index are two different types of indexes used in the system. \n",
      "\n",
      "The Tree Index is a tree-structured index that is built specifically for each query. It allows for the construction of a query-specific tree from leaf nodes to return a response. The Tree Index is designed to provide a more optimized and efficient way of retrieving nodes based on a query.\n",
      "\n",
      "On the other hand, the List Index is a keyword table index that supports operations such as inserting and deleting documents, retrieving nodes based on a query, and refreshing the index with updated documents. The List Index is a simpler index that uses a keyword table approach for retrieval.\n",
      "\n",
      "Both indexes have their own advantages and use cases. The choice between them depends on the specific requirements and constraints of the system.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
