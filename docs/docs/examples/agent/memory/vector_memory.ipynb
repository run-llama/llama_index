{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "791115e1",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/agent/memory/vector_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cfc1b0-f28a-43ee-8242-90cfdb49fec2",
   "metadata": {},
   "source": [
    "# Vector Memory\n",
    "\n",
    "**NOTE:** This example of memory is deprecated in favor of the newer and more flexible `Memory` class. See the [latest docs](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/memory/).\n",
    "\n",
    "The vector memory module uses vector search (backed by a vector db) to retrieve relevant conversation items given a user input.\n",
    "\n",
    "This notebook shows you how to use the `VectorMemory` class. We show you how to use its individual functions. A typical usecase for vector memory is as a long-term memory storage of chat messages. You can"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc3cc2c-96e2-49ff-a029-0ce4ff314538",
   "metadata": {},
   "source": [
    "![VectorMemoryIllustration](https://d3ddy8balm3goa.cloudfront.net/llamaindex/vector-memory.excalidraw.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d29a822-727d-4a87-aa7f-e3091a311d5f",
   "metadata": {},
   "source": [
    "### Initialize and Experiment with Memory Module\n",
    "\n",
    "Here we initialize a raw memory module and demonstrate its functions - to put and retrieve from ChatMessage objects.\n",
    "\n",
    "- Note that `retriever_kwargs` is the same args you'd specify on the `VectorIndexRetriever` or from `index.as_retriever(..)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fefa31-4c6c-4ae4-9957-ea73781ecc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import VectorMemory\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "\n",
    "vector_memory = VectorMemory.from_defaults(\n",
    "    vector_store=None,  # leave as None to use default in-memory vector store\n",
    "    embed_model=OpenAIEmbedding(),\n",
    "    retriever_kwargs={\"similarity_top_k\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073c255-a35e-432d-a917-12dafaf377e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "msgs = [\n",
    "    ChatMessage.from_str(\"Jerry likes juice.\", \"user\"),\n",
    "    ChatMessage.from_str(\"Bob likes burgers.\", \"user\"),\n",
    "    ChatMessage.from_str(\"Alice likes apples.\", \"user\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44017c1-129a-4864-a070-c00ca6d6cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load into memory\n",
    "for m in msgs:\n",
    "    vector_memory.put(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa077b6e-6477-406a-8d52-d8e375daae6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='Jerry likes juice.', additional_kwargs={})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve from memory\n",
    "msgs = vector_memory.get(\"What does Jerry like?\")\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4baa16-55d1-418c-ac7f-a8f30252e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_memory.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f17529-bd4a-4edb-8176-7332e48016ea",
   "metadata": {},
   "source": [
    "Now let's try resetting and trying again. This time, we'll add an assistant message. Note that user/assistant messages are bundled by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c427d1-c157-4916-a9c2-17b3fede4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = [\n",
    "    ChatMessage.from_str(\"Jerry likes burgers.\", \"user\"),\n",
    "    ChatMessage.from_str(\"Bob likes apples.\", \"user\"),\n",
    "    ChatMessage.from_str(\"Indeed, Bob likes apples.\", \"assistant\"),\n",
    "    ChatMessage.from_str(\"Alice likes juice.\", \"user\"),\n",
    "]\n",
    "vector_memory.set(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4402fe-f140-415b-9e07-accbde8ae2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='Bob likes apples.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Indeed, Bob likes apples.', additional_kwargs={})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = vector_memory.get(\"What does Bob like?\")\n",
    "msgs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index-core",
   "language": "python",
   "name": "llama-index-core"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
