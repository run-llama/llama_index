{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3902b09d",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/agent/memory/composable_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3869de9c-846a-4295-ab6d-1b3fcfc4aa6f",
   "metadata": {},
   "source": [
    "# Simple Composable Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330ee9d2-263a-4ec6-93e6-3fb62562a286",
   "metadata": {},
   "source": [
    "**NOTE:** This example of memory is deprecated in favor of the newer and more flexible `Memory` class. See the [latest docs](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/memory/).\n",
    "\n",
    "In this notebook, we demonstrate how to inject multiple memory sources into an agent. Specifically, we use the `SimpleComposableMemory` which is comprised of a `primary_memory` as well as potentially several secondary memory sources (stored in `secondary_memory_sources`). The main difference is that `primary_memory` will be used as the main chat buffer for the agent, where as any retrieved messages from `secondary_memory_sources` will be injected to the system prompt message only.\n",
    "\n",
    "Multiple memory sources may be of use for example in situations where you have a longer-term memory such as `VectorMemory` that you want to use in addition to the default `ChatMemoryBuffer`. What you'll see in this notebook is that with a `SimpleComposableMemory` you'll be able to effectively \"load\" the desired messages from long-term memory into the main memory (i.e. the `ChatMemoryBuffer`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6531f04b-9016-4046-989a-9957a49af944",
   "metadata": {},
   "source": [
    "## How `SimpleComposableMemory` Works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6ff6f-eeee-44fa-b85c-862f08366b74",
   "metadata": {},
   "source": [
    "We begin with the basic usage of the `SimpleComposableMemory`. Here we construct a `VectorMemory` as well as a default `ChatMemoryBuffer`. The `VectorMemory` will be our secondary memory source, whereas the `ChatMemoryBuffer` will be the main or primary one. To instantiate a `SimpleComposableMemory` object, we need to supply a `primary_memory` and (optionally) a list of `secondary_memory_sources`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4609da-ac1d-405e-98d7-8a228f4577d3",
   "metadata": {},
   "source": [
    "![SimpleComposableMemoryIllustration](https://d3ddy8balm3goa.cloudfront.net/llamaindex/simple-composable-memory.excalidraw.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51784f0-98a9-497a-8e35-257a62c55141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import (\n",
    "    VectorMemory,\n",
    "    SimpleComposableMemory,\n",
    "    ChatMemoryBuffer,\n",
    ")\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "vector_memory = VectorMemory.from_defaults(\n",
    "    vector_store=None,  # leave as None to use default in-memory vector store\n",
    "    embed_model=OpenAIEmbedding(),\n",
    "    retriever_kwargs={\"similarity_top_k\": 1},\n",
    ")\n",
    "\n",
    "# let's set some initial messages in our secondary vector memory\n",
    "msgs = [\n",
    "    ChatMessage.from_str(\"You are a SOMEWHAT helpful assistant.\", \"system\"),\n",
    "    ChatMessage.from_str(\"Bob likes burgers.\", \"user\"),\n",
    "    ChatMessage.from_str(\"Indeed, Bob likes apples.\", \"assistant\"),\n",
    "    ChatMessage.from_str(\"Alice likes apples.\", \"user\"),\n",
    "]\n",
    "vector_memory.set(msgs)\n",
    "\n",
    "chat_memory_buffer = ChatMemoryBuffer.from_defaults()\n",
    "\n",
    "composable_memory = SimpleComposableMemory.from_defaults(\n",
    "    primary_memory=chat_memory_buffer,\n",
    "    secondary_memory_sources=[vector_memory],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c6ee0-969a-4346-9a5d-a21de36dc383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMemoryBuffer(chat_store=SimpleChatStore(store={}), chat_store_key='chat_history', token_limit=3000, tokenizer_fn=functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composable_memory.primary_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb7cbe1-0793-48c7-9643-a324b569b96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VectorMemory(vector_index=<llama_index.core.indices.vector_store.base.VectorStoreIndex object at 0x137b912a0>, retriever_kwargs={'similarity_top_k': 1}, batch_by_user_message=True, cur_batch_textnode=TextNode(id_='288b0ef3-570e-4698-a1ae-b3531df66361', embedding=None, metadata={'sub_dicts': [{'role': <MessageRole.USER: 'user'>, 'content': 'Alice likes apples.', 'additional_kwargs': {}}]}, excluded_embed_metadata_keys=['sub_dicts'], excluded_llm_metadata_keys=['sub_dicts'], relationships={}, text='Alice likes apples.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composable_memory.secondary_memory_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa65eb4-7c00-4d25-bbc7-5e7d8d5794e2",
   "metadata": {},
   "source": [
    "### `put()` messages into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c1c752-07b5-42b9-9393-3623841b860d",
   "metadata": {},
   "source": [
    "Since `SimpleComposableMemory` is itself a subclass of `BaseMemory`, we add messages to it in the same way as we do for other memory modules. Note that for `SimpleComposableMemory`, invoking `.put()` effectively calls `.put()` on all memory sources. In other words, the message gets added to `primary` and `secondary` sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd78f09-1bbb-4e5c-9535-a222cce9190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = [\n",
    "    ChatMessage.from_str(\"You are a REALLY helpful assistant.\", \"system\"),\n",
    "    ChatMessage.from_str(\"Jerry likes juice.\", \"user\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0a9b2d-64f0-4d24-a980-94157d906043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load into all memory sources modules\"\n",
    "for m in msgs:\n",
    "    composable_memory.put(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e59b8-2df7-411f-9529-58c1bc77cfd8",
   "metadata": {},
   "source": [
    "### `get()` messages from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987d6b7-6651-4d57-a290-7ed2deaed9da",
   "metadata": {},
   "source": [
    "When `.get()` is invoked, we similarly execute all of the `.get()` methods of `primary` memory as well as all of the `secondary` sources. This leaves us with sequence of lists of messages that we have to must \"compose\" into a sensible single set of messages (to pass downstream to our agents). Special care must be applied here in general to ensure that the final sequence of messages are both sensible and conform to the chat APIs of the LLM provider.\n",
    "\n",
    "For `SimpleComposableMemory`, we **inject the messages from the `secondary` sources in the system message of the `primary` memory**. The rest of the message history of the `primary` source is left intact, and this composition is what is ultimately returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74fc439-9f5b-4560-b06d-e9cb3c078cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='You are a REALLY helpful assistant.\\n\\nBelow are a set of relevant dialogues retrieved from potentially several memory sources:\\n\\n=====Relevant messages from memory source 1=====\\n\\n\\tUSER: Bob likes burgers.\\n\\tASSISTANT: Indeed, Bob likes apples.\\n\\n=====End of relevant messages from memory source 1======\\n\\nThis is the end of the retrieved message dialogues.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='Jerry likes juice.', additional_kwargs={})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = composable_memory.get(\"What does Bob like?\")\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae49db4-15ef-4532-b84d-ee9a6fed1337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: You are a REALLY helpful assistant.\n",
      "\n",
      "Below are a set of relevant dialogues retrieved from potentially several memory sources:\n",
      "\n",
      "=====Relevant messages from memory source 1=====\n",
      "\n",
      "\tUSER: Bob likes burgers.\n",
      "\tASSISTANT: Indeed, Bob likes apples.\n",
      "\n",
      "=====End of relevant messages from memory source 1======\n",
      "\n",
      "This is the end of the retrieved message dialogues.\n"
     ]
    }
   ],
   "source": [
    "# see the memory injected into the system message of the primary memory\n",
    "print(msgs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb966da-b5cb-4744-b189-ef139e0972cf",
   "metadata": {},
   "source": [
    "### Successive calls to `get()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15531d59-acf8-4a5a-b6ad-57a787f4e804",
   "metadata": {},
   "source": [
    "Successive calls of `get()` will simply replace the loaded `secondary` memory messages in the system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca764fb4-5597-4e4c-9ab4-8ec1a3bafb4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='You are a REALLY helpful assistant.\\n\\nBelow are a set of relevant dialogues retrieved from potentially several memory sources:\\n\\n=====Relevant messages from memory source 1=====\\n\\n\\tUSER: Alice likes apples.\\n\\n=====End of relevant messages from memory source 1======\\n\\nThis is the end of the retrieved message dialogues.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='Jerry likes juice.', additional_kwargs={})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = composable_memory.get(\"What does Alice like?\")\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ff24f-7dc1-4027-b078-13e818fe05be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: You are a REALLY helpful assistant.\n",
      "\n",
      "Below are a set of relevant dialogues retrieved from potentially several memory sources:\n",
      "\n",
      "=====Relevant messages from memory source 1=====\n",
      "\n",
      "\tUSER: Alice likes apples.\n",
      "\n",
      "=====End of relevant messages from memory source 1======\n",
      "\n",
      "This is the end of the retrieved message dialogues.\n"
     ]
    }
   ],
   "source": [
    "# see the memory injected into the system message of the primary memory\n",
    "print(msgs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfce6b59-21ba-4e6f-b9bf-1a19dbacfe21",
   "metadata": {},
   "source": [
    "### What if `get()` retrieves `secondary` messages that already exist in `primary` memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2839b2-40da-4714-92ee-eb6c795d92bb",
   "metadata": {},
   "source": [
    "In the event that messages retrieved from `secondary` memory already exist in `primary` memory, then these rather redundant secondary messages will not get added to the system message. In the below example, the message \"Jerry likes juice.\" was `put` into all memory sources, so the system message is not altered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96604e4-3b4e-490f-a196-f046d28c13f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='You are a REALLY helpful assistant.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='Jerry likes juice.', additional_kwargs={})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = composable_memory.get(\"What does Jerry like?\")\n",
    "msgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b3a61e-9704-4547-825c-e73f09116684",
   "metadata": {},
   "source": [
    "### How to `reset` memory "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948e792-b5c0-4f53-a2ca-b71210e9e445",
   "metadata": {},
   "source": [
    "Similar to the other methods `put()` and `get()`, calling `reset()` will execute `reset()` on both the `primary` and `secondary` memory sources. If you want to reset only the `primary` then you should call the `reset()` method only from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b008b6a-160e-41a8-b1fb-32edc5fd3cc3",
   "metadata": {},
   "source": [
    "#### `reset()` only primary memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77e557e-c6b2-4c80-80e4-b30398aa0f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "composable_memory.primary_memory.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf6aa5-65cf-488d-841c-bcda29db0613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composable_memory.primary_memory.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527fbdba-d0d4-47a2-b05c-bcd05c7b3764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='Alice likes apples.', additional_kwargs={})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composable_memory.secondary_memory_sources[0].get(\"What does Alice like?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b2eba6-2c74-419d-9d8e-f0e4a5f3250b",
   "metadata": {},
   "source": [
    "#### `reset()` all memory sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3e0989-8873-437d-9f5a-508a757745bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "composable_memory.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907779e6-1e6c-4494-b60c-85e1a3b18c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composable_memory.primary_memory.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5546827-39d1-41b9-96e3-fc67c34722ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composable_memory.secondary_memory_sources[0].get(\"What does Alice like?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79882a-ccc1-4336-87e0-fbcd789cf991",
   "metadata": {},
   "source": [
    "## Use `SimpleComposableMemory` With An Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2898764d-245c-4ceb-a748-fef14212d24d",
   "metadata": {},
   "source": [
    "Here we will use a `SimpleComposableMemory` with an agent and demonstrate how a secondary, long-term memory source can be used to use messages from on agent conversation as part of another conversation with another agent session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d37976-bdb0-45c8-94b8-94df62a0853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.agent import FunctionCallingAgent\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a04de0-b0f6-4f83-b09c-7a83f94a4112",
   "metadata": {},
   "source": [
    "### Define our memory modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca5351-91b8-4c7d-a4e7-f2d068a87688",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_memory = VectorMemory.from_defaults(\n",
    "    vector_store=None,  # leave as None to use default in-memory vector store\n",
    "    embed_model=OpenAIEmbedding(),\n",
    "    retriever_kwargs={\"similarity_top_k\": 2},\n",
    ")\n",
    "\n",
    "chat_memory_buffer = ChatMemoryBuffer.from_defaults()\n",
    "\n",
    "composable_memory = SimpleComposableMemory.from_defaults(\n",
    "    primary_memory=chat_memory_buffer,\n",
    "    secondary_memory_sources=[vector_memory],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67958c2b-267e-4c77-a4ee-20b7d5ef3d93",
   "metadata": {},
   "source": [
    "### Define our Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f7258-1bd4-4a75-a3cd-85e913919e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def mystery(a: int, b: int) -> int:\n",
    "    \"\"\"Mystery function on two numbers\"\"\"\n",
    "    return a**2 - b**2\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "mystery_tool = FunctionTool.from_defaults(fn=mystery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f611c-9c0f-4be0-952a-7580ce14b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "agent = FunctionCallingAgent.from_tools(\n",
    "    [multiply_tool, mystery_tool],\n",
    "    llm=llm,\n",
    "    memory=composable_memory,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d7bd71-76a9-4034-8126-22329168860a",
   "metadata": {},
   "source": [
    "### Execute some function calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad0a320-9f41-4249-bfa4-2366f87c49f4",
   "metadata": {},
   "source": [
    "When `.chat()` is invoked, the messages are put into the composable memory, which we understand from the previous section implies that all the messages are put in both `primary` and `secondary` sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc6fc63-7330-4e2e-b8a4-25e87c2325b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is the mystery function on 5 and 6?\n",
      "=== Calling Function ===\n",
      "Calling function: mystery with args: {\"a\": 5, \"b\": 6}\n",
      "=== Function Output ===\n",
      "-11\n",
      "=== LLM Response ===\n",
      "The mystery function on 5 and 6 returns -11.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is the mystery function on 5 and 6?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f79abef-dd0f-4a0a-b9ca-cd5d0d2e7d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What happens if you multiply 2 and 3?\n",
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\"a\": 2, \"b\": 3}\n",
      "=== Function Output ===\n",
      "6\n",
      "=== LLM Response ===\n",
      "If you multiply 2 and 3, the result is 6.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What happens if you multiply 2 and 3?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e684f-d5d0-44b4-8b0f-1e107c43ca28",
   "metadata": {},
   "source": [
    "### New Agent Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a2a4f7-322e-46d4-92ca-ce140951faf0",
   "metadata": {},
   "source": [
    "Now that we've added the messages to our `vector_memory`, we can see the effect of having this memory be used with a new agent session versus when it is used. Specifically, we ask the new agents to \"recall\" the outputs of the function calls, rather than re-computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c1312e-91eb-4167-9a07-8042c4a6846f",
   "metadata": {},
   "source": [
    "#### An Agent without our past memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c1124-f9a2-4b8c-ab94-c65e63f9314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "agent_without_memory = FunctionCallingAgent.from_tools(\n",
    "    [multiply_tool, mystery_tool], llm=llm, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a73b5-13e5-4927-a1a4-18fbaa843e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What was the output of the mystery function on 5 and 6 again? Don't recompute.\n",
      "=== LLM Response ===\n",
      "I'm sorry, but I don't have access to the previous output of the mystery function on 5 and 6.\n"
     ]
    }
   ],
   "source": [
    "response = agent_without_memory.chat(\n",
    "    \"What was the output of the mystery function on 5 and 6 again? Don't recompute.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ca898-e799-45a2-87f9-9747ba673692",
   "metadata": {},
   "source": [
    "#### An Agent with our past memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a806952-abff-497a-b829-baed461efc4b",
   "metadata": {},
   "source": [
    "We see that the agent without access to the our past memory cannot complete the task. With this next agent we will indeed pass in our previous long-term memory (i.e., `vector_memory`). Note that we even use a fresh `ChatMemoryBuffer` which means there is no `chat_history` with this agent. Nonetheless, it will be able to retrieve from our long-term memory to get the past dialogue it needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7f702-79b1-4203-8394-8afe1a0d0f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "\n",
    "composable_memory = SimpleComposableMemory.from_defaults(\n",
    "    primary_memory=ChatMemoryBuffer.from_defaults(),\n",
    "    secondary_memory_sources=[\n",
    "        vector_memory.copy(\n",
    "            deep=True\n",
    "        )  # using a copy here for illustration purposes\n",
    "        # later will use original vector_memory again\n",
    "    ],\n",
    ")\n",
    "\n",
    "agent_with_memory = FunctionCallingAgent.from_tools(\n",
    "    [multiply_tool, mystery_tool],\n",
    "    llm=llm,\n",
    "    memory=composable_memory,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc205eb5-b5c7-4f60-9e39-92a91149dcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_memory.chat_history  # an empty chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538cd6ec-b9dd-4622-9690-665b7eb578a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What was the output of the mystery function on 5 and 6 again? Don't recompute.\n",
      "=== LLM Response ===\n",
      "The output of the mystery function on 5 and 6 is -11.\n"
     ]
    }
   ],
   "source": [
    "response = agent_with_memory.chat(\n",
    "    \"What was the output of the mystery function on 5 and 6 again? Don't recompute.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ec6a0-8a78-46eb-b727-26eb4817cdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What was the output of the multiply function on 2 and 3 again? Don't recompute.\n",
      "=== LLM Response ===\n",
      "The output of the multiply function on 2 and 3 is 6.\n"
     ]
    }
   ],
   "source": [
    "response = agent_with_memory.chat(\n",
    "    \"What was the output of the multiply function on 2 and 3 again? Don't recompute.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5187ead5-8388-4305-9918-0b61d9874ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content=\"What was the output of the mystery function on 5 and 6 again? Don't recompute.\", additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='The output of the mystery function on 5 and 6 is -11.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content=\"What was the output of the multiply function on 2 and 3 again? Don't recompute.\", additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='The output of the multiply function on 2 and 3 is 6.', additional_kwargs={})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_memory.chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4fb96-6bae-48a7-a43b-4a99a1989e90",
   "metadata": {},
   "source": [
    "### What happens under the hood with `.chat(user_input)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ed139-c7a9-44ac-a455-d2f1c96122eb",
   "metadata": {},
   "source": [
    "Under the hood, `.chat(user_input)` call effectively will call the memory's `.get()` method with `user_input` as the argument. As we learned in the previous section, this will ultimately return a composition of the `primary` and all of the `secondary` memory sources. These composed messages are what is being passed to the LLM's chat API as the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f42289-e7cb-463a-b10b-44696bf3dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "composable_memory = SimpleComposableMemory.from_defaults(\n",
    "    primary_memory=ChatMemoryBuffer.from_defaults(),\n",
    "    secondary_memory_sources=[\n",
    "        vector_memory.copy(\n",
    "            deep=True\n",
    "        )  # copy for illustrative purposes to explain what\n",
    "        # happened under the hood from previous subsection\n",
    "    ],\n",
    ")\n",
    "agent_with_memory = agent_worker.as_agent(memory=composable_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3040fea-ba11-4703-a47b-395c3fd68d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='You are a helpful assistant.\\n\\nBelow are a set of relevant dialogues retrieved from potentially several memory sources:\\n\\n=====Relevant messages from memory source 1=====\\n\\n\\tUSER: What is the mystery function on 5 and 6?\\n\\tASSISTANT: None\\n\\tTOOL: -11\\n\\tASSISTANT: The mystery function on 5 and 6 returns -11.\\n\\n=====End of relevant messages from memory source 1======\\n\\nThis is the end of the retrieved message dialogues.', additional_kwargs={})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_memory.memory.get(\n",
    "    \"What was the output of the mystery function on 5 and 6 again? Don't recompute.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f363dc-957f-43d2-83ea-6e03b89f348c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: You are a helpful assistant.\n",
      "\n",
      "Below are a set of relevant dialogues retrieved from potentially several memory sources:\n",
      "\n",
      "=====Relevant messages from memory source 1=====\n",
      "\n",
      "\tUSER: What is the mystery function on 5 and 6?\n",
      "\tASSISTANT: None\n",
      "\tTOOL: -11\n",
      "\tASSISTANT: The mystery function on 5 and 6 returns -11.\n",
      "\n",
      "=====End of relevant messages from memory source 1======\n",
      "\n",
      "This is the end of the retrieved message dialogues.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    agent_with_memory.memory.get(\n",
    "        \"What was the output of the mystery function on 5 and 6 again? Don't recompute.\"\n",
    "    )[0]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index-caVs7DDe-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
