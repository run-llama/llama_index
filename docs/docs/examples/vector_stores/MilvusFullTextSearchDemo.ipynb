{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1496f9de",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/MilvusFullTextSearchDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b692c73",
   "metadata": {},
   "source": [
    "# Using Full-Text Search with LlamaIndex and Milvus\n",
    "\n",
    "In the complex landscape of information retrieval, [full-text search](https://milvus.io/docs/full-text-search.md#Full-Text-Search) emerges as a precision tool for document discovery. Unlike semantic search, which navigates contextual nuances, full-text search delivers exact keyword matching through the BM25 algorithm — a ranking method particularly powerful in Retrieval-Augmented Generation (RAG) applications. \n",
    "\n",
    "With [Milvus 2.5](https://milvus.io/blog/introduce-milvus-2-5-full-text-search-powerful-metadata-filtering-and-more.md)'s Sparse-BM25 approach, raw text is automatically transformed into sparse vectors, eliminating manual embedding generation and enabling a hybrid search strategy that combines semantic understanding with keyword relevance.\n",
    "\n",
    "This tutorial will demonstrate how to implement full-text search using LlamaIndex and Milvus, creating a sophisticated search system that bridges semantic understanding with keyword precision.\n",
    "\n",
    "> Before proceeding with this tutorial, ensure you have a basic understanding of [full-text search](https://milvus.io/docs/full-text-search.md#Full-Text-Search) and the [basic usage](https://docs.llamaindex.ai/en/stable/examples/vector_stores/MilvusIndexDemo/) of Milvus vector store in LlamaIndex."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f81e2c81",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**Install dependencies**\n",
    "\n",
    "To get ready for this tutorial, make sure you have the following dependencies installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install llama-index-vector-stores-milvus\n",
    "! pip install llama-index-embeddings-openai\n",
    "! pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04f0ae",
   "metadata": {},
   "source": [
    "> If you're using Google Colab, you may need to **restart the runtime** (Navigate to the \"Runtime\" menu at the top of the interface, and select \"Restart session\" from the dropdown menu.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d58733",
   "metadata": {},
   "source": [
    "**Set up accounts**\n",
    "\n",
    "We will use OpenAI services to generate text embeddings and answers. You need to prepare the [OpenAI API key](https://platform.openai.com/api-keys). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b2d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"sk-\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f445b",
   "metadata": {},
   "source": [
    "To use the Milvus vector store, specify your Milvus server `URI` (and optionally with the `TOKEN`). To start a Milvus server, you may follow [Milvus documentation](https://milvus.io/docs/install-overview.md) for installation or simply [register with Zilliz Cloud](https://docs.zilliz.com/docs/register-with-zilliz-cloud). \n",
    "\n",
    "> Full-text search is currently available in Milvus Standalone, Milvus Distributed, and Zilliz Cloud, though not yet supported in Milvus Lite (which has this feature planned for future implementation). Reach out support@zilliz.com for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb18406",
   "metadata": {},
   "outputs": [],
   "source": [
    "URI = \"http://localhost:19530\"\n",
    "# TOKEN = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5889ef",
   "metadata": {},
   "source": [
    "**Download example data**\n",
    "\n",
    "The following commands will download example documents to the relative directory \"data/paul_graham\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76c9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-26 08:19:15--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n",
      "\n",
      "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2025-03-26 08:19:16 (1.09 MB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p 'data/paul_graham/'\n",
    "! wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ed34a",
   "metadata": {},
   "source": [
    "## RAG with Full-Text Search\n",
    "\n",
    "Full-text search enhances RAG systems by enabling precise keyword matching across large document collections. This approach helps to find the most relevant information quickly, leading to more accurate and contextually grounded responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cdd451",
   "metadata": {},
   "source": [
    "Use the `SimpleDirectoryReaderLoad` to load document from the essay by Paul Graham with the title \"What I Worked On\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcc6479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example document:\n",
      " Doc ID: 195a1e8a-7c19-4425-92d8-074db2859fc2\n",
      "Text: What I Worked On  February 2021  Before college the two main\n",
      "things I worked on, outside of school, were writing and programming. I\n",
      "didn't write essays. I wrote what beginning writers were supposed to\n",
      "write then, and probably still are: short stories. My stories were\n",
      "awful. They had hardly any plot, just characters with strong feelings,\n",
      "which I ...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "\n",
    "# Let's take a look at the first document\n",
    "print(\"Example document:\\n\", documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a4043",
   "metadata": {},
   "source": [
    "### BM25 only\n",
    "\n",
    "LlamaIndex's `MilvusVectorStore` introduces a powerful full-text search capability through sparse field indexing. By integrating a built-in function as the `sparse_embedding_function`, the system can rank text fields using the BM25 algorithm.\n",
    "\n",
    "In this section, we will demonstrates how to implement RAG with full-text search using BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb19b205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 08:19:21,240 [DEBUG][_create_connection]: Created new connection using: d1d2f906f03c45589092809db07c8ac4 (async_milvus_client.py:547)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.vector_stores.milvus.utils import BM25BuiltInFunction\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Skip dense embedding model\n",
    "Settings.embed_model = None\n",
    "\n",
    "# Build Milvus vector store creating a new collection\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=URI,\n",
    "    # token=TOKEN,\n",
    "    enable_dense=False,\n",
    "    enable_sparse=True,\n",
    "    sparse_embedding_function=BM25BuiltInFunction(),\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# Store documents in Milvus\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b9072c",
   "metadata": {},
   "source": [
    "The above code inserts example documents into Milvus and builds index to enable BM25 ranking for full-text search. It disables dense embedding and utilizes `BM25BuiltInFunction` with default arguments in the Milvus vector store.\n",
    "\n",
    "You can specify the input and output fields for this function in the parameters of the `BM25BuiltInFunction`:\n",
    "\n",
    "- `input_field_names (str)`: The name of the input field, defaults to \"text\". It indicates which text field the BM25 algorithm applied to. Change this if using your own collection with a different name of the text field.\n",
    "- `output_field_names (str)`: The name of the output field, default is \"sparse_embedding\". It indicates which field this function outputs the computed result to.\n",
    "\n",
    "Now we have the vector store ready for retrieval. To query with full-text search through Milvus vector store, select the query mode of \"sparse\" or \"text_search\". Let's test with a sample question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d26ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author learned that the traditional approach to artificial intelligence, which involved explicit\n",
      "data structures representing concepts, was not effective in truly understanding natural language.\n",
      "This realization led the author to shift his focus to Lisp and eventually write a book about Lisp\n",
      "hacking. Additionally, the author learned the importance of focusing on projects that align with\n",
      "one's strengths and interests, as attention is a zero-sum game and choosing the right project is\n",
      "crucial to maximizing productivity and success.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    vector_store_query_mode=\"sparse\", similarity_top_k=5  # or \"text_search\"\n",
    ")\n",
    "answer = query_engine.query(\"What did the author learn?\")\n",
    "print(textwrap.fill(str(answer), 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92373153",
   "metadata": {},
   "source": [
    "#### Customize text analyzer\n",
    "\n",
    "Analyzers are essential in full-text search by breaking the sentence into tokens and performing lexical analysis like stemming and stop word removal. They are typically language-specific. To learn more about Milvus analyzers, you can refer to the [guide](https://milvus.io/docs/analyzer-overview.md#Analyzer-Overview).\n",
    "\n",
    "Milvus supports two types of analyzers: Built-in Analyzers and Custom Analyzers. By default, the `BM25BuiltInFunction` will use the standard built-in analyzer, which is the most basic analyzer that tokenizes the text with punctuation.\n",
    "\n",
    "To use a different analyzer or customize the current one, you can pass value to the argument `analyzer_params`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3e9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_function = BM25BuiltInFunction(\n",
    "    analyzer_params={\n",
    "        \"tokenizer\": \"standard\",\n",
    "        \"filter\": [\n",
    "            \"lowercase\",  # Built-in filter\n",
    "            {\"type\": \"length\", \"max\": 40},  # Custom filter\n",
    "            {\"type\": \"stop\", \"stop_words\": [\"of\", \"to\"]},  # Custom filter\n",
    "        ],\n",
    "    },\n",
    "    enable_match=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6897a455",
   "metadata": {},
   "source": [
    "### Hybrid Search with reranker\n",
    "\n",
    "Furthermore, we are able to build an optimized RAG system with hybrid search combining semantic search and full-text search.\n",
    "\n",
    "The following example uses OpenAI embedding for semantic search and BM25 for full-text search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1558b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 08:19:56,999 [DEBUG][_create_connection]: Created new connection using: b44cc978f84e4b7799832a81ffb7f99d (async_milvus_client.py:547)\n"
     ]
    }
   ],
   "source": [
    "# Create index over the documnts\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=URI,\n",
    "    # token=Token,\n",
    "    enable_dense=True,  # by default\n",
    "    dim=1536,\n",
    "    enable_sparse=True,\n",
    "    sparse_embedding_function=BM25BuiltInFunction(),\n",
    "    overwrite=True,\n",
    "    hybrid_ranker=\"RRFRanker\",  # by default\n",
    "    hybrid_ranker_params={},  # by default\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=\"default\",  # \"default\" will use OpenAI embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e94cab2",
   "metadata": {},
   "source": [
    "The above code stores documents in Milvus collection with both dense and sparse embedding fields. The dense field is used for OpenAI embedding outputs and the sparse field takes the sparse embedding function outputs. In this case, we use `BM25BuiltInFunction` as the sparse embedding function to allow full-text search.\n",
    "\n",
    "In this example, we use the reranking strategy \"RRFRanker\" with its default parameters. To customize reranker, you are able to configure `hybrid_ranker` and `hybrid_ranker_params`. For more details, you can refer to [Milvus Reranking](https://milvus.io/docs/reranking.md).\n",
    "\n",
    "Now let's test the RAG system with a sample question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e960b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author learned about programming on early computers like the IBM 1401 and later on\n",
      "microcomputers like the TRS-80. They also learned about Lisp programming and its association with\n",
      "Artificial Intelligence. Additionally, the author learned about the limitations of traditional AI\n",
      "approaches and the importance of focusing on practical applications and helping startups as an angel\n",
      "investor through their experiences with Y Combinator.\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "query_engine = index.as_query_engine(\n",
    "    vector_store_query_mode=\"hybrid\", similarity_top_k=5\n",
    ")\n",
    "answer = query_engine.query(\"What did the author learn?\")\n",
    "print(textwrap.fill(str(answer), 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
