{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1496f9de",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/MilvusHybridIndexDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b692c73",
   "metadata": {},
   "source": [
    "# Milvus Vector Store With Hybrid Retrieval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e7787c2",
   "metadata": {},
   "source": [
    "In this notebook we are going to show a quick demo of using the MilvusVectorStore with hybrid retrieval. (Milvus version should higher than 2.4.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f81e2c81",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-vector-stores-milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3ddfc5",
   "metadata": {},
   "source": [
    "BGE-M3 from FlagEmbedding is used as the default sparse embedding method, so it needs to be installed along with llama-index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b80700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index\n",
    "%pip install FlagEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47264e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Uncomment to see debug logs\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from IPython.display import Markdown, display\n",
    "import textwrap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9b97a89",
   "metadata": {},
   "source": [
    "### Setup OpenAI\n",
    "Lets first begin by adding the openai api key. This will allow us to access openai for embeddings and to use chatgpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f4d21-145a-401e-95ff-ccb259e8ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"sk-\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3d4e638",
   "metadata": {},
   "source": [
    "Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e24d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-25 17:44:59--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: â€˜data/paul_graham/paul_graham_essay.txtâ€™\n",
      "\n",
      "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2024-04-25 17:45:00 (994 KB/s) - â€˜data/paul_graham/paul_graham_essay.txtâ€™ saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir -p 'data/paul_graham/'\n",
    "%wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59ff935d",
   "metadata": {},
   "source": [
    "### Generate our data\n",
    "With our LLM set, lets start using the Milvus Index. As a first example, lets generate a document from the file found in the `data/paul_graham/` folder. In this folder there is a single essay from Paul Graham titled `What I Worked On`. To generate the documents we will use the SimpleDirectoryReader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cbd239-880e-41a3-98d8-dbb3fab55431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: ca3f5dbc-f772-41da-9a4f-bb4884691793\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "\n",
    "print(\"Document ID:\", documents[0].doc_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd270925",
   "metadata": {},
   "source": [
    "### Create an index across the data\n",
    "Now that we have a document, we can can create an index and insert the document. For the index we will use a MilvusVectorStore. MilvusVectorStore takes the following arguments related to hybrid index:\n",
    "\n",
    "> basic args\n",
    "- `uri (str, optional)`: The URI to connect to, comes in the form of \"https://address:port\" for Milvus or Zilliz Cloud service, or \"path/to/local/milvus.db\" for the lite local Milvus. Defaults to \"./milvus_llamaindex.db\".\n",
    "- `token (str, optional)`: The token for log in. Empty if not using rbac, if using rbac it will most likely be \"username:password\".\n",
    "- `collection_name (str, optional)`: The name of the collection where data will be stored. Defaults to \"llamalection\".\n",
    "- `overwrite (bool, optional)`: Whether to overwrite existing collection with the same name. Defaults to False.\n",
    "\n",
    "> dense field\n",
    "- `enable_dense (bool)`: A boolean flag to enable or disable dense embedding. Defaults to True.\n",
    "- `dim (int, optional)`: The dimension of the embedding vectors for the collection. Required when creating a new collection with enable_sparse is False.\n",
    "- `embedding_field (str, optional)`: The name of the dense embedding field for the collection, defaults to DEFAULT_EMBEDDING_KEY.\n",
    "- `index_config (dict, optional)`: The configuration used for building the dense embedding index. Defaults to None.\n",
    "- `search_config (dict, optional)`: The configuration used for searching the Milvus dense index. Note that this must be compatible with the index type specified by `index_config`. Defaults to None.\n",
    "- `similarity_metric (str, optional)`: The similarity metric to use for dense embedding, currently supports IP, COSINE and L2.\n",
    "\n",
    "> sparse field\n",
    "- `enable_sparse (bool)`: A boolean flag to enable or disable sparse embedding. Defaults to False.\n",
    "- `sparse_embedding_field (str)`: The name of sparse embedding field, defaults to DEFAULT_SPARSE_EMBEDDING_KEY.\n",
    "- `sparse_embedding_function (Union[BaseSparseEmbeddingFunction, BaseMilvusBuiltInFunction], optional)`: If enable_sparse is True, this object should be provided to convert text to a sparse embedding. If None, the default sparse embedding function (BGEM3SparseEmbeddingFunction) will be used.\n",
    "- `sparse_index_config (dict, optional)`: The configuration used to build the sparse embedding index. Defaults to None.\n",
    "\n",
    "> hybrid ranker\n",
    "- `hybrid_ranker (str)`: Specifies the type of ranker used in hybrid search queries. Currently only supports [\"RRFRanker\", \"WeightedRanker\"]. Defaults to \"RRFRanker\".\n",
    "- `hybrid_ranker_params (dict, optional)`: Configuration parameters for the hybrid ranker. The structure of this dictionary depends on the specific ranker being used:\n",
    "    - For \"RRFRanker\", it should include:\n",
    "        - \"k\" (int): A parameter used in Reciprocal Rank Fusion (RRF). This value is used to calculate the rank scores as part of the RRF algorithm, which combines multiple ranking strategies into a single score to improve search relevance.\n",
    "    - For \"WeightedRanker\", it expects:\n",
    "        - \"weights\" (list of float): A list of exactly two weights:\n",
    "            1. The weight for the dense embedding component.\n",
    "            2. The weight for the sparse embedding component.\n",
    "            These weights are used to adjust the importance of the dense and sparse components of the embeddings in the hybrid retrieval process.\n",
    "    Defaults to an empty dictionary, implying that the ranker will operate with its predefined default settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe075f5",
   "metadata": {},
   "source": [
    "Now, let's begin creating a MilvusVectorStore for hybrid retrieval. We need to set `enable_sparse` to True to enable sparse embedding generation, and we also need to configure the RRFRanker for reranking. For more details, please refer to [Milvus Reranking](https://milvus.io/docs/reranking.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1558b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sparse embedding function is not provided, using default.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a60f3c94f3d456b9c15876d021511bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------using 2*GPUs----------\n"
     ]
    }
   ],
   "source": [
    "# Create an index over the documnts\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    dim=1536,\n",
    "    overwrite=True,\n",
    "    enable_sparse=True,\n",
    "    hybrid_ranker=\"RRFRanker\",\n",
    "    hybrid_ranker_params={\"k\": 60},\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04304299-fc3e-40a0-8600-f50c3292767e",
   "metadata": {},
   "source": [
    "### Query the data\n",
    "Now that we have our document stored in the index, we can ask questions against the index while enable hybrid mode by specifying `vector_store_query_mode`. The index will use the data stored in itself as the knowledge base for chatgpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35369eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author learned that the field of AI, as practiced at the time, was not as promising as initially\n",
      "believed. The author realized that the approach of using explicit data structures to represent\n",
      "concepts in AI was not effective in truly understanding natural language. This led the author to\n",
      "shift focus from traditional AI to exploring Lisp for its own merits, ultimately deciding to write a\n",
      "book about Lisp hacking.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(vector_store_query_mode=\"hybrid\")\n",
    "response = query_engine.query(\"What did the author learn?\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99212d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealing with the stress and pressure related to managing Hacker News was a challenging moment for\n",
      "the author.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What was a hard moment for the author?\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c8b12",
   "metadata": {},
   "source": [
    "### Customized sparse embedding function \n",
    "\n",
    "Here, we are using the default sparse embedding function, which utilizes the [BGE-M3](https://arxiv.org/abs/2402.03216) model. Below, we describe how to prepare a customized sparse embedding function.\n",
    "\n",
    "You will need to create a class similar to ExampleEmbeddingFunction. This class should include methods such as:\n",
    "- encode_queries: This method converts texts into list of sparse embeddings for queries.\n",
    "- encode_documents: This method converts text into list of sparse embeddings for documents.\n",
    "\n",
    "The format of the sparse embedding is a dictionary, where the key (an integer) represents the dimension, and its corresponding value (a float) represents the embedding's magnitude in that dimension.(e.g., {1: 0.5, 2: 0.3})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a07def",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install FlagEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d7e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from typing import List\n",
    "from llama_index.vector_stores.milvus.utils import BaseSparseEmbeddingFunction\n",
    "\n",
    "\n",
    "class ExampleEmbeddingFunction(BaseSparseEmbeddingFunction):\n",
    "    def __init__(self):\n",
    "        self.model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=False)\n",
    "\n",
    "    def encode_queries(self, queries: List[str]):\n",
    "        outputs = self.model.encode(\n",
    "            queries,\n",
    "            return_dense=False,\n",
    "            return_sparse=True,\n",
    "            return_colbert_vecs=False,\n",
    "        )[\"lexical_weights\"]\n",
    "        return [self._to_standard_dict(output) for output in outputs]\n",
    "\n",
    "    def encode_documents(self, documents: List[str]):\n",
    "        outputs = self.model.encode(\n",
    "            documents,\n",
    "            return_dense=False,\n",
    "            return_sparse=True,\n",
    "            return_colbert_vecs=False,\n",
    "        )[\"lexical_weights\"]\n",
    "        return [self._to_standard_dict(output) for output in outputs]\n",
    "\n",
    "    def _to_standard_dict(self, raw_output):\n",
    "        result = {}\n",
    "        for k in raw_output:\n",
    "            result[int(k)] = raw_output[k]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52723bf",
   "metadata": {},
   "source": [
    "now we can use this in our hybrid retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c465a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e5ec8bd9a14dceb9ee4b4d1c66f38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------using 2*GPUs----------\n"
     ]
    }
   ],
   "source": [
    "vector_store = MilvusVectorStore(\n",
    "    dim=1536,\n",
    "    overwrite=True,\n",
    "    enable_sparse=True,\n",
    "    sparse_embedding_function=ExampleEmbeddingFunction(),\n",
    "    hybrid_ranker=\"RRFRanker\",\n",
    "    hybrid_ranker_params={\"k\": 60},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
