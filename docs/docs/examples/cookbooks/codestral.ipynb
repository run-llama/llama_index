{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/cookbooks/codestral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codestral from MistralAI Cookbook\n",
    "\n",
    "MistralAI released [codestral-latest](https://mistral.ai/news/mixtral-8x22b/) - a code model.\n",
    "\n",
    "It is a sparse Mixture-of-Experts (SMoE) model that uses only 39B active parameters out of 141B, offering unparalleled cost efficiency for its size with 64K tokens context window, multilingual, strong maths coding, coding and Function calling capabilities.\n",
    "\n",
    "This is a cook-book in showcasing the usage of `codestral-latest` model with llama-index."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"<YOUR MISTRAL API KEY>\"\n",
    "\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "\n",
    "llm = MistralAI(model=\"codestral-latest\", temperature=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruct mode usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a function for fibonacci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Sure, here is a simple Python function that calculates the Fibonacci sequence up to a given number `n`.\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    fib_sequence = [0, 1]\n",
      "    while fib_sequence[-1] < n:\n",
      "        fib_sequence.append(fib_sequence[-1] + fib_sequence[-2])\n",
      "    return fib_sequence[:-1]\n",
      "```\n",
      "\n",
      "This function starts with a list containing the first two numbers in the Fibonacci sequence (0 and 1). It then enters a loop where it continually appends the sum of the last two numbers in the list. This loop continues until the last number in the list is greater than or equal to `n`. At that point, the function returns the list, excluding the last number (which is greater than `n`).\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [ChatMessage(role=\"user\", content=\"Write a function for fibonacci\")]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a function to build RAG pipeline using LlamaIndex.\n",
    "\n",
    "Note: The output is mostly accurate, but it is based on an older LlamaIndex package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Sure, I can help you with that. Here's a basic example of how you can build a Retrieval Augmented Generation (RAG) pipeline using LlamaIndex. This example assumes that you have a collection of documents and a query.\n",
      "\n",
      "```python\n",
      "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
      "\n",
      "def build_rag_pipeline(documents_directory, query):\n",
      "    # Load documents\n",
      "    documents = SimpleDirectoryReader(documents_directory).load_data()\n",
      "\n",
      "    # Create an index\n",
      "    index = VectorStoreIndex.from_documents(documents)\n",
      "\n",
      "    # Query the index\n",
      "    query_engine = index.as_query_engine()\n",
      "    response = query_engine.query(query)\n",
      "\n",
      "    return response\n",
      "```\n",
      "\n",
      "In this function, `documents_directory` is the path to the directory containing your documents, and `query` is the question you want to ask. The function loads the documents, creates an index, and then queries the index with your question. The response is then returned.\n",
      "\n",
      "Please note that this is a simplified example. In a real-world application, you might want to add more functionality, such as error handling, logging, or more advanced querying. Also, you might want to use a more sophisticated method for loading and indexing your documents, depending on their format and the size of your dataset.\n",
      "\n",
      "Also, make sure to install the necessary libraries by running `pip install llama-index` in your terminal.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        content=\"Write a function to build RAG pipeline using LlamaIndex.\",\n",
    "    )\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anthropic_env",
   "language": "python",
   "name": "anthropic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
