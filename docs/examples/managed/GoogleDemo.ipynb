{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/managed/GoogleDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Generative Language Semantic Retriever\n",
    "\n",
    "In this notebook, we will show you how to get started quickly with using Google's Generative Language Semantic Retriever, which offers specialized embedding models for high quality retrieval and a tuned model for producing grounded output with customizable safety settings. We will also show you some advanced examples on how to combine the power of LlamaIndex and this unique offering from Google."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-gemini\n",
    "%pip install llama-index-vector-stores-google\n",
    "%pip install llama-index-indices-managed-google\n",
    "%pip install llama-index-response-synthesizers-google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index\n",
    "%pip install \"google-ai-generativelanguage>=0.4,<=1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Authentication Overview\n",
    "\n",
    "The Google Semantic Retriever API lets you perform semantic search on your own data. Since it's **your data**, this needs stricter access controls than API keys. Authenticate with OAuth with service accounts or through your user credentials (example in the bottom of the notebook).\n",
    "\n",
    "This quickstart uses a simplified authentication approach meant for a testing environment, and service account setup are typically easier to start from. Demo recording for authenticating using service accounts: [Demo](https://drive.google.com/file/d/199LzrdhuuiordS15MJAxVrPKAwEJGPOh/view?usp=sharing).\n",
    "\n",
    "For a production environment, learn about [authentication and authorization](https://developers.google.com/workspace/guides/auth-overview) before choosing the [access credentials](https://developers.google.com/workspace/guides/create-credentials#choose_the_access_credential_that_is_right_for_you) that are appropriate for your app.\n",
    "\n",
    "**Note**: At this time, the Google Generative AI Semantic Retriever API is [only available in certain regions](https://ai.google.dev/available_regions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup OAuth using service accounts\n",
    "\n",
    "Follow the steps below to setup OAuth using service accounts:\n",
    "\n",
    "\n",
    "1. Enable the [Generative Language API](https://console.cloud.google.com/flows/enableapi?apiid=generativelanguage.googleapis.com).\n",
    "\n",
    "2. Create the Service Account by following the [documentation](https://developers.google.com/identity/protocols/oauth2/service-account#creatinganaccount).\n",
    "\n",
    " * After creating the service account, generate a service account key.\n",
    "\n",
    "3. Upload your service account file by using the file icon on the left sidebar, then the upload icon, as shown in the screenshot below.\n",
    "\n",
    " * Rename the uploaded file to `service_account_key.json` or change the variable `service_account_file_name` in the code below.\n",
    "\n",
    "<img width=400 src=\"https://developers.generativeai.google/tutorials/images/colab_upload.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-auth-oauthlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from llama_index.vector_stores.google import set_google_config\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    \"service_account_key.json\",\n",
    "    scopes=[\n",
    "        \"https://www.googleapis.com/auth/generative-language.retriever\",\n",
    "    ],\n",
    ")\n",
    "set_google_config(auth_credentials=credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p 'data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's create some helper functions behind the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_index.core.vector_stores.google.generativeai.genai_extension as genaix\n",
    "from typing import Iterable\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "LLAMA_INDEX_COLAB_CORPUS_ID_PREFIX = f\"llama-index-colab\"\n",
    "SESSION_CORPUS_ID_PREFIX = (\n",
    "    f\"{LLAMA_INDEX_COLAB_CORPUS_ID_PREFIX}-{randrange(1000000)}\"\n",
    ")\n",
    "\n",
    "\n",
    "def corpus_id(num_id: int) -> str:\n",
    "    return f\"{SESSION_CORPUS_ID_PREFIX}-{num_id}\"\n",
    "\n",
    "\n",
    "SESSION_CORPUS_ID = corpus_id(1)\n",
    "\n",
    "\n",
    "def list_corpora() -> Iterable[genaix.Corpus]:\n",
    "    client = genaix.build_semantic_retriever()\n",
    "    yield from genaix.list_corpora(client=client)\n",
    "\n",
    "\n",
    "def delete_corpus(*, corpus_id: str) -> None:\n",
    "    client = genaix.build_semantic_retriever()\n",
    "    genaix.delete_corpus(corpus_id=corpus_id, client=client)\n",
    "\n",
    "\n",
    "def cleanup_colab_corpora():\n",
    "    for corpus in list_corpora():\n",
    "        if corpus.corpus_id.startswith(LLAMA_INDEX_COLAB_CORPUS_ID_PREFIX):\n",
    "            try:\n",
    "                delete_corpus(corpus_id=corpus.corpus_id)\n",
    "                print(f\"Deleted corpus {corpus.corpus_id}.\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "\n",
    "# Remove any previously leftover corpora from this colab.\n",
    "cleanup_colab_corpora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "A `corpus` is a collection of `document`s. A `document` is a body of text that is broken into `chunk`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.indices.managed.google import GoogleIndex\n",
    "from llama_index.core import Response\n",
    "import time\n",
    "\n",
    "# Create a corpus.\n",
    "index = GoogleIndex.create_corpus(\n",
    "    corpus_id=SESSION_CORPUS_ID, display_name=\"My first corpus!\"\n",
    ")\n",
    "print(f\"Newly created corpus ID is {index.corpus_id}.\")\n",
    "\n",
    "# Ingestion.\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "index.insert_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that what we've ingested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for corpus in list_corpora():\n",
    "    print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask the index a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying.\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did Paul Graham do growing up?\")\n",
    "assert isinstance(response, Response)\n",
    "\n",
    "# Show response.\n",
    "print(f\"Response is {response.response}\")\n",
    "\n",
    "# Show cited passages that were used to construct the response.\n",
    "for cited_text in [node.text for node in response.source_nodes]:\n",
    "    print(f\"Cited text: {cited_text}\")\n",
    "\n",
    "# Show answerability. 0 means not answerable from the passages.\n",
    "# 1 means the model is certain the answer can be provided from the passages.\n",
    "if response.metadata:\n",
    "    print(\n",
    "        f\"Answerability: {response.metadata.get('answerable_probability', 0)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Corpus\n",
    "\n",
    "There are various ways to create a corpus.\n",
    "\n",
    "```python\n",
    "# The Google server will provide a corpus ID for you.\n",
    "index = GoogleIndex.create_corpus(display_name=\"My first corpus!\")\n",
    "print(index.corpus_id)\n",
    "\n",
    "# You can also provide your own corpus ID. However, this ID needs to be globally\n",
    "# unique. You will get an exception if someone else has this ID already.\n",
    "index = GoogleIndex.create_corpus(\n",
    "    corpus_id=\"my-first-corpus\", display_name=\"My first corpus!\"\n",
    ")\n",
    "\n",
    "# If you do not provide any parameter, Google will provide ID and a default\n",
    "# display name for you.\n",
    "index = GoogleIndex.create_corpus()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing a Corpus\n",
    "\n",
    "Corpora you created persists on the Google servers under your account.\n",
    "You can use its ID to get a handle back.\n",
    "Then, you can query it, add more document to it, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a previously created corpus.\n",
    "index = GoogleIndex.from_corpus(corpus_id=SESSION_CORPUS_ID)\n",
    "\n",
    "# Query it again!\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Which company did Paul Graham build?\")\n",
    "assert isinstance(response, Response)\n",
    "\n",
    "# Show response.\n",
    "print(f\"Response is {response.response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing and Deleting Corpora\n",
    "\n",
    "See the Python library [google-generativeai](https://github.com/google/generative-ai-python) for further documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Documents\n",
    "\n",
    "Many node parsers and text splitters in LlamaIndex automatically add to each node a *source_node* to associate it to a file, e.g.\n",
    "\n",
    "```python\n",
    "    relationships={\n",
    "        NodeRelationship.SOURCE: RelatedNodeInfo(\n",
    "            node_id=\"abc-123\",\n",
    "            metadata={\"file_name\": \"Title for the document\"},\n",
    "        )\n",
    "    },\n",
    "```\n",
    "\n",
    "Both `GoogleIndex` and `GoogleVectorStore` recognize this source node,\n",
    "and will automatically create documents under your corpus on the Google servers.\n",
    "\n",
    "In case you are writing your own chunker, you should supply this source node relationship too like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeRelationship, RelatedNodeInfo, TextNode\n",
    "\n",
    "index = GoogleIndex.from_corpus(corpus_id=SESSION_CORPUS_ID)\n",
    "index.insert_nodes(\n",
    "    [\n",
    "        TextNode(\n",
    "            text=\"It was the best of times.\",\n",
    "            relationships={\n",
    "                NodeRelationship.SOURCE: RelatedNodeInfo(\n",
    "                    node_id=\"123\",\n",
    "                    metadata={\"file_name\": \"Tale of Two Cities\"},\n",
    "                )\n",
    "            },\n",
    "        ),\n",
    "        TextNode(\n",
    "            text=\"It was the worst of times.\",\n",
    "            relationships={\n",
    "                NodeRelationship.SOURCE: RelatedNodeInfo(\n",
    "                    node_id=\"123\",\n",
    "                    metadata={\"file_name\": \"Tale of Two Cities\"},\n",
    "                )\n",
    "            },\n",
    "        ),\n",
    "        TextNode(\n",
    "            text=\"Bugs Bunny: Wassup doc?\",\n",
    "            relationships={\n",
    "                NodeRelationship.SOURCE: RelatedNodeInfo(\n",
    "                    node_id=\"456\",\n",
    "                    metadata={\"file_name\": \"Bugs Bunny Adventure\"},\n",
    "                )\n",
    "            },\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your nodes do not have a source node, then Google server will put your nodes in a default document under your corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing and Deleting Documents\n",
    "\n",
    "See the Python library [google-generativeai](https://github.com/google/generative-ai-python) for further documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Corpus\n",
    "\n",
    "Google's query engine is backed by a specially tuned LLM that grounds its response based on retrieved passages. For each response, an *answerability probability* is returned to indicate how confident the LLM was in answering the question from the retrieved passages.\n",
    "\n",
    "Furthermore, Google's query engine supports *answering styles*, such as `ABSTRACTIVE` (succint but abstract), `EXTRACTIVE` (very brief and extractive) and `VERBOSE` (extra details).\n",
    "\n",
    "The engine also supports *safety settings*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.ai.generativelanguage import (\n",
    "    GenerateAnswerRequest,\n",
    "    HarmCategory,\n",
    "    SafetySetting,\n",
    ")\n",
    "\n",
    "index = GoogleIndex.from_corpus(corpus_id=SESSION_CORPUS_ID)\n",
    "query_engine = index.as_query_engine(\n",
    "    # We recommend temperature between 0 and 0.2.\n",
    "    temperature=0.2,\n",
    "    # See package `google-generativeai` for other voice styles.\n",
    "    answer_style=GenerateAnswerRequest.AnswerStyle.ABSTRACTIVE,\n",
    "    # See package `google-generativeai` for additional safety settings.\n",
    "    safety_setting=[\n",
    "        SafetySetting(\n",
    "            category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "            threshold=SafetySetting.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "        ),\n",
    "        SafetySetting(\n",
    "            category=HarmCategory.HARM_CATEGORY_VIOLENCE,\n",
    "            threshold=SafetySetting.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What was Bugs Bunny's favorite saying?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the Python library [google-generativeai](https://github.com/google/generative-ai-python) for further documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Response\n",
    "\n",
    "response = query_engine.query(\"What were Paul Graham's achievements?\")\n",
    "assert isinstance(response, Response)\n",
    "\n",
    "# Show response.\n",
    "print(f\"Response is {response.response}\")\n",
    "\n",
    "# Show cited passages that were used to construct the response.\n",
    "for cited_text in [node.text for node in response.source_nodes]:\n",
    "    print(f\"Cited text: {cited_text}\")\n",
    "\n",
    "# Show answerability. 0 means not answerable from the passages.\n",
    "# 1 means the model is certain the answer can be provided from the passages.\n",
    "if response.metadata:\n",
    "    print(\n",
    "        f\"Answerability: {response.metadata.get('answerable_probability', 0)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced RAG\n",
    "\n",
    "The `GoogleIndex` is built based on `GoogleVectorStore` and `GoogleTextSynthesizer`.\n",
    "These components can be combined with other powerful constructs in LlamaIndex to produce advanced RAG applications.\n",
    "\n",
    "Below we show a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, you need an API key. Get one from [AI Studio](https://makersuite.google.com/app/apikey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "GEMINI_API_KEY = \"\"  # @param {type:\"string\"}\n",
    "gemini = Gemini(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranker + Google Retriever\n",
    "\n",
    "Converting content into vectors is a lossy process. LLM-based Reranking\n",
    "remediates this by reranking the retrieved content using LLM, which has higher\n",
    "fidelity because it has access to both the actual query and the passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response_synthesizers.google import GoogleTextSynthesizer\n",
    "from llama_index.vector_stores.google import GoogleVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "# Set up the query engine with a reranker.\n",
    "store = GoogleVectorStore.from_corpus(corpus_id=SESSION_CORPUS_ID)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=store,\n",
    ")\n",
    "response_synthesizer = GoogleTextSynthesizer.from_defaults(\n",
    "    temperature=0.2,\n",
    "    answer_style=GenerateAnswerRequest.AnswerStyle.ABSTRACTIVE,\n",
    ")\n",
    "reranker = LLMRerank(\n",
    "    top_n=10,\n",
    "    llm=gemini,\n",
    ")\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=VectorIndexRetriever(\n",
    "        index=index,\n",
    "        similarity_top_k=20,\n",
    "    ),\n",
    "    node_postprocessors=[reranker],\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# Query.\n",
    "response = query_engine.query(\"What were Paul Graham's achievements?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Query + Google Retriever\n",
    "\n",
    "Sometimes, a user's query can be too complex. You may get better retrieval result if you break down the original query into smaller, better focused queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform.base import (\n",
    "    StepDecomposeQueryTransform,\n",
    ")\n",
    "from llama_index.core.query_engine import MultiStepQueryEngine\n",
    "\n",
    "# Set up the query engine with multi-turn query-rewriter.\n",
    "store = GoogleVectorStore.from_corpus(corpus_id=SESSION_CORPUS_ID)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=store,\n",
    ")\n",
    "response_synthesizer = GoogleTextSynthesizer.from_defaults(\n",
    "    temperature=0.2,\n",
    "    answer_style=GenerateAnswerRequest.AnswerStyle.ABSTRACTIVE,\n",
    ")\n",
    "single_step_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "step_decompose_transform = StepDecomposeQueryTransform(\n",
    "    llm=gemini,\n",
    "    verbose=True,\n",
    ")\n",
    "query_engine = MultiStepQueryEngine(\n",
    "    query_engine=single_step_query_engine,\n",
    "    query_transform=step_decompose_transform,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    index_summary=\"Ask me anything.\",\n",
    "    num_steps=6,\n",
    ")\n",
    "\n",
    "# Query.\n",
    "response = query_engine.query(\"What were Paul Graham's achievements?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE + Google Retriever\n",
    "\n",
    "When you can write prompt that would produce fake answers that share many traits\n",
    "with the real answer, you can try HyDE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "\n",
    "# Set up the query engine with multi-turn query-rewriter.\n",
    "store = GoogleVectorStore.from_corpus(corpus_id=SESSION_CORPUS_ID)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=store,\n",
    ")\n",
    "response_synthesizer = GoogleTextSynthesizer.from_defaults(\n",
    "    temperature=0.2,\n",
    "    answer_style=GenerateAnswerRequest.AnswerStyle.ABSTRACTIVE,\n",
    ")\n",
    "base_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "hyde = HyDEQueryTransform(\n",
    "    llm=gemini,\n",
    "    include_original=False,\n",
    ")\n",
    "hyde_query_engine = TransformQueryEngine(base_query_engine, hyde)\n",
    "\n",
    "# Query.\n",
    "response = query_engine.query(\"What were Paul Graham's achievements?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Query + Reranker + HyDE + Google Retriever\n",
    "\n",
    "Or combine them all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google's retriever and AQA model setup.\n",
    "store = GoogleVectorStore.from_corpus(corpus_id=SESSION_CORPUS_ID)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=store,\n",
    ")\n",
    "response_synthesizer = GoogleTextSynthesizer.from_defaults(\n",
    "    temperature=0.2, answer_style=GenerateAnswerRequest.AnswerStyle.ABSTRACTIVE\n",
    ")\n",
    "\n",
    "# Reranker setup.\n",
    "reranker = LLMRerank(\n",
    "    top_n=10,\n",
    "    llm=gemini,\n",
    ")\n",
    "single_step_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=20,\n",
    "    node_postprocessors=[reranker],\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# HyDE setup.\n",
    "hyde = HyDEQueryTransform(\n",
    "    llm=gemini,\n",
    "    include_original=False,\n",
    ")\n",
    "hyde_query_engine = TransformQueryEngine(single_step_query_engine, hyde)\n",
    "\n",
    "# Multi-query setup.\n",
    "step_decompose_transform = StepDecomposeQueryTransform(\n",
    "    llm=gemini, verbose=True\n",
    ")\n",
    "query_engine = MultiStepQueryEngine(\n",
    "    query_engine=hyde_query_engine,\n",
    "    query_transform=step_decompose_transform,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    index_summary=\"Ask me anything.\",\n",
    "    num_steps=6,\n",
    ")\n",
    "\n",
    "# Query.\n",
    "response = query_engine.query(\"What were Paul Graham's achievements?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup corpora created in the colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_colab_corpora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Setup OAuth with user credentials\n",
    "\n",
    "Please follow [OAuth Quickstart](https://developers.generativeai.google/tutorials/oauth_quickstart) to setup OAuth using user credentials. Below are overview of steps from the documentation that are required.\n",
    "\n",
    "1. Enable the `Generative Language API`: [Documentation](https://developers.generativeai.google/tutorials/oauth_quickstart#1_enable_the_api)\n",
    "\n",
    "1. Configure the OAuth consent screen: [Documentation](https://developers.generativeai.google/tutorials/oauth_quickstart#2_configure_the_oauth_consent_screen)\n",
    "\n",
    "1. Authorize credentials for a desktop application: [Documentation](https://developers.generativeai.google/tutorials/oauth_quickstart#3_authorize_credentials_for_a_desktop_application)\n",
    "  * If you want to run this notebook in Colab start by uploading your\n",
    "`client_secret*.json` file using the \"File > Upload\" option.\n",
    "\n",
    " * Rename the uploaded file to `client_secret.json` or change the variable `client_file_name` in the code below.\n",
    "\n",
    "<img width=400 src=\"https://developers.generativeai.google/tutorials/images/colab_upload.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace TODO-your-project-name with the project used in the OAuth Quickstart\n",
    "project_name = \"TODO-your-project-name\"  #  @param {type:\"string\"}\n",
    "# Replace TODO-your-email@gmail.com with the email added as a test user in the OAuth Quickstart\n",
    "email = \"TODO-your-email@gmail.com\"  #  @param {type:\"string\"}\n",
    "# Replace client_secret.json with the client_secret_* file name you uploaded.\n",
    "client_file_name = \"client_secret.json\"\n",
    "\n",
    "# IMPORTANT: Follow the instructions from the output - you must copy the command\n",
    "# to your terminal and copy the output after authentication back here.\n",
    "!gcloud config set project $project_name\n",
    "!gcloud config set account $email\n",
    "\n",
    "# NOTE: The simplified project setup in this tutorial triggers a \"Google hasn't verified this app.\" dialog.\n",
    "# This is normal, click \"Advanced\" -> \"Go to [app name] (unsafe)\"\n",
    "!gcloud auth application-default login --no-browser --client-id-file=$client_file_name --scopes=\"https://www.googleapis.com/auth/generative-language.retriever,https://www.googleapis.com/auth/cloud-platform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will provide you with a URL, which you should enter into your local browser.\n",
    "Follow the instruction to complete the authentication and authorization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
