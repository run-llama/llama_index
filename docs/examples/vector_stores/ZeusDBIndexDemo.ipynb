{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c177cbd-3774-4317-b9ae-1afedcf00b17",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/vector_stores/ZeusDBIndexDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c760fca-af3a-4116-b75d-27b87656a9e9",
   "metadata": {},
   "source": [
    "# ZeusDB Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dcf44a-6aa5-4e8b-8dba-d096f0d74209",
   "metadata": {},
   "source": [
    "This document explains how to use ZeusDB as a vector store in LlamaIndex. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e512769f-6cb2-458a-a673-9cfe5d0f5d9f",
   "metadata": {},
   "source": [
    "[ZeusDB](https://www.zeusdb.com) is a high-performance vector database written in Rust, offering features like product quantization, persistent storage, and enterprise-grade logging. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c4e2d7-80b6-4b7e-9ba3-89e60d2ea528",
   "metadata": {},
   "source": [
    "Follow these instructions and examples below to enhance your LlamaIndex apps with ZeusDB's production capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ed4632-0ae2-45cd-bbb7-87034f47396f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce9d72-00da-4738-b18b-b752a14f1586",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb5ebe6-6b72-468a-b6e1-54f342518a06",
   "metadata": {},
   "source": [
    "Install the ZeusDB LlamaIndex integration package from PyPi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d694a171-023f-4873-a406-e26bce46df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install llama-index-vector-stores-zeusdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863b8c9-d723-4093-bd19-7db3ffd94fde",
   "metadata": {},
   "source": [
    "*Setup in Jupyter Notebooks*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f320c4-f9ea-45a9-9267-da84b0409c58",
   "metadata": {},
   "source": [
    "> üí° Tip: If you‚Äôre working inside Jupyter or Google Colab, use the %pip magic command so the package is installed into the active kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e101a7a-eb30-459f-9252-9de93a54ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-vector-stores-zeusdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d894de-fb54-4a51-aa3c-6278fe364ec0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bbd268-108c-49c2-a253-8b4644480ea9",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d717cfc5-9d0a-4eec-9768-9677d82893dd",
   "metadata": {},
   "source": [
    "This example uses OpenAIEmbeddings, which requires an OpenAI API key ‚Äì [Get your OpenAI API key here](https://platform.openai.com/api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b4dd9-0f80-4251-919c-e6d2fe258abd",
   "metadata": {},
   "source": [
    "Install the LlamaIndex Core and OpenAI integration packages from PyPi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd590d-58ef-49da-a5ff-b9673ff078fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install llama-index-core\n",
    "pip install llama-index-llms-openai\n",
    "pip install llama-index-embeddings-openai\n",
    "\n",
    "# Use these commands if inside Jupyter Notebooks\n",
    "# %pip install llama-index-core\n",
    "# %pip install llama-index-llms-openai\n",
    "# %pip install llama-index-embeddings-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9f61f4-742c-40b4-93ed-bf61cb1e6d85",
   "metadata": {},
   "source": [
    "#### Please choose an option below for your OpenAI key integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc0d74c-dc82-4b48-82f6-179618ecfbdb",
   "metadata": {},
   "source": [
    "*Option 1: üîë Enter your API key each time*  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bb6169-e34c-4c25-b607-c6e29ec60b60",
   "metadata": {},
   "source": [
    "Use getpass in Jupyter to securely input your key for the current session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd1f59-7308-4ba5-96f3-84f99c263c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5422513a-4f60-4de7-ade2-3d1baff8f8e5",
   "metadata": {},
   "source": [
    "*Option 2: üóÇÔ∏è Use a .env file*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787eb08c-703c-4fc2-b7f1-26d66bc32829",
   "metadata": {},
   "source": [
    "Keep your key in a local .env file and load it automatically with python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b696af7-77ef-49c5-8198-8b077f550243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # reads .env and sets OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97b7a5-7d1b-4690-ba5b-b506976638b0",
   "metadata": {},
   "source": [
    "üéâüéâ That's it! You are good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206d1c9-5b75-4386-b69f-03a6f21afb36",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0201e1-3863-4ceb-a338-0a535bf2d770",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14584c6d-d5ee-4fc6-8426-7cbb48fe6d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Packages and Classes\n",
    "from llama_index.core import VectorStoreIndex, Document, StorageContext\n",
    "from llama_index.vector_stores.zeusdb import ZeusDBVectorStore\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3798bb8-0f85-4fb0-b4d5-c0b55695e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up embedding model and LLM\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "Settings.llm = OpenAI(model=\"gpt-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6097ef9a-4ab0-43f5-b7e1-61df1821018d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca8ee11-5adb-43e8-8e20-ced5b91d9f8c",
   "metadata": {},
   "source": [
    "## Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42960e47-48ba-41ca-a1fe-03ba695d1714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ZeusDB vector store\n",
    "vector_store = ZeusDBVectorStore(\n",
    "    dim=1536, distance=\"cosine\", index_type=\"hnsw\"  # OpenAI embedding dimension\n",
    ")\n",
    "\n",
    "# Create storage context\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Create documents\n",
    "documents = [\n",
    "    Document(text=\"ZeusDB is a high-performance vector database.\"),\n",
    "    Document(text=\"LlamaIndex provides RAG capabilities.\"),\n",
    "    Document(text=\"Vector search enables semantic similarity.\"),\n",
    "]\n",
    "\n",
    "# Create index and store documents\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "\n",
    "# Query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is ZeusDB?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a53714-e531-4cfe-95c5-a957bb33a1c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373dd643-b8c0-49ff-9d27-41e99a6ca3a0",
   "metadata": {},
   "source": [
    "## Direct Query Interface Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7509be-1953-442a-9f02-3114f610777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import VectorStoreQuery\n",
    "\n",
    "# Create query\n",
    "embed_model = Settings.embed_model\n",
    "query_embedding = embed_model.get_text_embedding(\"machine learning\")\n",
    "\n",
    "query_obj = VectorStoreQuery(query_embedding=query_embedding, similarity_top_k=2)\n",
    "\n",
    "# Execute query\n",
    "results = vector_store.query(query_obj)\n",
    "\n",
    "# Results contain IDs and similarities\n",
    "print(f\"Found {len(results.ids or [])} results:\")\n",
    "for node_id, similarity in zip(results.ids or [], results.similarities or []):\n",
    "    print(f\"  ID: {node_id}, Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053344dd-871e-4e1c-b851-31548e46fc80",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6e17e7-c3f8-476d-9d4d-8207d1e1dd82",
   "metadata": {},
   "source": [
    "## MMR Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce77669-8ee6-4d65-9307-92d7ca74c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMR search via direct query\n",
    "mmr_results = vector_store.query(\n",
    "    query_obj,\n",
    "    mmr=True,\n",
    "    fetch_k=10,\n",
    "    mmr_lambda=0.7,  # 0.0=max diversity, 1.0=pure relevance\n",
    ")\n",
    "\n",
    "print(f\"MMR Results: {len(mmr_results.ids or [])} items (with diversity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340fa93-2018-4423-859a-d9a6a5096f21",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71a5be3-b7e9-4cf8-bd93-30cfce02ed2a",
   "metadata": {},
   "source": [
    "## Search with Metadata Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696bad5f-1c32-4137-8349-65b89f8bd9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import (\n",
    "    MetadataFilters,\n",
    "    FilterOperator,\n",
    "    FilterCondition,\n",
    ")\n",
    "\n",
    "# Create a fresh vector store for this example\n",
    "vector_store = ZeusDBVectorStore(dim=1536, distance=\"cosine\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Create documents with metadata\n",
    "documents_with_meta = [\n",
    "    Document(\n",
    "        text=\"Python is great for data science\",\n",
    "        metadata={\"category\": \"tech\", \"year\": 2024},\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"JavaScript is for web development\",\n",
    "        metadata={\"category\": \"tech\", \"year\": 2023},\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"Climate change impacts ecosystems\",\n",
    "        metadata={\"category\": \"environment\", \"year\": 2024},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Build index with metadata\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents_with_meta, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# Create metadata filter\n",
    "filters = MetadataFilters.from_dicts(\n",
    "    [\n",
    "        {\"key\": \"category\", \"value\": \"tech\", \"operator\": FilterOperator.EQ},\n",
    "        {\"key\": \"year\", \"value\": 2024, \"operator\": FilterOperator.GTE},\n",
    "    ],\n",
    "    condition=FilterCondition.AND,\n",
    ")\n",
    "\n",
    "# Use the retriever with filters (recommended approach)\n",
    "retriever = index.as_retriever(similarity_top_k=5, filters=filters)\n",
    "filtered_results = retriever.retrieve(\"programming\")\n",
    "\n",
    "# Process results\n",
    "for r in filtered_results:\n",
    "    print(f\"- {r.node.get_content(metadata_mode='none')}\")\n",
    "    print(f\"  Metadata: {r.node.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f288b835-6150-4a07-bb9d-891c073ec377",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556fb9ef-ec59-496e-a054-7424ea4b36e7",
   "metadata": {},
   "source": [
    "## Save and Load indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50747f8a-e010-412b-9c0e-e118503488fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save index\n",
    "save_path = \"my_index.zdb\"\n",
    "vector_store.save_index(save_path)\n",
    "print(f\"‚úÖ Index saved to {save_path}\")\n",
    "print(f\"   Vector count: {vector_store.get_vector_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea3708-b096-4597-a394-dddb15e144ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load index\n",
    "loaded_store = ZeusDBVectorStore.load_index(save_path)\n",
    "print(f\"‚úÖ Index loaded from {save_path}\")\n",
    "print(f\"   Vector count: {loaded_store.get_vector_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92122449-26ee-481c-9ead-70c82316fcf5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4665eb68-7d5e-40f0-8ab3-25b62ff97b18",
   "metadata": {},
   "source": [
    "## Quantization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b7f87f-c399-4efb-b109-6993ee0aadf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create quantized vector store for memory efficiency\n",
    "quantization_config = {\n",
    "    \"type\": \"pq\",\n",
    "    \"subvectors\": 8,\n",
    "    \"bits\": 8,\n",
    "    \"training_size\": 1000,\n",
    "    \"storage_mode\": \"quantized_only\",\n",
    "}\n",
    "\n",
    "vector_store = ZeusDBVectorStore(\n",
    "    dim=1536,\n",
    "    distance=\"cosine\",\n",
    "    index_type=\"hnsw\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "# Check quantization status\n",
    "print(f\"Is quantized: {vector_store.is_quantized()}\")\n",
    "print(f\"Can use quantization: {vector_store.can_use_quantization()}\")\n",
    "print(f\"Training progress: {vector_store.get_training_progress():.1f}%\")\n",
    "print(f\"Storage mode: {vector_store.get_storage_mode()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8b0a1c-5bf1-458f-af50-5accb0950033",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb4f859-2b2d-481a-ae34-3fc551d6824b",
   "metadata": {},
   "source": [
    "## Delete Operations Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d467cd-7ea7-4afe-8f61-826c9f2b1299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, Document, StorageContext\n",
    "from llama_index.vector_stores.zeusdb import ZeusDBVectorStore\n",
    "\n",
    "# Create a fresh vector store for this example\n",
    "delete_vs = ZeusDBVectorStore(dim=1536, distance=\"cosine\")\n",
    "delete_sc = StorageContext.from_defaults(vector_store=delete_vs)\n",
    "\n",
    "# Create documents\n",
    "delete_docs = [Document(text=f\"Document {i}\", metadata={\"doc_id\": i}) for i in range(5)]\n",
    "\n",
    "# Build index\n",
    "delete_index = VectorStoreIndex.from_documents(delete_docs, storage_context=delete_sc)\n",
    "\n",
    "print(f\"Before delete: {delete_vs.get_vector_count()} vectors\")\n",
    "\n",
    "# Get node IDs to delete\n",
    "retriever = delete_index.as_retriever(similarity_top_k=10)\n",
    "results = retriever.retrieve(\"document\")\n",
    "\n",
    "if results:\n",
    "    # Extract node IDs from results\n",
    "    node_ids_to_delete = [result.node.node_id for result in results[:2]]\n",
    "    print(f\"Deleting node IDs: {node_ids_to_delete[0][:8]}...\")\n",
    "\n",
    "    # Delete by node IDs\n",
    "    delete_vs.delete_nodes(node_ids=node_ids_to_delete)\n",
    "    print(f\"After delete: {delete_vs.get_vector_count()} vectors\")\n",
    "    print(\"‚úÖ delete_nodes(node_ids=[...]) works!\")\n",
    "\n",
    "# Demonstrate unsupported delete by ref_doc_id\n",
    "try:\n",
    "    delete_vs.delete(ref_doc_id=\"doc_1\")\n",
    "    print(\"‚ùå Should have raised NotImplementedError\")\n",
    "except NotImplementedError as e:\n",
    "    print(\"‚ùå delete(ref_doc_id='...') raises NotImplementedError\")\n",
    "    print(f\"   (This is expected - not supported by backend)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a455833-7fd5-4fe5-9d1c-d7c20fab2441",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e9a2b-590a-46a8-8f97-bf10f35235ee",
   "metadata": {},
   "source": [
    "## Async Operations Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51f3dde-3247-4480-9e05-3465614ed82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "# In Jupyter, use nest_asyncio to handle event loops\n",
    "try:\n",
    "    import nest_asyncio\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "async def async_operations():\n",
    "    # Create nodes\n",
    "    nodes = [TextNode(text=f\"Document {i}\", metadata={\"doc_id\": i}) for i in range(10)]\n",
    "\n",
    "    # Generate embeddings (required before adding)\n",
    "    embed_model = Settings.embed_model\n",
    "    for node in nodes:\n",
    "        node.embedding = embed_model.get_text_embedding(node.text)\n",
    "\n",
    "    # Add nodes asynchronously\n",
    "    node_ids = await vector_store.async_add(nodes)\n",
    "    print(f\"Added {len(node_ids)} nodes\")\n",
    "\n",
    "    # Query asynchronously\n",
    "    query_embedding = embed_model.get_text_embedding(\"document\")\n",
    "    query_obj = VectorStoreQuery(query_embedding=query_embedding, similarity_top_k=3)\n",
    "\n",
    "    results = await vector_store.aquery(query_obj)\n",
    "    print(f\"Found {len(results.ids or [])} results\")\n",
    "\n",
    "    # Delete asynchronously\n",
    "    await vector_store.adelete_nodes(node_ids=node_ids[:2])\n",
    "    print(f\"Deleted 2 nodes, {vector_store.get_vector_count()} remaining\")\n",
    "\n",
    "\n",
    "# Run async function\n",
    "await async_operations()  # In Jupyter\n",
    "# asyncio.run(async_operations())  # In regular Python scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12926879-24b7-46cb-9c0c-f5e46e36a0c3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d9f49-dac7-4d7d-9519-c6a5f68cb85a",
   "metadata": {},
   "source": [
    "## Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed7b40-1f87-4307-909c-80ddb1e082f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get index statistics\n",
    "stats = vector_store.get_zeusdb_stats()\n",
    "print(f\"Key stats: vectors={stats.get('total_vectors')}, space={stats.get('space')}\")\n",
    "\n",
    "# Get vector count\n",
    "count = vector_store.get_vector_count()\n",
    "print(f\"Vector count: {count}\")\n",
    "\n",
    "# Get detailed index info\n",
    "info = vector_store.info()\n",
    "print(f\"Index info: {info}\")\n",
    "\n",
    "# Check quantization status\n",
    "if vector_store.is_quantized():\n",
    "    progress = vector_store.get_training_progress()\n",
    "    quant_info = vector_store.get_quantization_info()\n",
    "    print(f\"Quantization: {progress:.1f}% complete\")\n",
    "    print(f\"Compression: {quant_info['compression_ratio']:.1f}x\")\n",
    "else:\n",
    "    print(\"Index is not quantized\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
