{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upstash Vector Store\n",
    "\n",
    "We're going to look at how to use LlamaIndex to interface with Upstash Vector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q llama-index upstash-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.vector_stores import UpstashVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "import textwrap\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the OpenAI API\n",
    "openai.api_key = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-03 20:04:25--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n",
      "\n",
      "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2024-02-03 20:04:25 (5.96 MB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download data\n",
    "! mkdir -p 'data/paul_graham/'\n",
    "! wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the documents using the LlamaIndex SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Documents: 1\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "\n",
    "print(\"# Documents:\", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an index on Upstash, visit https://console.upstash.com/vector, create an index with 1536 dimensions and `Cosine` distance metric. Copy the URL and token below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = UpstashVectorStore(url=\"https://...\", token=\"...\")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've successfully created an index and populated it with vectors from the essay! The data will take a second to index and then it'll be ready for querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author learned that the study of philosophy in college did not live up to their expectations.\n",
      "They found that other fields took up most of the space of ideas, leaving little room for what they\n",
      "perceived as the ultimate truths that philosophy was supposed to explore. As a result, they decided\n",
      "to switch to studying AI.\n",
      "\n",
      "\n",
      "The author's opinion on startups is that they are in need of help and support, especially in the\n",
      "beginning stages. The author believes that founders of startups are often helpless and face various\n",
      "challenges, such as getting incorporated and understanding the intricacies of running a company. The\n",
      "author's investment firm, Y Combinator, aims to provide seed funding and comprehensive support to\n",
      "startups, offering them the guidance and resources they need to succeed.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "res1 = query_engine.query(\"What did the author learn?\")\n",
    "print(textwrap.fill(str(res1), 100))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "res2 = query_engine.query(\"What is the author's opinion on startups?\")\n",
    "print(textwrap.fill(str(res2), 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Filtering\n",
    "\n",
    "You can pass `MetadataFilters` with your `VectorStoreQuery` to filter the nodes returned from Upstash vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from llama_index.vector_stores.upstash import UpstashVectorStore\n",
    "from llama_index.core.vector_stores.types import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    "    FilterOperator,\n",
    ")\n",
    "\n",
    "vector_store = UpstashVectorStore(\n",
    "    url=os.environ.get(\"UPSTASH_VECTOR_URL\") or \"\",\n",
    "    token=os.environ.get(\"UPSTASH_VECTOR_TOKEN\") or \"\",\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(\n",
    "            key=\"author\", value=\"Marie Curie\", operator=FilterOperator.EQ\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "retriever = index.as_retriever(filters=filters)\n",
    "\n",
    "retriever.retrieve(\"What is inception about?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine multiple `MetadataFilters` with `AND` or `OR` condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import FilterOperator, FilterCondition\n",
    "\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(\n",
    "            key=\"theme\",\n",
    "            value=[\"Fiction\", \"Horror\"],\n",
    "            operator=FilterOperator.IN,\n",
    "        ),\n",
    "        MetadataFilter(key=\"year\", value=1997, operator=FilterOperator.GT),\n",
    "    ],\n",
    "    condition=FilterCondition.AND,\n",
    ")\n",
    "\n",
    "retriever = index.as_retriever(filters=filters)\n",
    "retriever.retrieve(\"Harry Potter?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
