{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b692c73",
   "metadata": {},
   "source": [
    "# Redis Vector Store"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e7787c2",
   "metadata": {},
   "source": [
    "In this notebook we are going to show a quick demo of using the RedisVectorStore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47264e32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T12:20:23.988789Z",
     "start_time": "2023-02-10T12:20:23.967877Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import textwrap\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# stop huggingface warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Uncomment to see debug logs\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, Document\n",
    "from llama_index.vector_stores import RedisVectorStore\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c692310",
   "metadata": {},
   "source": [
    "### Start Redis\n",
    "\n",
    "The easiest way to start Redis as a vector database is using the [redis-stack](https://hub.docker.com/r/redis/redis-stack) docker image.\n",
    "\n",
    "To follow every step of this tutorial, launch the image as follows:\n",
    "\n",
    "```bash\n",
    "docker run --name redis-vecdb -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n",
    "```\n",
    "\n",
    "This will also launch the RedisInsight UI on port 8001 which you can view at http://localhost:8001.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9b97a89",
   "metadata": {},
   "source": [
    "### Setup OpenAI\n",
    "Lets first begin by adding the openai api key. This will allow us to access openai for embeddings and to use chatgpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9f4d21-145a-401e-95ff-ccb259e8ef84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T12:20:24.908956Z",
     "start_time": "2023-02-10T12:20:24.537064Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-<your key here>\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59ff935d",
   "metadata": {},
   "source": [
    "### Read in a dataset\n",
    "Here we will use a set of Paul Graham essays to provide the text to turn into embeddings, store in a ``RedisVectorStore`` and query to find context for our LLM QnA loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68cbd239-880e-41a3-98d8-dbb3fab55431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T12:20:30.175678Z",
     "start_time": "2023-02-10T12:20:30.172456Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: faa23c94-ac9e-4763-92ba-e0f87bf38195 Document Hash: 77ae91ab542f3abb308c4d7c77c9bc4c9ad0ccd63144802b7cbe7e1bb3a4094e\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"../data/paul_graham\").load_data()\n",
    "print(\"Document ID:\", documents[0].doc_id, \"Document Hash:\", documents[0].doc_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce335c9a",
   "metadata": {},
   "source": [
    "You can process your files individually using [SimpleDirectoryReader]([Title](https://gpt-index.readthedocs.io/en/latest/examples/data_connectors/simple_directory_reader.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f67cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = SimpleDirectoryReader(\"../data/paul_graham\")\n",
    "documents = loader.load_data()\n",
    "for file in loader.input_files:\n",
    "    print(file)\n",
    "    # Here is where you would do any preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd270925",
   "metadata": {},
   "source": [
    "### Initialize the Redis Vector Store\n",
    "\n",
    "Now we have our documents read in, we can initialize the Redis Vector Store. This will allow us to store our vectors in Redis and create an index.\n",
    "\n",
    "Here is the docstring for the RedisVectorStore:\n",
    "\n",
    "```python\n",
    "class RedisVectorStore(VectorStore):\n",
    "    \n",
    "def __init__(\n",
    "        self,\n",
    "        index_name: str,\n",
    "        index_prefix: str = \"llama_index\",\n",
    "        prefix_ending: str = \"/vector\",\n",
    "        index_args: Optional[Dict[str, Any]] = None,\n",
    "        metadata_fields: Optional[List[str]] = None,\n",
    "        redis_url: str = \"redis://localhost:6379\",\n",
    "        overwrite: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize RedisVectorStore.\n",
    "\n",
    "        For index arguments that can be passed to RediSearch, see\n",
    "        https://redis.io/docs/stack/search/reference/vectors/\n",
    "\n",
    "        The index arguments will depend on the index type chosen. There\n",
    "        are two available index types\n",
    "            - FLAT: a flat index that uses brute force search\n",
    "            - HNSW: a hierarchical navigable small world graph index\n",
    "\n",
    "        Args:\n",
    "            index_name (str): Name of the index.\n",
    "            index_prefix (str): Prefix for the index. Defaults to \"llama_index\".\n",
    "                The actual prefix used by Redis will be\n",
    "                \"{index_prefix}{prefix_ending}\".\n",
    "            prefix_ending (str): Prefix ending for the index. Be careful when\n",
    "                changing this: https://github.com/jerryjliu/llama_index/pull/6665.\n",
    "                Defaults to \"/vector\".\n",
    "            index_args (Dict[str, Any]): Arguments for the index. Defaults to None.\n",
    "            metadata_fields (List[str]): List of metadata fields to store in the index (only supports TAG fields).\n",
    "            redis_url (str): URL for the redis instance.\n",
    "                Defaults to \"redis://localhost:6379\".\n",
    "            overwrite (bool): Whether to overwrite the index if it already exists.\n",
    "                Defaults to False.\n",
    "            kwargs (Any): Additional arguments to pass to the redis client.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If redis-py is not installed\n",
    "            ValueError: If RediSearch is not installed\n",
    "\n",
    "        Examples:\n",
    "            >>> from llama_index.vector_stores.redis import RedisVectorStore\n",
    "            >>> # Create a RedisVectorStore\n",
    "            >>> vector_store = RedisVectorStore(\n",
    "            >>>     index_name=\"my_index\",\n",
    "            >>>     index_prefix=\"llama_index\",\n",
    "            >>>     index_args={\"algorithm\": \"HNSW\", \"m\": 16, \"ef_construction\": 200,\n",
    "                \"distance_metric\": \"cosine\"},\n",
    "            >>>     redis_url=\"redis://localhost:6379/\",\n",
    "            >>>     overwrite=True)\n",
    "\n",
    "        \"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba1558b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T12:20:33.735897Z",
     "start_time": "2023-02-10T12:20:30.404245Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "vector_store = RedisVectorStore(\n",
    "    index_name=\"pg_essays\",\n",
    "    index_prefix=\"llama\",\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    overwrite=True,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b481dc47",
   "metadata": {},
   "source": [
    "With logging on, it prints out the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53288b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFO:llama_index.vector_stores.redis:Creating index pg_essays\n",
    "Creating index pg_essays\n",
    "INFO:llama_index.vector_stores.redis:Added 15 documents to index pg_essays\n",
    "Added 15 documents to index pg_essays\n",
    "INFO:llama_index.vector_stores.redis:Saving index to disk in background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02256f9",
   "metadata": {},
   "source": [
    "Now you can browse these index in redis-cli and read/write it as Redis hash. It looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1fb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "$ redis-cli\n",
    "127.0.0.1:6379> keys *\n",
    " 1) \"llama/vector_0f125320-f5cf-40c2-8462-aefc7dbff490\"\n",
    " 2) \"llama/vector_bd667698-4311-4a67-bb8b-0397b03ec794\"\n",
    "127.0.0.1:6379> HGETALL \"llama/vector_bd667698-4311-4a67-bb8b-0397b03ec794\"\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405d445f",
   "metadata": {},
   "source": [
    "### Handle duplicated index\n",
    "\n",
    "Regardless of whether overwrite=True is used in RedisVectorStore(), the process of generating the index and storing data in Redis still takes time. Currently, it is necessary to implement your own logic to manage duplicate indexes. One possible approach is to set a flag in Redis to indicate the readiness of the index. If the flag is set, you can bypass the index generation step and directly load the index from Redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "r = redis.Redis()\n",
    "index_name = \"pg_essays\"\n",
    "r.set(f\"added:{index_name}\", \"true\")\n",
    "\n",
    "# Later in code\n",
    "if r.get(f\"added:{index_name}\"):\n",
    "    # Skip to deploy your index, restore it. Please see \"Restore index from Redis\" section below. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04304299-fc3e-40a0-8600-f50c3292767e",
   "metadata": {},
   "source": [
    "# Query the data\n",
    "Now that we have our document stored in the index, we can ask questions against the index. The index will use the data stored in itself as the knowledge base for ChatGPT. The default setting for as_query_engine() utilizes OpenAI embeddings and ChatGPT as the language model. Therefore, an OpenAI key is required unless you opt for a customized or local language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35369eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T12:20:51.328762Z",
     "start_time": "2023-02-10T12:20:33.822688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The author learned that it is possible to publish essays online, and that working on things that\n",
      "are not prestigious can be a sign that one is on the right track. They also learned that impure\n",
      "motives can lead ambitious people astray, and that it is possible to make connections with people\n",
      "through cleverly planned events. Finally, the author learned that they could find love through a\n",
      "chance meeting at a party.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author learn?\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99212d33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T12:21:10.337294Z",
     "start_time": "2023-02-10T12:20:51.338718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A hard moment for the author was when he realized that he had been working on things that weren't\n",
      "prestigious. He had been drawn to these types of work despite their lack of prestige, and he was\n",
      "worried that his ambition was leading him astray. He was also concerned that people would give him a\n",
      "\"glassy eye\" when he explained what he was writing.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What was a hard moment for the author?\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d7bc976",
   "metadata": {},
   "source": [
    "### Saving and Loading\n",
    "\n",
    "Redis allows the user to perform backups in the background or synchronously. With Llamaindex, the ``RedisVectorStore.persist()`` function can be used to trigger such a backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09836567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redis  redisinsight\n"
     ]
    }
   ],
   "source": [
    "!docker exec -it redis-vecdb ls /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93ef500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.persist(persist_path=\"\")  # persist_path means nothing for RedisVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed5ab256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump.rdb  redis  redisinsight\n"
     ]
    }
   ],
   "source": [
    "!docker exec -it redis-vecdb ls /data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8849ba",
   "metadata": {},
   "source": [
    "### Restore index from Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95817a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "redisURL = \"redis://localhost:6379\"\n",
    "index_name = \"pg_essays\"\n",
    "vector_store = RedisVectorStore(\n",
    "        index_name=index_name,\n",
    "        index_prefix=\"llama\",\n",
    "        redis_url=redisURL,\n",
    "        overwrite=True,\n",
    "    )\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12c9f2",
   "metadata": {},
   "source": [
    "Now you can reuse your index as discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437dc580",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgQuery = index.as_query_engine()\n",
    "pgQuery.query(\"What is the meaning of life?\")\n",
    "# or\n",
    "pgRetriever = index.as_retriever()\n",
    "pgRetriever.retrieve(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b185f",
   "metadata": {},
   "source": [
    "Learn more about [query_engine](https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/query_engine/root.html)  and [retriveve]([Title](https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/retriever/usage_pattern.html))."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52b975a7",
   "metadata": {},
   "source": [
    "### Deleting documents or index completely\n",
    "\n",
    "Sometimes it may be useful to delete documents or the entire index. This can be done using the `delete` and `delete_index` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6fe322f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'faa23c94-ac9e-4763-92ba-e0f87bf38195'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id = documents[0].doc_id\n",
    "document_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae4fb2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents 20\n"
     ]
    }
   ],
   "source": [
    "redis_client = vector_store.client\n",
    "print(\"Number of documents\", len(redis_client.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ce45788",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.delete(document_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a1ac683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents\", len(redis_client.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c380605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets delete the index entirely (happens in the background, may take a second)\n",
    "# this will delete all the documents and the index\n",
    "vector_store.delete_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "474ad4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents\", len(redis_client.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61b67496",
   "metadata": {},
   "source": [
    "# Working with Metadata\n",
    "\n",
    "RedisVectorStore supports adding metadata and then using it in your queries (for example, to limit the scope of documents retrieved). However, there are a couple of important caveats:\n",
    "1. Currently, only [Tag fields](https://redis.io/docs/stack/search/reference/tags/) are supported, and only with exact match.\n",
    "2. You must declare the metadata when creating the index (usually when initializing RedisVectorStore). If you do not do this, your queries will come back empty. There is no way to modify an existing index after it had already been created (this is a Redis limitation).\n",
    "\n",
    "Here's how to work with Metadata:\n",
    "\n",
    "\n",
    "### When **creating** the index\n",
    "\n",
    "Make sure to declare the metadata when you **first** create the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9889ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = RedisVectorStore(\n",
    "    index_name=\"pg_essays_with_metadata\",\n",
    "    index_prefix=\"llama\",\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    overwrite=True,\n",
    "    metadata_fields=[\"user_id\", \"favorite_color\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8d6dc21",
   "metadata": {},
   "source": [
    "Note: the field names `text`, `doc_id`, `id` and the name of your vector field (`vector` by default) should **not** be used as metadata field names, as they are are reserved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "429947d5",
   "metadata": {},
   "source": [
    "### When adding a document\n",
    "\n",
    "Add your metadata under the `metadata` key. You can add metadata to documents you load in just by looping over them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89781b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 6a5aa8dd-2771-454b-befc-bcfc311d2008 Document Hash: 77ae91ab542f3abb308c4d7c77c9bc4c9ad0ccd63144802b7cbe7e1bb3a4094e Metadata: {'user_id': '12345', 'favorite_color': 'blue'}\n"
     ]
    }
   ],
   "source": [
    "# load your documents normally, then add your metadata\n",
    "documents = SimpleDirectoryReader(\"../data/paul_graham\").load_data()\n",
    "\n",
    "for document in documents:\n",
    "    document.metadata = {\"user_id\": \"12345\", \"favorite_color\": \"blue\"}\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "\n",
    "# load documents\n",
    "print(\n",
    "    \"Document ID:\",\n",
    "    documents[0].doc_id,\n",
    "    \"Document Hash:\",\n",
    "    documents[0].doc_hash,\n",
    "    \"Metadata:\",\n",
    "    documents[0].metadata,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42b24e76",
   "metadata": {},
   "source": [
    "### When querying the index\n",
    "\n",
    "To filter by your metadata fields, include one or more of your metadata keys, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b01f346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The author learned that it was possible to publish anything online, and that working on things that\n",
      "weren't prestigious could lead to discovering something real. They also learned that impure motives\n",
      "were a big danger for the ambitious, and that it was possible for programs not to terminate.\n",
      "Finally, they learned that computers were expensive in those days, and that they could write\n",
      "programs on the IBM 1401.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.vector_stores.types import MetadataFilters, ExactMatchFilter\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    filters=MetadataFilters(\n",
    "        filters=[\n",
    "            ExactMatchFilter(key=\"user_id\", value=\"12345\"),\n",
    "            ExactMatchFilter(key=\"favorite_color\", value=\"blue\"),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What did the author learn?\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07514f85",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "In case you run into issues retrieving your documents from the index, you might get a message similar to this.\n",
    "```\n",
    "No docs found on index 'pg_essays' with prefix 'llama' and filters '(@user_id:{12345} & @favorite_color:{blue})'.\n",
    "* Did you originally create the index with a different prefix?\n",
    "* Did you index your metadata fields when you created the index?\n",
    "```\n",
    "\n",
    "If you get this error, there a couple of gotchas to be aware of when working with Redis:\n",
    "#### Prefix issues\n",
    "\n",
    "If you first create your index with a specific `prefix` but later change that prefix in your code, your query will come back empty. Redis saves the prefix your originally created your index with and expects it to be consistent.\n",
    "\n",
    "To see what prefix your index was created with, you can run `FT.INFO <name of your index>` in the Redis CLI and look under `index_definition` => `prefixes`.\n",
    "\n",
    "#### Empty queries when using metadata\n",
    "\n",
    "If you add metadata to the index *after* it has already been created and then try to query over that metadata, your queries will come back empty.\n",
    "\n",
    "Redis indexes fields upon index creation only (similar to how it indexes the prefixes, above).\n",
    "\n",
    "If you have an existing index and want to make sure it's dropped, you can run `FT.DROPINDEX <name of your index>` in the Redis CLI. Note that this will *not* drop your actual data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c09d1199",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
