{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6463dfe0-31f0-494e-995e-9d3b96db0eeb",
   "metadata": {},
   "source": [
    "# Azure Cosmos DB No SQL Vector Store\n",
    "\n",
    "In this notebook we are going to show a quick demo of how to use AzureCosmosDBNoSqlVectorSearch to perform vector searches in LlamaIndex.\n",
    "\n",
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d865e38e-7cfb-44fc-a811-ccbbb6bd5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-embeddings-openai\n",
    "%pip install llama-index-llms-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed73758-4a14-4c9e-a4de-7c9c584fbdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609a213-479b-4924-8a31-07f9076bcb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf060d-7ab1-4a56-8098-4fb306d3401e",
   "metadata": {},
   "source": [
    "# Setup Azure OpenAI\n",
    "\n",
    "The first step is to configure the llm and the embeddings model. These models will be used to create embeddings for the documents loaded into the database and for llm completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c386e-b118-4cef-aabe-37f78e478f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(\n",
    "    model=\"AZURE_OPENAI_MODEL\",\n",
    "    deployment_name=\"AZURE_OPENAI_DEPLOYMENT_NAME\",\n",
    "    azure_endpoint=\"AZURE_OPENAI_BASE\",\n",
    "    api_key=\"AZURE_OPENAI_KEY\",\n",
    "    api_version=\"AZURE_OPENAI_VERSION\",\n",
    ")\n",
    "\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"AZURE_OPENAI_EMBEDDING_MODEL\",\n",
    "    deployment_name=\"AZURE_OPENAI_EMBEDDING_MODEL_DEPLOYMENT_NAME\",\n",
    "    azure_endpoint=\"AZURE_OPENAI_BASE\",\n",
    "    api_key=\"AZURE_OPENAI_KEY\",\n",
    "    api_version=\"AZURE_OPENAI_VERSION\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da35d45-9689-4f3a-9011-1cda0fb361ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084aa964-7222-47b2-bdab-825c85a6ffed",
   "metadata": {},
   "source": [
    "# Loading Documents\n",
    "\n",
    "In this example we will be using the paul_graham essay which will be processed by the SimpleDirectoryReader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f689978-93c6-4c34-9a6e-9fca606a1058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[r\"\\docs\\examples\\data\\paul_graham\\paul_graham_essay.txt\"]\n",
    ").load_data()\n",
    "\n",
    "print(\"Document ID:\", documents[0].doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c5f4bf-411e-482d-8ada-580dad6575ee",
   "metadata": {},
   "source": [
    "# Create the index\n",
    "\n",
    "Here we establish the connection to cosmos db nosql and create a vector store index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb1251c-8bbb-416d-9c32-c7260d039900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cosmos import CosmosClient, PartitionKey\n",
    "from llama_index.vector_stores.azurecosmosnosql import (\n",
    "    AzureCosmosDBNoSqlVectorSearch,\n",
    ")\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# create cosmos client\n",
    "URI = \"AZURE_COSMOSDB_URI\"\n",
    "KEY = \"AZURE_COSMOSDB_KEY\"\n",
    "client = CosmosClient(URI, credential=KEY)\n",
    "\n",
    "# specify vector store properties\n",
    "indexing_policy = {\n",
    "    \"indexingMode\": \"consistent\",\n",
    "    \"includedPaths\": [{\"path\": \"/*\"}],\n",
    "    \"excludedPaths\": [{\"path\": '/\"_etag\"/?'}],\n",
    "    \"vectorIndexes\": [{\"path\": \"/embedding\", \"type\": \"quantizedFlat\"}],\n",
    "}\n",
    "\n",
    "vector_embedding_policy = {\n",
    "    \"vectorEmbeddings\": [\n",
    "        {\n",
    "            \"path\": \"/embedding\",\n",
    "            \"dataType\": \"float32\",\n",
    "            \"distanceFunction\": \"cosine\",\n",
    "            \"dimensions\": 3072,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "partition_key = PartitionKey(path=\"/id\")\n",
    "cosmos_container_properties_test = {\"partition_key\": partition_key}\n",
    "cosmos_database_properties_test = {}\n",
    "\n",
    "# create vector store\n",
    "store = AzureCosmosDBNoSqlVectorSearch(\n",
    "    cosmos_client=client,\n",
    "    vector_embedding_policy=vector_embedding_policy,\n",
    "    indexing_policy=indexing_policy,\n",
    "    cosmos_container_properties=cosmos_container_properties_test,\n",
    "    cosmos_database_properties=cosmos_database_properties_test,\n",
    "    create_container=True,\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70671760-c408-4f94-b4c8-f9b7aad47644",
   "metadata": {},
   "source": [
    "# Query the index\n",
    "We can now ask questions using our index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930a6143-62c9-4377-8955-0c05bfb7e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author love working on?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c572a6cd-34db-47e1-897c-a03048173882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
