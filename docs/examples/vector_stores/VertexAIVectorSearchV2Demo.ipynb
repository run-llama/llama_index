{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/vector_stores/VertexAIVectorSearchV2Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Vertex AI Vector Search v2.0\n",
    "\n",
    "This notebook demonstrates how to use **Vertex AI Vector Search v2.0** with LlamaIndex.\n",
    "\n",
    "> [Vertex AI Vector Search v2.0](https://cloud.google.com/vertex-ai/docs/vector-search/overview) introduces a simplified **collection-based architecture** that eliminates the need for separate index creation and endpoint deployment.\n",
    "\n",
    "## v2.0 vs v1.0\n",
    "\n",
    "| Feature | v1.0 | v2.0 |\n",
    "|---------|------|------|\n",
    "| Architecture | Index + Endpoint | Collection |\n",
    "| Setup Steps | Create index â†’ Deploy to endpoint | Create collection |\n",
    "| GCS Bucket | Required for batch updates | Not needed |\n",
    "\n",
    "**Note**: For v1.0 usage, see [VertexAIVectorSearchDemo.ipynb](./VertexAIVectorSearchDemo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Install LlamaIndex with v2 support:\n",
    "\n",
    "> **Note**: V2 support requires `llama-index-vector-stores-vertexaivectorsearch` version that supports Vertex AI Vector Search v2.0 API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install with v2 support (the [v2] extra installs google-cloud-vectorsearch)\n",
    "# !pip install 'llama-index-vector-stores-vertexaivectorsearch[v2]' llama-index-embeddings-vertex llama-index-llms-vertex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication(if using Colab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab authentication.\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()\n",
    "    print(\"Authenticated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your Google Cloud project details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Cloud Configuration\n",
    "PROJECT_ID = \"your-project-id\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "COLLECTION_ID = \"llamaindex-demo-collection\"  # @param {type:\"string\"}\n",
    "\n",
    "# Embedding dimensions (768 for textembedding-gecko@003)\n",
    "EMBEDDING_DIMENSION = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a v2 Collection\n",
    "\n",
    "Unlike v1.0 which requires creating an index and deploying it to an endpoint, v2.0 only requires creating a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import vectorsearch_v1beta\n",
    "\n",
    "# Initialize the client\n",
    "client = vectorsearch_v1beta.VectorSearchServiceClient()\n",
    "\n",
    "# Check if collection already exists\n",
    "parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "collection_name = f\"{parent}/collections/{COLLECTION_ID}\"\n",
    "\n",
    "try:\n",
    "    request = vectorsearch_v1beta.GetCollectionRequest(name=collection_name)\n",
    "    collection = client.get_collection(request=request)\n",
    "    print(f\"Collection already exists: {collection.name}\")\n",
    "except Exception as e:\n",
    "    if \"404\" in str(e) or \"NotFound\" in str(e):\n",
    "        print(f\"Creating collection: {COLLECTION_ID}\")\n",
    "        request = vectorsearch_v1beta.CreateCollectionRequest(\n",
    "            parent=parent,\n",
    "            collection_id=COLLECTION_ID,\n",
    "            collection={\n",
    "                \"data_schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"text\": {\"type\": \"string\"},\n",
    "                        \"ref_doc_id\": {\"type\": \"string\"},\n",
    "                        \"price\": {\"type\": \"number\"},\n",
    "                        \"color\": {\"type\": \"string\"},\n",
    "                        \"category\": {\"type\": \"string\"},\n",
    "                    },\n",
    "                },\n",
    "                \"vector_schema\": {\n",
    "                    \"embedding\": {\n",
    "                        \"dense_vector\": {\"dimensions\": EMBEDDING_DIMENSION}\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        operation = client.create_collection(request=request)\n",
    "        collection = operation.result()\n",
    "        print(f\"Collection created: {collection.name}\")\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up LlamaIndex Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from llama_index.core import Settings, StorageContext, VectorStoreIndex\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.vector_stores.types import (\n",
    "    MetadataFilters,\n",
    "    MetadataFilter,\n",
    "    FilterOperator,\n",
    ")\n",
    "from llama_index.embeddings.vertex import VertexTextEmbedding\n",
    "from llama_index.llms.vertex import Vertex\n",
    "from llama_index.vector_stores.vertexaivectorsearch import VertexAIVectorStore\n",
    "\n",
    "# Authentication - get default credentials\n",
    "import google.auth\n",
    "\n",
    "credentials, project = google.auth.default()\n",
    "print(f\"Authenticated with project: {project}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure embedding model\n",
    "embed_model = VertexTextEmbedding(\n",
    "    model_name=\"text-embedding-004\",\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    credentials=credentials,\n",
    ")\n",
    "\n",
    "# Configure LLM\n",
    "llm = Vertex(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    credentials=credentials,\n",
    ")\n",
    "\n",
    "# Set as defaults\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm\n",
    "\n",
    "print(\"Embedding model and LLM configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create v2 Vector Store\n",
    "\n",
    "Creating a v2 vector store is simple - just specify `api_version=\"v2\"` and your `collection_id`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create v2 vector store\n",
    "vector_store = VertexAIVectorStore(\n",
    "    api_version=\"v2\",  # Use v2 API\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    collection_id=COLLECTION_ID,\n",
    "    # No index_id, endpoint_id, or gcs_bucket_name needed!\n",
    ")\n",
    "\n",
    "print(f\"Vector store created with api_version={vector_store.api_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Documents\n",
    "\n",
    "### Simple Text Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some sample text nodes\n",
    "texts = [\n",
    "    \"LlamaIndex is a data framework for LLM applications.\",\n",
    "    \"Vertex AI Vector Search provides scalable vector similarity search.\",\n",
    "    \"RAG combines retrieval with generation for better AI responses.\",\n",
    "    \"Embeddings convert text into numerical vectors for similarity matching.\",\n",
    "]\n",
    "\n",
    "# Create nodes with embeddings\n",
    "nodes = [\n",
    "    TextNode(text=text, embedding=embed_model.get_text_embedding(text))\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "# Add to vector store\n",
    "ids = vector_store.add(nodes)\n",
    "print(f\"Added {len(ids)} nodes to vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample product data with metadata\n",
    "products = [\n",
    "    {\n",
    "        \"description\": \"A versatile pair of dark-wash denim jeans. Made from durable cotton with a classic straight-leg cut.\",\n",
    "        \"price\": 65.00,\n",
    "        \"color\": \"blue\",\n",
    "        \"category\": \"pants\",\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"A lightweight linen button-down shirt in crisp white. Perfect for keeping cool with breathable fabric.\",\n",
    "        \"price\": 34.99,\n",
    "        \"color\": \"white\",\n",
    "        \"category\": \"shirts\",\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"A soft chunky knit sweater in vibrant forest green. Oversized fit and cozy wool blend.\",\n",
    "        \"price\": 89.99,\n",
    "        \"color\": \"green\",\n",
    "        \"category\": \"sweaters\",\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"Classic crewneck t-shirt in heathered blue. Comfortable cotton jersey, a wardrobe essential.\",\n",
    "        \"price\": 19.99,\n",
    "        \"color\": \"blue\",\n",
    "        \"category\": \"shirts\",\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"Tailored black trousers in comfortable stretch fabric. Perfect for work or dressy events.\",\n",
    "        \"price\": 59.99,\n",
    "        \"color\": \"black\",\n",
    "        \"category\": \"pants\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create nodes with metadata\n",
    "product_nodes = []\n",
    "for product in products:\n",
    "    text = product.pop(\"description\")\n",
    "    embedding = embed_model.get_text_embedding(text)\n",
    "    node = TextNode(\n",
    "        text=text,\n",
    "        embedding=embedding,\n",
    "        metadata=product,  # remaining fields become metadata\n",
    "    )\n",
    "    product_nodes.append(node)\n",
    "\n",
    "# Add to vector store\n",
    "ids = vector_store.add(product_nodes)\n",
    "print(f\"Added {len(ids)} product nodes with metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the Vector Store\n",
    "\n",
    "### Simple Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index from vector store\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, embed_model=embed_model\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "# Query\n",
    "results = retriever.retrieve(\"comfortable pants for work\")\n",
    "\n",
    "print(\"Search Results:\")\n",
    "print(\"-\" * 60)\n",
    "for result in results:\n",
    "    print(f\"Score: {result.get_score():.3f}\")\n",
    "    print(f\"Text: {result.get_text()[:100]}...\")\n",
    "    print(f\"Metadata: {result.metadata}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search with Metadata Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by color\n",
    "filters = MetadataFilters(filters=[MetadataFilter(key=\"color\", value=\"blue\")])\n",
    "\n",
    "retriever = index.as_retriever(filters=filters, similarity_top_k=3)\n",
    "results = retriever.retrieve(\"casual clothing\")\n",
    "\n",
    "print(\"Blue items only:\")\n",
    "print(\"-\" * 60)\n",
    "for result in results:\n",
    "    print(\n",
    "        f\"Score: {result.get_score():.3f} | Color: {result.metadata.get('color')}\"\n",
    "    )\n",
    "    print(f\"Text: {result.get_text()[:80]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by price range\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(key=\"price\", operator=FilterOperator.LT, value=50.0),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever = index.as_retriever(filters=filters, similarity_top_k=3)\n",
    "results = retriever.retrieve(\"clothing\")\n",
    "\n",
    "print(\"Items under $50:\")\n",
    "print(\"-\" * 60)\n",
    "for result in results:\n",
    "    print(\n",
    "        f\"Score: {result.get_score():.3f} | Price: ${result.metadata.get('price')}\"\n",
    "    )\n",
    "    print(f\"Text: {result.get_text()[:80]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Query with LLM\n",
    "\n",
    "Use the vector store with an LLM for retrieval-augmented generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query engine\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "# Ask a question\n",
    "response = query_engine.query(\n",
    "    \"What blue clothing items do you have and what are their prices?\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Question: What blue clothing items do you have and what are their prices?\"\n",
    ")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Answer: {response.response}\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Sources:\")\n",
    "for node in response.source_nodes:\n",
    "    print(f\"  - {node.text[:60]}... (score: {node.score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2-Only Features\n",
    "\n",
    "### Delete Specific Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete specific nodes by ID\n",
    "# vector_store.delete_nodes(node_ids=[\"node_id_1\", \"node_id_2\"])\n",
    "print(\"delete_nodes() - Delete specific nodes by their IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear All Data (v2 only)\n",
    "\n",
    "v2 supports clearing all data from a collection - this is NOT available in v1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear all data from the collection\n",
    "# WARNING: This deletes ALL data in the collection!\n",
    "# vector_store.clear()\n",
    "print(\"clear() - Clears all data from collection (v2 only!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Delete the collection when done to avoid charges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANUP = True  # Set to True to delete the collection\n",
    "\n",
    "if CLEANUP:\n",
    "    from google.cloud import vectorsearch_v1beta\n",
    "\n",
    "    client = vectorsearch_v1beta.VectorSearchServiceClient()\n",
    "    collection_name = (\n",
    "        f\"projects/{PROJECT_ID}/locations/{REGION}/collections/{COLLECTION_ID}\"\n",
    "    )\n",
    "\n",
    "    print(f\"Deleting collection: {collection_name}\")\n",
    "    client.delete_collection(name=collection_name)\n",
    "    print(\"Collection deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Simple Setup**: v2 only requires a collection - no index/endpoint deployment\n",
    "2. **Easy Integration**: Just add `api_version=\"v2\"` to use the new API\n",
    "3. **Same Interface**: All LlamaIndex operations (add, query, delete) work the same\n",
    "4. **New Features**: v2 adds `clear()` method not available in v1\n",
    "\n",
    "### Migration from v1\n",
    "\n",
    "```python\n",
    "# v1 (old)\n",
    "vector_store = VertexAIVectorStore(\n",
    "    project_id=\"...\",\n",
    "    region=\"...\",\n",
    "    index_id=\"projects/.../indexes/123\",\n",
    "    endpoint_id=\"projects/.../indexEndpoints/456\",\n",
    "    gcs_bucket_name=\"my-bucket\"\n",
    ")\n",
    "\n",
    "# v2 (new)\n",
    "vector_store = VertexAIVectorStore(\n",
    "    api_version=\"v2\",\n",
    "    project_id=\"...\",\n",
    "    region=\"...\",\n",
    "    collection_id=\"my-collection\"\n",
    ")\n",
    "```\n",
    "\n",
    "For detailed migration instructions, see [V2_MIGRATION.md](https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/vector_stores/llama-index-vector-stores-vertexaivectorsearch/V2_MIGRATION.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
