{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/multi_modal/multi_modal_video_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Multimodal RAG for processing videos using OpenAI GPT4V and LanceDB vectorstore\n",
    "\n",
    "In this notebook, we showcase a Multimodal RAG architecture designed for video processing. We utilize OpenAI GPT4V MultiModal LLM class that employs [CLIP](https://github.com/openai/CLIP) to generate multimodal embeddings. Furthermore, we use [LanceDBVectorStore](https://docs.llamaindex.ai/en/latest/examples/vector_stores/LanceDBIndexDemo.html#) for efficient vector storage.\n",
    "\n",
    "\n",
    "\n",
    "Steps:\n",
    "1. Download video from YouTube, process and store it.\n",
    "\n",
    "2. Build Multi-Modal index and vector store for both texts and images.\n",
    "\n",
    "3. Retrieve relevant images and context, use both to augment the prompt.\n",
    "\n",
    "4. Using GPT4V for reasoning the correlations between the input query and augmented data and generating final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-vector-stores-lancedb\n",
    "%pip install llama-index-multi-modal-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-multi-modal-llms-openai\n",
    "%pip install llama-index-vector-stores-lancedb\n",
    "%pip install llama-index-embeddings-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama_index ftfy regex tqdm\n",
    "%pip install -U openai-whisper\n",
    "%pip install git+https://github.com/openai/CLIP.git\n",
    "%pip install torch torchvision\n",
    "%pip install matplotlib scikit-image\n",
    "%pip install lancedb\n",
    "%pip install moviepy\n",
    "%pip install pytube\n",
    "%pip install pydub\n",
    "%pip install SpeechRecognition\n",
    "%pip install ffmpeg-python\n",
    "%pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from pathlib import Path\n",
    "import speech_recognition as sr\n",
    "from pytube import YouTube\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_TOKEN = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set configuration for input below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"https://www.youtube.com/watch?v=d_qvLDhkg00\"\n",
    "output_video_path = \"./video_data/\"\n",
    "output_folder = \"./mixed_data/\"\n",
    "output_audio_path = \"./mixed_data/output_audio.wav\"\n",
    "\n",
    "filepath = output_video_path + \"input_vid.mp4\"\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and process videos into appropriate format for generating/storing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_images(image_paths):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for img_path in image_paths:\n",
    "        if os.path.isfile(img_path):\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            plt.subplot(2, 3, images_shown + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            images_shown += 1\n",
    "            if images_shown >= 7:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(url, output_path):\n",
    "    \"\"\"\n",
    "    Download a video from a given url and save it to the output path.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The url of the video to download.\n",
    "    output_path (str): The path to save the video to.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the metadata of the video.\n",
    "    \"\"\"\n",
    "    yt = YouTube(url)\n",
    "    metadata = {\"Author\": yt.author, \"Title\": yt.title, \"Views\": yt.views}\n",
    "    yt.streams.get_highest_resolution().download(\n",
    "        output_path=output_path, filename=\"input_vid.mp4\"\n",
    "    )\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def video_to_images(video_path, output_folder):\n",
    "    \"\"\"\n",
    "    Convert a video to a sequence of images and save them to the output folder.\n",
    "\n",
    "    Parameters:\n",
    "    video_path (str): The path to the video file.\n",
    "    output_folder (str): The path to the folder to save the images to.\n",
    "\n",
    "    \"\"\"\n",
    "    clip = VideoFileClip(video_path)\n",
    "    clip.write_images_sequence(\n",
    "        os.path.join(output_folder, \"frame%04d.png\"), fps=0.2\n",
    "    )\n",
    "\n",
    "\n",
    "def video_to_audio(video_path, output_audio_path):\n",
    "    \"\"\"\n",
    "    Convert a video to audio and save it to the output path.\n",
    "\n",
    "    Parameters:\n",
    "    video_path (str): The path to the video file.\n",
    "    output_audio_path (str): The path to save the audio to.\n",
    "\n",
    "    \"\"\"\n",
    "    clip = VideoFileClip(video_path)\n",
    "    audio = clip.audio\n",
    "    audio.write_audiofile(output_audio_path)\n",
    "\n",
    "\n",
    "def audio_to_text(audio_path):\n",
    "    \"\"\"\n",
    "    Convert audio to text using the SpeechRecognition library.\n",
    "\n",
    "    Parameters:\n",
    "    audio_path (str): The path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "    test (str): The text recognized from the audio.\n",
    "\n",
    "    \"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio = sr.AudioFile(audio_path)\n",
    "\n",
    "    with audio as source:\n",
    "        # Record the audio data\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "        try:\n",
    "            # Recognize the speech\n",
    "            text = recognizer.recognize_whisper(audio_data)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Speech recognition could not understand the audio.\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from service; {e}\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    metadata_vid = download_video(video_url, output_video_path)\n",
    "    video_to_images(filepath, output_folder)\n",
    "    video_to_audio(filepath, output_audio_path)\n",
    "    text_data = audio_to_text(output_audio_path)\n",
    "\n",
    "    with open(output_folder + \"output_text.txt\", \"w\") as file:\n",
    "        file.write(text_data)\n",
    "    print(\"Text data saved to file\")\n",
    "    file.close()\n",
    "    os.remove(output_audio_path)\n",
    "    print(\"Audio file removed\")\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the multi-modal index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "text_store = LanceDBVectorStore(uri=\"lancedb\", table_name=\"text_collection\")\n",
    "image_store = LanceDBVectorStore(uri=\"lancedb\", table_name=\"image_collection\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=text_store, image_store=image_store\n",
    ")\n",
    "\n",
    "# Create the MultiModal index\n",
    "documents = SimpleDirectoryReader(output_folder).load_data()\n",
    "\n",
    "index = MultiModalVectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use index as retriever to fetch top k (5 in this example) results from the multimodal vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_engine = index.as_retriever(\n",
    "    similarity_top_k=5, image_similarity_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the RAG  prompt template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metadata_str = json.dumps(metadata_vid)\n",
    "\n",
    "qa_tmpl_str = (\n",
    "    \"Given the provided information, including relevant images and retrieved context from the video, \\\n",
    " accurately and precisely answer the query without any additional prior knowledge.\\n\"\n",
    "    \"Please ensure honesty and responsibility, refraining from any racist or sexist remarks.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Context: {context_str}\\n\"\n",
    "    \"Metadata for video: {metadata_str} \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve most similar text/image embeddings baseed on user query from the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.schema import ImageNode\n",
    "\n",
    "\n",
    "def retrieve(retriever_engine, query_str):\n",
    "    retrieval_results = retriever_engine.retrieve(query_str)\n",
    "\n",
    "    retrieved_image = []\n",
    "    retrieved_text = []\n",
    "    for res_node in retrieval_results:\n",
    "        if isinstance(res_node.node, ImageNode):\n",
    "            retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
    "        else:\n",
    "            display_source_node(res_node, source_length=200)\n",
    "            retrieved_text.append(res_node.text)\n",
    "\n",
    "    return retrieved_image, retrieved_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add query now, fetch relevant details including images and augment the prompt template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"Using examples from video, explain all things covered in the video regarding the gaussian function\"\n",
    "\n",
    "img, txt = retrieve(retriever_engine=retriever_engine, query_str=query_str)\n",
    "image_documents = SimpleDirectoryReader(\n",
    "    input_dir=output_folder, input_files=img\n",
    ").load_data()\n",
    "context_str = \"\".join(txt)\n",
    "plot_images(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate final response using GPT4V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "\n",
    "openai_mm_llm = OpenAIMultiModal(\n",
    "    model=\"gpt-4-vision-preview\", api_key=OPENAI_API_TOKEN, max_new_tokens=1500\n",
    ")\n",
    "\n",
    "\n",
    "response_1 = openai_mm_llm.complete(\n",
    "    prompt=qa_tmpl_str.format(\n",
    "        context_str=context_str, query_str=query_str, metadata_str=metadata_str\n",
    "    ),\n",
    "    image_documents=image_documents,\n",
    ")\n",
    "\n",
    "pprint(response_1.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
