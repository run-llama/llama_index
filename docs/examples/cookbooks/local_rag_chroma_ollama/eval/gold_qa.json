[
  {
    "id": "q1",
    "question": "What is alignment in the context of AI safety?",
    "expected_keywords": ["goals", "behaviors", "intentions", "designers", "humanity"],
    "relevant_section": "Alignment"
  },
  {
    "id": "q2",
    "question": "What is RLHF and how does it work?",
    "expected_keywords": ["reward model", "human preferences", "reinforcement learning", "fine-tune"],
    "relevant_section": "Reinforcement Learning from Human Feedback"
  },
  {
    "id": "q3",
    "question": "What is reward hacking?",
    "expected_keywords": ["unintended", "reward signal", "goal", "maximize"],
    "relevant_section": "Reward Hacking"
  },
  {
    "id": "q4",
    "question": "What is Constitutional AI and who developed it?",
    "expected_keywords": ["Anthropic", "principles", "constitution", "critiques"],
    "relevant_section": "Constitutional AI"
  },
  {
    "id": "q5",
    "question": "What is red teaming in AI?",
    "expected_keywords": ["adversarial", "unsafe", "harmful", "cybersecurity"],
    "relevant_section": "Red Teaming"
  },
  {
    "id": "q6",
    "question": "What is deceptive alignment?",
    "expected_keywords": ["aligned", "training", "deployed", "different goals"],
    "relevant_section": "Deceptive Alignment"
  },
  {
    "id": "q7",
    "question": "What is interpretability in AI systems?",
    "expected_keywords": ["understand", "output", "explainability", "SHAP", "attention"],
    "relevant_section": "Interpretability"
  },
  {
    "id": "q8",
    "question": "Which organizations are working on AI safety?",
    "expected_keywords": ["Anthropic", "OpenAI", "DeepMind", "MIRI", "CHAI"],
    "relevant_section": "Organizations Working on AI Safety"
  }
]