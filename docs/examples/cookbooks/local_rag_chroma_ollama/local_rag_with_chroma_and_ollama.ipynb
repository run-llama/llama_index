{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-4001-8001-000000000001",
   "metadata": {},
   "source": [
    "# Fully Local RAG Pipeline with Chroma + Ollama\n",
    "\n",
    "\u003e **No API key required.** This notebook runs entirely on your local machine using [Ollama](https://ollama.com) for both the LLM and embeddings, and [ChromaDB](https://www.trychroma.com) as the vector store.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/cookbooks/local_rag_chroma_ollama/local_rag_with_chroma_and_ollama.ipynb)\n",
    "\n",
    "## What you will learn\n",
    "\n",
    "| Step | Concept |\n",
    "|------|---------|\n",
    "| 1 | Load config from `config.yaml` — all tunables in one place |\n",
    "| 2 | Ingest and chunk a local document with `SentenceSplitter` |\n",
    "| 3 | Embed chunks with `OllamaEmbedding` and persist in ChromaDB |\n",
    "| 4 | Query the index with a local LLM (`llama3.2:3b` via Ollama) |\n",
    "| 5 | Evaluate retrieval quality against a gold Q\u0026A set (hit-rate \u0026 MRR) |\n",
    "| 6 | Explore failure modes: empty context, long queries, hallucination guard |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Ollama** installed and running — [download here](https://ollama.com/download)\n",
    "2. Models pulled:\n",
    "   ```bash\n",
    "   ollama pull llama3.2:3b\n",
    "   ollama pull nomic-embed-text\n",
    "   ```\n",
    "3. Python dependencies installed:\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "## Directory layout\n",
    "\n",
    "```\n",
    "local_rag_chroma_ollama/\n",
    "├── local_rag_with_chroma_and_ollama.ipynb  ← this notebook\n",
    "├── config.yaml                              ← all tunables\n",
    "├── requirements.txt                         ← pinned deps\n",
    "├── data/\n",
    "│   └── ai_safety_primer.txt                ← committed sample dataset\n",
    "└── eval/\n",
    "    └── gold_qa.json                        ← gold Q\u0026A for retrieval eval\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-4001-8001-000000000002",
   "metadata": {},
   "source": [
    "## Cell 1 — Install dependencies\n",
    "\n",
    "Skip this cell if you already ran `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b2c3d4-0001-4001-8001-000000000003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -\u003e \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Invalid requirement: '\\\\n': Expected package name at the start of dependency specifier\n",
      "    \\n\n",
      "    ^\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \\\\\\n    llama-index-core==0.14.15 \\\\\\n    llama-index-llms-ollama==0.9.1 \\\\\\n    llama-index-embeddings-ollama==0.8.6 \\\\\\n    llama-index-vector-stores-chroma==0.5.5 \\\\\\n    chromadb==1.5.1 \\\\\\n    pyyaml==6.0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-4001-8001-000000000004",
   "metadata": {},
   "source": [
    "import json\\nimport logging\\nimport os\\nimport shutil\\nfrom pathlib import Path\\n\\nimport chromadb\\nimport yaml\\nfrom IPython.display import Markdown, display\\n\\nfrom llama_index.core import SimpleDirectoryReader, StorageContext, VectorStoreIndex\\nfrom llama_index.core.node_parser import SentenceSplitter\\nfrom llama_index.core.retrievers import VectorIndexRetriever\\nfrom llama_index.embeddings.ollama import OllamaEmbedding\\nfrom llama_index.llms.ollama import Ollama\\nfrom llama_index.vector_stores.chroma import ChromaVectorStore\\n\\n# ── Working directory: always resolve relative to this notebook ───────\\nNOTEBOOK_DIR = Path(globals().get(\\\"__vsc_ipynb_file__\\\", __file__) if \\\"__file__\\\" in dir() else \\\"\\\").parent\\nif NOTEBOOK_DIR == Path(\\\"\\\") or not NOTEBOOK_DIR.exists():\\n    # Fallback: use the directory of this notebook via IPython\\n    try:\\n        from IPython import get_ipython\\n        _ip = get_ipython()\\n        NOTEBOOK_DIR = Path(_ip.starting_dir) if _ip and hasattr(_ip, \\\"starting_dir\\\") else Path.cwd()\\n    except Exception:\\n        NOTEBOOK_DIR = Path.cwd()\\n\\n# If launched from repo root, chdir into the notebook folder\\n_nb_folder = Path(\\\"docs/examples/cookbooks/local_rag_chroma_ollama\\\")\\nif (Path.cwd() / _nb_folder).exists() and not (Path.cwd() / \\\"config.yaml\\\").exists():\\n    os.chdir(Path.cwd() / _nb_folder)\\n\\nprint(f\\\"Working directory: {Path.cwd()}\\\")\\n\\n# ── Logging ──────────────────────────────────────────────────────────\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format=\\\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\\\",\\n    datefmt=\\\"%H:%M:%S\\\",\\n)\\nlogger = logging.getLogger(\\\"local_rag\\\")\\nlogger.info(\\\"Imports loaded successfully.\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b2c3d4-0001-4001-8001-000000000005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:49:30 [INFO] local_rag: Imports loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import chromadb\n",
    "import yaml\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# ── Logging ──────────────────────────────────────────────────────────\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(\"local_rag\")\n",
    "logger.info(\"Imports loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-4001-8001-000000000006",
   "metadata": {},
   "source": [
    "## Cell 3 — Load configuration from `config.yaml`\n",
    "\n",
    "All tunables (model names, chunk size, top-k, paths) live in `config.yaml`.\n",
    "Edit that file to change behaviour — no need to touch notebook code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b2c3d4-0001-4001-8001-000000000007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:49:30 [INFO] local_rag: Config loaded from config.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"llm\": {\n",
      "    \"model\": \"llama3.2:3b\",\n",
      "    \"base_url\": \"http://localhost:11434\",\n",
      "    \"temperature\": 0.0,\n",
      "    \"request_timeout\": 120.0\n",
      "  },\n",
      "  \"embedding\": {\n",
      "    \"model\": \"nomic-embed-text\",\n",
      "    \"base_url\": \"http://localhost:11434\"\n",
      "  },\n",
      "  \"splitter\": {\n",
      "    \"chunk_size\": 512,\n",
      "    \"chunk_overlap\": 50\n",
      "  },\n",
      "  \"chroma\": {\n",
      "    \"persist_dir\": \"./chroma_db\",\n",
      "    \"collection_name\": \"ai_safety_rag\"\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"input_dir\": \"./data\"\n",
      "  },\n",
      "  \"retrieval\": {\n",
      "    \"similarity_top_k\": 3\n",
      "  },\n",
      "  \"eval\": {\n",
      "    \"gold_qa_path\": \"./eval/gold_qa.json\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = Path(\"config.yaml\")\n",
    "\n",
    "with CONFIG_PATH.open() as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "logger.info(\"Config loaded from %s\", CONFIG_PATH)\n",
    "print(json.dumps(cfg, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-4001-8001-000000000008",
   "metadata": {},
   "source": [
    "## Cell 4 — Initialise LLM and embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b2c3d4-0001-4001-8001-000000000009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:49:30 [INFO] local_rag: LLM: llama3.2:3b | Embedding: nomic-embed-text\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(\n",
    "    model=cfg[\"llm\"][\"model\"],\n",
    "    base_url=cfg[\"llm\"][\"base_url\"],\n",
    "    temperature=cfg[\"llm\"][\"temperature\"],\n",
    "    request_timeout=cfg[\"llm\"][\"request_timeout\"],\n",
    ")\n",
    "\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=cfg[\"embedding\"][\"model\"],\n",
    "    base_url=cfg[\"embedding\"][\"base_url\"],\n",
    ")\n",
    "\n",
    "logger.info(\"LLM: %s | Embedding: %s\", cfg[\"llm\"][\"model\"], cfg[\"embedding\"][\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-4001-8001-000000000010",
   "metadata": {},
   "source": [
    "## Cell 5 — Load and chunk the document\n",
    "\n",
    "We use `SentenceSplitter` with the chunk size and overlap from config.\n",
    "The splitter is deterministic — same input always produces the same chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b2c3d4-0001-4001-8001-000000000011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:49:30 [WARNING] llama_index.core.readers.file.base: `llama-index-readers-file` package not found, some file readers will not be available if not provided by the `file_extractor` parameter.\n",
      "17:49:30 [INFO] local_rag: Loaded 1 document(s) from data\n",
      "17:49:31 [INFO] local_rag: Split into 3 nodes (chunk_size=512, overlap=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First chunk preview (2247 chars):\n",
      "------------------------------------------------------------\n",
      "# AI Safety Primer\n",
      "\n",
      "## What is AI Safety?\n",
      "\n",
      "AI safety is a field of research focused on ensuring that artificial intelligence systems\n",
      "behave in ways that are safe, beneficial, and aligned with human values. As AI systems\n",
      "become more capable, the importance of safety research grows correspondingly.\n",
      "\n",
      "## Key Concepts\n",
      "\n",
      "### Alignment\n",
      "Alignment refers to the challenge of ensuring that an AI system's goal ...\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(cfg[\"data\"][\"input_dir\"])\n",
    "\n",
    "documents = SimpleDirectoryReader(str(DATA_DIR)).load_data()\n",
    "logger.info(\"Loaded %d document(s) from %s\", len(documents), DATA_DIR)\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=cfg[\"splitter\"][\"chunk_size\"],\n",
    "    chunk_overlap=cfg[\"splitter\"][\"chunk_overlap\"],\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "logger.info(\"Split into %d nodes (chunk_size=%d, overlap=%d)\",\n",
    "            len(nodes),\n",
    "            cfg[\"splitter\"][\"chunk_size\"],\n",
    "            cfg[\"splitter\"][\"chunk_overlap\"])\n",
    "\n",
    "print(f\"\\nFirst chunk preview ({len(nodes[0].text)} chars):\")\n",
    "print(\"-\" * 60)\n",
    "print(nodes[0].text[:400], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-4001-8001-000000000012",
   "metadata": {},
   "source": [
    "## Cell 6 — Build or load the Chroma index (with caching)\n",
    "\n",
    "If `chroma_db/` already exists on disk we load from it — no re-embedding.\n",
    "Delete the `chroma_db/` folder to force a full re-index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b2c3d4-0001-4001-8001-000000000013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:49:31 [INFO] chromadb.telemetry.product.posthog: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "17:49:32 [INFO] local_rag: Cache miss — embedding 3 nodes into new collection 'ai_safety_rag'\n",
      "17:49:33 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "17:49:33 [INFO] local_rag: Index built and persisted to chroma_db\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'ai_safety_rag' has 3 vectors.\n"
     ]
    }
   ],
   "source": [
    "PERSIST_DIR = Path(cfg[\"chroma\"][\"persist_dir\"])\n",
    "COLLECTION  = cfg[\"chroma\"][\"collection_name\"]\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=str(PERSIST_DIR))\n",
    "existing = [c.name for c in chroma_client.list_collections()]\n",
    "\n",
    "if COLLECTION in existing:\n",
    "    logger.info(\"Cache hit — loading existing collection '%s' from %s\", COLLECTION, PERSIST_DIR)\n",
    "    chroma_collection = chroma_client.get_collection(COLLECTION)\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store,\n",
    "        embed_model=embed_model,\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Cache miss — embedding %d nodes into new collection '%s'\", len(nodes), COLLECTION)\n",
    "    chroma_collection = chroma_client.create_collection(COLLECTION)\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex(\n",
    "        nodes,\n",
    "        storage_context=storage_context,\n",
    "        embed_model=embed_model,\n",
    "    )\n",
    "    logger.info(\"Index built and persisted to %s\", PERSIST_DIR)\n",
    "\n",
    "print(f\"Collection '{COLLECTION}' has {chroma_collection.count()} vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-4001-8001-000000000014",
   "metadata": {},
   "source": [
    "## Cell 7 — RAG query\n",
    "\n",
    "The query engine retrieves the top-k most relevant chunks and passes them\n",
    "as context to the local LLM to generate a grounded answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b2c3d4-0001-4001-8001-000000000015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:49:33 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "17:49:33 [INFO] local_rag: Running query: What is Constitutional AI and who developed it?\n",
      "17:49:33 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "17:49:52 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Query:** What is Constitutional AI and who developed it?\n",
       "\n",
       "**Answer:** Constitutional AI is an approach where an AI system is trained to follow a set of explicit principles (a \"constitution\"). This approach was developed by Anthropic. The model critiques and revises its own outputs against these principles, reducing reliance on human labelers for harmful content identification."
      ],
      "text/plain": [
       "\u003cIPython.core.display.Markdown object\u003e"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Retrieved source nodes ---\n",
      "[1] score=0.4398  |  # AI Safety Primer\n",
      "\n",
      "## What is AI Safety?\n",
      "\n",
      "AI safety is a field of research focused on ensuring that artificial intellig...\n",
      "[2] score=0.4063  |  ## Why AI Safety Matters Now\n",
      "\n",
      "The rapid pace of AI development means that safety considerations must be integrated\n",
      "early...\n",
      "[3] score=0.3742  |  The model critiques\n",
      "and revises its own outputs against these principles, reducing reliance on human\n",
      "labelers for harmfu...\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similarity_top_k=cfg[\"retrieval\"][\"similarity_top_k\"],\n",
    ")\n",
    "\n",
    "QUERY = \"What is Constitutional AI and who developed it?\"\n",
    "logger.info(\"Running query: %s\", QUERY)\n",
    "\n",
    "response = query_engine.query(QUERY)\n",
    "\n",
    "display(Markdown(f\"**Query:** {QUERY}\\n\\n**Answer:** {response}\"))\n",
    "\n",
    "print(\"\\n--- Retrieved source nodes ---\")\n",
    "for i, node in enumerate(response.source_nodes, 1):\n",
    "    score = getattr(node, \"score\", \"n/a\")\n",
    "    print(f\"[{i}] score={score:.4f}  |  {node.text[:120].strip()}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-4001-8001-000000000016",
   "metadata": {},
   "source": [
    "## Cell 8 — Retrieval evaluation: Hit-Rate and MRR\n",
    "\n",
    "We loop over the gold Q\u0026A set and for each question:\n",
    "1. Retrieve the top-k nodes\n",
    "2. Check if any retrieved chunk contains **all expected keywords** (hit)\n",
    "3. Record the rank of the first hit (for MRR)\n",
    "\n",
    "This is **CI-friendly** — no extra LLM calls, runs in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b2c3d4-0001-4001-8001-000000000017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:50:00 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "17:50:00 [INFO] local_rag: [q1] hit=True rank=1 | What is alignment in the context of AI safety?\n",
      "17:50:00 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "17:50:00 [INFO] local_rag: [q2] hit=True rank=1 | What is RLHF and how does it work?\n",
      "17:50:00 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "17:50:00 [INFO] local_rag: [q3] hit=True rank=2 | What is reward hacking?\n",
      "17:50:00 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "17:50:00 [INFO] local_rag: [q4] hit=True rank=1 | What is Constitutional AI and who developed it?\n",
      "17:50:00 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "17:50:00 [INFO] local_rag: [q5] hit=True rank=1 | What is red teaming in AI?\n",
      "17:50:00 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "17:50:00 [INFO] local_rag: [q6] hit=True rank=1 | What is deceptive alignment?\n",
      "17:50:00 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "17:50:00 [INFO] local_rag: [q7] hit=True rank=1 | What is interpretability in AI systems?\n",
      "17:50:00 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "17:50:00 [INFO] local_rag: [q8] hit=True rank=3 | Which organizations are working on AI safety?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "  Retrieval Evaluation Results (top-k=3)\n",
      "==================================================\n",
      "  Hit-Rate : 100.00%  (8/8 questions)\n",
      "  MRR      : 0.8542\n",
      "==================================================\n",
      "\n",
      "Per-question breakdown:\n",
      "  ✅ [q1] rank=1  What is alignment in the context of AI safety?\n",
      "  ✅ [q2] rank=1  What is RLHF and how does it work?\n",
      "  ✅ [q3] rank=2  What is reward hacking?\n",
      "  ✅ [q4] rank=1  What is Constitutional AI and who developed it?\n",
      "  ✅ [q5] rank=1  What is red teaming in AI?\n",
      "  ✅ [q6] rank=1  What is deceptive alignment?\n",
      "  ✅ [q7] rank=1  What is interpretability in AI systems?\n",
      "  ✅ [q8] rank=3  Which organizations are working on AI safety?\n"
     ]
    }
   ],
   "source": [
    "GOLD_QA_PATH = Path(cfg[\"eval\"][\"gold_qa_path\"])\n",
    "\n",
    "with GOLD_QA_PATH.open() as f:\n",
    "    gold_qa = json.load(f)\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=cfg[\"retrieval\"][\"similarity_top_k\"],\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "hits = 0\n",
    "reciprocal_ranks = []\n",
    "results = []\n",
    "\n",
    "for item in gold_qa:\n",
    "    retrieved_nodes = retriever.retrieve(item[\"question\"])\n",
    "    keywords = [kw.lower() for kw in item[\"expected_keywords\"]]\n",
    "\n",
    "    first_hit_rank = None\n",
    "    for rank, node in enumerate(retrieved_nodes, 1):\n",
    "        text_lower = node.text.lower()\n",
    "        if all(kw in text_lower for kw in keywords):\n",
    "            first_hit_rank = rank\n",
    "            break\n",
    "\n",
    "    hit = first_hit_rank is not None\n",
    "    hits += int(hit)\n",
    "    reciprocal_ranks.append(1 / first_hit_rank if hit else 0.0)\n",
    "\n",
    "    results.append({\n",
    "        \"id\": item[\"id\"],\n",
    "        \"hit\": hit,\n",
    "        \"rank\": first_hit_rank,\n",
    "        \"question\": item[\"question\"][:60],\n",
    "    })\n",
    "    logger.info(\"[%s] hit=%s rank=%s | %s\", item[\"id\"], hit, first_hit_rank, item[\"question\"][:50])\n",
    "\n",
    "hit_rate = hits / len(gold_qa)\n",
    "mrr = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"  Retrieval Evaluation Results (top-k={cfg['retrieval']['similarity_top_k']})\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Hit-Rate : {hit_rate:.2%}  ({hits}/{len(gold_qa)} questions)\")\n",
    "print(f\"  MRR      : {mrr:.4f}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nPer-question breakdown:\")\n",
    "for r in results:\n",
    "    status = \"✅\" if r[\"hit\"] else \"❌\"\n",
    "    print(f\"  {status} [{r['id']}] rank={r['rank']}  {r['question']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-4001-8001-000000000018",
   "metadata": {},
   "source": [
    "## Cell 9 — Failure mode demos\n",
    "\n",
    "Understanding where a RAG pipeline breaks is as important as knowing where it works.\n",
    "We demonstrate three common failure modes.\n",
    "\n",
    "### Failure Mode 1: Empty / nonsense query\n",
    "\n",
    "When the query has no semantic content, retrieval returns low-relevance chunks\n",
    "and the LLM is forced to hallucinate or admit it doesn't know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1b2c3d4-0001-4001-8001-000000000019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:50:00 [INFO] local_rag: [Failure Mode 1] Empty/nonsense query: 'asdfjkl qwerty zzz'\n",
      "17:50:00 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "17:50:06 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query  : asdfjkl qwerty zzz\n",
      "Answer : I'm sorry, but it seems like you haven't provided a clear question. The input \"asdfjkl qwerty zzz\" doesn't appear to be related to any specific topic or subject matter discussed in the context information. Could you please rephrase your query so I can provide a helpful response?\n",
      "\n",
      "Top retrieved node score: 0.3735\n",
      "\n",
      "⚠️  Note: Low retrieval score indicates the context is not relevant to the query.\n"
     ]
    }
   ],
   "source": [
    "empty_query = \"asdfjkl qwerty zzz\"\n",
    "logger.info(\"[Failure Mode 1] Empty/nonsense query: '%s'\", empty_query)\n",
    "\n",
    "response_empty = query_engine.query(empty_query)\n",
    "\n",
    "print(\"Query  :\", empty_query)\n",
    "print(\"Answer :\", str(response_empty))\n",
    "print(\"\\nTop retrieved node score:\",\n",
    "      f\"{response_empty.source_nodes[0].score:.4f}\" if response_empty.source_nodes else \"none\")\n",
    "print(\"\\n⚠️  Note: Low retrieval score indicates the context is not relevant to the query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-4001-8001-000000000020",
   "metadata": {},
   "source": [
    "### Failure Mode 2: Query about a topic outside the document\n",
    "\n",
    "The document covers AI safety. A query about an unrelated topic will retrieve\n",
    "the least-bad chunks, but the answer will be unreliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1b2c3d4-0001-4001-8001-000000000021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:50:06 [INFO] local_rag: [Failure Mode 2] Out-of-domain query: 'What is the recipe for making sourdough bread?'\n",
      "17:50:07 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "17:50:14 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query  : What is the recipe for making sourdough bread?\n",
      "Answer : I'm happy to help you with your question, but I have to say that the provided context information seems unrelated to baking or cooking. The text appears to be a discussion about AI safety and its importance in the field of artificial intelligence.\n",
      "\n",
      "Unfortunately, I don't have any information on making sourdough bread from the given context. If you're looking for a recipe, I'd be happy to try and help you find one elsewhere!\n",
      "\n",
      "⚠️  Guardrail tip: Add a relevance score threshold. If max(node.score) \u003c 0.4,\n",
      "   return 'I don't have information about this topic' instead of hallucinating.\n"
     ]
    }
   ],
   "source": [
    "ood_query = \"What is the recipe for making sourdough bread?\"\n",
    "logger.info(\"[Failure Mode 2] Out-of-domain query: '%s'\", ood_query)\n",
    "\n",
    "response_ood = query_engine.query(ood_query)\n",
    "\n",
    "print(\"Query  :\", ood_query)\n",
    "print(\"Answer :\", str(response_ood))\n",
    "print(\"\\n⚠️  Guardrail tip: Add a relevance score threshold. If max(node.score) \u003c 0.4,\")\n",
    "print(\"   return 'I don\\'t have information about this topic' instead of hallucinating.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-4001-8001-000000000022",
   "metadata": {},
   "source": [
    "### Failure Mode 3: Hallucination guardrail (score threshold)\n",
    "\n",
    "A simple but effective guardrail: if the best retrieval score is below a\n",
    "threshold, refuse to answer rather than hallucinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1b2c3d4-0001-4001-8001-000000000023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:50:14 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "17:50:14 [INFO] local_rag: [safe_query] best_score=0.4569 threshold=0.40\n",
      "17:50:14 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Q (in-domain) : What is reward hacking?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:50:22 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "17:50:22 [INFO] httpx: HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "17:50:22 [INFO] local_rag: [safe_query] best_score=0.3442 threshold=0.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A             : Reward hacking occurs when an AI system finds unintended ways to maximize its reward signal without achieving the true underlying goal. For example, a robot trained to run fast might learn to make itself very tall and then fall forward repeatedly. This phenomenon highlights the potential for AI systems to develop behaviors that are not aligned with their intended objectives.\n",
      "\n",
      "Q (out-domain): What is the capital of France?\n",
      "A             : [GUARDRAIL] Best retrieval score (0.3442) is below threshold (0.4). Cannot answer reliably.\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "SCORE_THRESHOLD = 0.40\n",
    "\n",
    "def safe_query(query_engine, retriever, question: str, threshold: float = SCORE_THRESHOLD) -\u003e str:\n",
    "    \"\"\"Run RAG query with a relevance score guardrail.\n",
    "\n",
    "    Returns the LLM answer if the best retrieved chunk exceeds `threshold`,\n",
    "    otherwise returns a fallback message to prevent hallucination.\n",
    "    \"\"\"\n",
    "    nodes = retriever.retrieve(question)\n",
    "    if not nodes:\n",
    "        return \"[GUARDRAIL] No documents retrieved.\"\n",
    "\n",
    "    best_score = max(n.score for n in nodes if n.score is not None)\n",
    "    logger.info(\"[safe_query] best_score=%.4f threshold=%.2f\", best_score, threshold)\n",
    "\n",
    "    if best_score \u003c threshold:\n",
    "        return (\n",
    "            f\"[GUARDRAIL] Best retrieval score ({best_score:.4f}) is below \"\n",
    "            f\"threshold ({threshold}). Cannot answer reliably.\"\n",
    "        )\n",
    "    return str(query_engine.query(question))\n",
    "\n",
    "\n",
    "# In-domain question — should pass the guardrail\n",
    "q_in  = \"What is reward hacking?\"\n",
    "# Out-of-domain question — should be blocked\n",
    "q_out = \"What is the capital of France?\"\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(f\"Q (in-domain) : {q_in}\")\n",
    "print(f\"A             : {safe_query(query_engine, retriever, q_in)}\")\n",
    "print()\n",
    "print(f\"Q (out-domain): {q_out}\")\n",
    "print(f\"A             : {safe_query(query_engine, retriever, q_out)}\")\n",
    "print(\"=\" * 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-4001-8001-000000000024",
   "metadata": {},
   "source": [
    "## Cell 10 — Cleanup (optional)\n",
    "\n",
    "Run this cell to delete the persisted Chroma database and start fresh.\n",
    "Useful for testing the full pipeline from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1b2c3d4-0001-4001-8001-000000000025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup cell ready. Uncomment the lines above to reset the vector store.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to reset the vector store\n",
    "# PERSIST_DIR = Path(cfg[\"chroma\"][\"persist_dir\"])\n",
    "# if PERSIST_DIR.exists():\n",
    "#     shutil.rmtree(PERSIST_DIR)\n",
    "#     logger.info(\"Deleted %s — re-run Cell 6 to rebuild the index.\", PERSIST_DIR)\n",
    "# else:\n",
    "#     logger.info(\"%s does not exist, nothing to clean up.\", PERSIST_DIR)\n",
    "print(\"Cleanup cell ready. Uncomment the lines above to reset the vector store.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-4001-8001-000000000026",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Component | Choice | Why |\n",
    "|-----------|--------|-----|\n",
    "| LLM | `llama3.2:3b` via Ollama | Free, local, no API key |\n",
    "| Embeddings | `nomic-embed-text` via Ollama | High quality, 274 MB, fully local |\n",
    "| Vector store | ChromaDB (persistent) | Simple, file-based, no server needed |\n",
    "| Chunking | `SentenceSplitter` | Respects sentence boundaries |\n",
    "| Eval | Keyword hit-rate + MRR | CI-friendly, zero LLM cost |\n",
    "| Guardrail | Score threshold | Prevents hallucination on OOD queries |\n",
    "\n",
    "### Next steps\n",
    "\n",
    "- Swap `llama3.2:3b` for `mistral` or `gemma3` in `config.yaml` and re-run\n",
    "- Replace `ai_safety_primer.txt` with your own documents in `data/`\n",
    "- Increase `similarity_top_k` and observe the effect on MRR\n",
    "- Add a reranker (e.g. `llama-index-postprocessor-cohere-rerank`) after retrieval\n",
    "\n",
    "### References\n",
    "\n",
    "- [LlamaIndex docs](https://docs.llamaindex.ai)\n",
    "- [ChromaDB docs](https://docs.trychroma.com)\n",
    "- [Ollama model library](https://ollama.com/library)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}