{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/retrievers/pathway_retriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pathway Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [Pathway](https://pathway.com/) is an open data processing framework. It allows you to easily develop data transformation pipelines and Machine Learning applications that work with live data sources and changing data.\n",
    "\n",
    "This notebook shows how to use the Pathway to deploy a live data indexing pipeline which can be queried from reader. You can add documents to Pathway from existing connectors or create your own connector with Python ensuring your LLM stays up to date with latest information. \n",
    "\n",
    "For more details about Pathwy vector store, visit [vector store pipeline](https://pathway.com/developers/showcases/vectorstore_pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import PathwayRetriever, PathwayVectorServer\n",
    "import pathway as pw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import pathway as pw\n",
    "\n",
    "# omit if embedder of choice is not OpenAI\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data sources for Pathway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pathway can listen to many sources simultaneously, such as local files, S3 folders, cloud storages and any data stream for data changes.\n",
    "\n",
    "See [pathway-io](https://pathway.com/developers/api-docs/pathway-io) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources = []\n",
    "data_sources.append(\n",
    "    pw.io.fs.read(\n",
    "        \"../data/paul_graham\",\n",
    "        format=\"binary\",\n",
    "        mode=\"streaming\",\n",
    "        with_metadata=True,\n",
    "    )  # This creates a `pathway` connector that tracks\n",
    "    # all the files in the sample_documents directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Transformation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create document ingestion pipeline. Pipeline should be list of `TransformComponent`, and end with embedder as last step for indexing.\n",
    "In this example, let's first split text with number of tokens then, embed with OpenAI embedder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.node_parser import TokenTextSplitter\n",
    "\n",
    "embed_model = OpenAIEmbedding(embed_batch_size=10)\n",
    "\n",
    "transformations_example = [\n",
    "    TokenTextSplitter(\n",
    "        chunk_size=150,\n",
    "        chunk_overlap=10,\n",
    "        separator=\" \",\n",
    "    ),\n",
    "    embed_model,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = PathwayVectorServer(\n",
    "    *data_sources,\n",
    "    transformations=transformations_example,\n",
    ")\n",
    "\n",
    "# Define the Host and port that Pathway will be on\n",
    "PATHWAY_HOST = \"127.0.0.1\"\n",
    "PATHWAY_PORT = 8754\n",
    "\n",
    "# `threaded` runs pathway in detached mode, we have to set it to False when running from terminal or container\n",
    "# for more information on `with_cache` check out https://pathway.com/developers/api-docs/persistence-api\n",
    "pr.run_server(\n",
    "    host=PATHWAY_HOST, port=PATHWAY_PORT, with_cache=False, threaded=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Retriever for llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = PathwayRetriever(host=PATHWAY_HOST, port=PATHWAY_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.retrieve(str_or_query_bundle=\"something\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
