{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "368686b4-f487-4dd4-aeff-37823976529d",
   "metadata": {},
   "source": [
    "# LlamaCPP \n",
    "\n",
    "In this short notebook, we show how to use the [LlamaCPP python](https://github.com/abetlen/llama-cpp-python) library with LlamaIndex.\n",
    "\n",
    "We use the [`llama-2-chat-13b-ggml`](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML) model by default, along with the proper prompt formatting.\n",
    "\n",
    "## Installation\n",
    "\n",
    "To get the best performance out of `LlamaCPP`, it is recomended to install the package so that it is compilied with GPU support. A full guide for installing this way is [here](https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal).\n",
    "\n",
    "Full MACOS instructions are also [here](https://llama-cpp-python.readthedocs.io/en/latest/install/macos/).\n",
    "\n",
    "In general:\n",
    "- Use `CuBLAS` if you have CUDA and an NVidia GPU\n",
    "- Use `METAL` if you are running on an M1/M2 MacBook\n",
    "- Use `CLBLAST` if you are running on an AMD/Intel GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7927630-0044-41fb-a8a6-8dc3d2adb608",
   "metadata": {},
   "source": [
    "## Setup LLM\n",
    "\n",
    "The LlamaCPP llm is highly configurable. Depending on the model being used, you'll want to pass in `messages_to_prompt` and `completion_to_prompt` functions to help format the model inputs.\n",
    "\n",
    "Since the default model is llama2-chat, we use the util functions found in [`llama_index.llms.llama_utils`](https://github.com/jerryjliu/llama_index/blob/main/llama_index/llms/llama_utils.py).\n",
    "\n",
    "For any kwargs that need to be passed in during initialization, set them in `model_kwargs`. A full list of available model kwargs is available in the [LlamaCPP docs](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.llama.Llama.__init__).\n",
    "\n",
    "For any kwargs that need to be passed in during inference, you can set them in `generate_kwargs`. See the full list of [generate kwargs here](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.llama.Llama.__call__).\n",
    "\n",
    "In general, the defaults are a great starting point. The example below shows configuration with all defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fa0ec4f-03ff-4e28-957f-b4b99a0faa20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/loganmarkewich/Library/Caches/llama_index/models/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 3900\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 7632.72 MB (+ 3046.88 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 3046.88 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/loganmarkewich/llama_index/llama-index/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x172e401d0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x172e422c0\n",
      "ggml_metal_init: loaded kernel_mul                            0x172e42ac0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x172e433f0\n",
      "ggml_metal_init: loaded kernel_scale                          0x172e43c50\n",
      "ggml_metal_init: loaded kernel_silu                           0x172e43e40\n",
      "ggml_metal_init: loaded kernel_relu                           0x172e419d0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x172e44db0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x172e446c0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x172e45780\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x172e45ca0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x172e47af0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x172e47290\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x172e48490\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x172e48d60\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x172e49600\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x172e49ed0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x172e4a7d0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x172e4b080\n",
      "ggml_metal_init: loaded kernel_norm                           0x172e4ba90\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x172e4c520\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x172e4cdb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x172e4d730\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x172e4e0a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x172e4eb80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x172e4f5b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x172e4ff00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x172e507f0\n",
      "ggml_metal_init: loaded kernel_rope                           0x172e51900\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x172e51430\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x172e527c0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x172e53270\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x172e4b800\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =    87.89 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, ( 6984.52 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.00 MB, ( 6996.52 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  3048.88 MB, (10045.39 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   445.00 MB, (10490.39 / 21845.34)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, (10682.39 / 21845.34)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    model_url=\"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin\",\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445453b1",
   "metadata": {},
   "source": [
    "We can tell that the model is using `metal` due to the logging!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2e6a78-7e5d-4915-bcbf-6087edb30276",
   "metadata": {},
   "source": [
    "## Start using our `LlamaCPP` LLM abstraction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cfaf34c-0348-415e-98bb-83f782d64fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! Here's a fun little poem about cats and dogs:\n",
      "\n",
      "Cats and dogs, so different yet the same,\n",
      "Both furry friends, with their own special game.\n",
      "\n",
      "Cats purr and curl up tight,\n",
      "Dogs wag their tails with delight.\n",
      "\n",
      "Cats chase mice with stealthy grace,\n",
      "Dogs bark and chase with joyful pace.\n",
      "\n",
      "But when the day is done,\n",
      "Both cats and dogs find comfort in a warm embrace.\n",
      "\n",
      "So here's to our feline and canine friends,\n",
      "Both equally loved, until the very end.\n",
      "\n",
      "I hope you enjoyed that little poem! Do you have any other questions or requests?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  8161.74 ms\n",
      "llama_print_timings:      sample time =   113.45 ms /   162 runs   (    0.70 ms per token,  1427.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8161.68 ms /    61 tokens (  133.80 ms per token,     7.47 tokens per second)\n",
      "llama_print_timings:        eval time =  6929.98 ms /   161 runs   (   43.04 ms per token,    23.23 tokens per second)\n",
      "llama_print_timings:       total time = 15406.04 ms\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"Hello! Can you tell me a poem about cats and dogs?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b059409-cd9d-4651-979c-03b3943e94af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure thing! Here's a poem about fast cars:\n",
      "\n",
      "Fast cars, oh how they thrill\n",
      "With their sleek designs and powerful bills\n",
      "They race down the road, a blur of speed\n",
      "Leaving all else in their dusty need\n",
      "\n",
      "Their engines purr, their tires squeal\n",
      "As they zip through the streets, it's a real deal\n",
      "The wind rushes by, a roar of sound\n",
      "As they leave all others in the ground\n",
      "\n",
      "With their shimmering paint and sleek lines\n",
      "They're a sight to behold, oh so fine\n",
      "They race and glide with graceful ease\n",
      "Fast cars, oh how they please\n",
      "\n",
      "So here's to the fast cars, a poem of praise\n",
      "For the thrill and joy they bring to our days\n",
      "May their engines roar and their wheels spin\n",
      "Forever and always, let them win."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  8161.74 ms\n",
      "llama_print_timings:      sample time =   160.54 ms /   201 runs   (    0.80 ms per token,  1252.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1127.02 ms /    14 tokens (   80.50 ms per token,    12.42 tokens per second)\n",
      "llama_print_timings:        eval time =  6295.21 ms /   200 runs   (   31.48 ms per token,    31.77 tokens per second)\n",
      "llama_print_timings:       total time =  7947.75 ms\n"
     ]
    }
   ],
   "source": [
    "response_iter = llm.stream_complete(\"Can you write me a poem about fast cars?\")\n",
    "for response in response_iter:\n",
    "    print(response.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedcd31d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
