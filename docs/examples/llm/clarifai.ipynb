{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/llm/clarifai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clarifai LLM\n",
    "\n",
    "## Example notebook to call different LLM models using clarifai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install clarifai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install clarifai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set clarifai PAT as environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CLARIFAI_PAT\"] = \"<YOUR CLARIFAI PAT>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import clarifai package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.clarifai import Clarifai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore various models according to your prefrence from\n",
    "[Our Models page](https://clarifai.com/explore/models?filterData=%5B%7B%22field%22%3A%22use_cases%22%2C%22value%22%3A%5B%22llm%22%5D%7D%5D&page=2&perPage=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example parameters\n",
    "params = dict(\n",
    "    user_id=\"clarifai\",\n",
    "    app_id=\"ml\",\n",
    "    model_name=\"llama2-7b-alternative-4k\",\n",
    "    model_url=(\n",
    "        \"https://clarifai.com/clarifai/ml/models/llama2-7b-alternative-4k\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method:1 using model_url parameter\n",
    "llm_model = Clarifai(model_url=params[\"model_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method:2 using model_name, app_id & user_id parameters\n",
    "llm_model = Clarifai(\n",
    "    model_name=params[\"model_name\"],\n",
    "    app_id=params[\"app_id\"],\n",
    "    user_id=params[\"user_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `complete` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_reponse = llm_model.complete(\n",
    "    prompt=\"write a 10 line rhyming poem about science\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "Science is fun, it's true!\n",
      "From atoms to galaxies, it's all new!\n",
      "With experiments and tests, we learn so fast,\n",
      "And discoveries come from the past.\n",
      "It helps us understand the world around,\n",
      "And makes our lives more profound.\n",
      "So let's embrace this wondrous art,\n",
      "And see where it takes us in the start!\n"
     ]
    }
   ],
   "source": [
    "print(llm_reponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `chat` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"write about climate change in 50 lines\")\n",
    "]\n",
    "Response = llm_model.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user:  or less.\n",
      "Climate change is a serious threat to our planet and its inhabitants. Rising temperatures are causing extreme weather events, such as hurricanes, droughts, and wildfires. Sea levels are rising, threatening coastal communities and ecosystems. The melting of polar ice caps is disrupting global navigation and commerce. Climate change is also exacerbating air pollution, which can lead to respiratory problems and other health issues. It's essential that we take action now to reduce greenhouse gas emissions and transition to renewable energy sources to mitigate the worst effects of climate change.\n"
     ]
    }
   ],
   "source": [
    "print(Response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Inference parameters\n",
    "Alternatively you can call models with inference parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an inference parameter example for GPT model.\n",
    "inference_params = dict(temperature=str(0.3), max_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_reponse = llm_model.complete(\n",
    "    prompt=\"What is nuclear fission and fusion?\",\n",
    "    inference_params=params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [ChatMessage(role=\"user\", content=\"Explain about the big bang\")]\n",
    "Response = llm_model.chat(messages, inference_params=params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
