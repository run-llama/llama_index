{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Client of Baidu Intelligent Cloud's Qianfan LLM Platform\n",
    "\n",
    "Baidu Intelligent Cloud's Qianfan LLM Platform offers API services for all Baidu LLMs, such as ERNIE-3.5-8K and ERNIE-4.0-8K. It also provides a small number of open-source LLMs like Llama-2-70b-chat.\n",
    "\n",
    "Before using the chat client, you need to activate the LLM service on the Qianfan LLM Platform console's [online service](https://console.bce.baidu.com/qianfan/ais/console/onlineService) page. Then, Generate an Access Key and a Secret Key in the [Security Authentication](https://console.bce.baidu.com/iam/#/iam/accesslist) page of the console."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install the necessary package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "installation-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-qianfan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initialization",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initialization-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.qianfan import Qianfan\n",
    "import asyncio\n",
    "\n",
    "access_key = \"XXX\"\n",
    "secret_key = \"XXX\"\n",
    "model_name = \"ERNIE-Speed-8K\"\n",
    "endpoint_url = \"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/ernie_speed\"\n",
    "context_window = 8192\n",
    "llm = Qianfan(access_key, secret_key, model_name, endpoint_url, context_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sync-chat",
   "metadata": {},
   "source": [
    "## Synchronous Chat\n",
    "\n",
    "Generate a chat response synchronously using the `chat` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sync-chat-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"Tell me a joke.\"),\n",
    "]\n",
    "chat_response = llm.chat(messages)\n",
    "print(chat_response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sync-stream-chat",
   "metadata": {},
   "source": [
    "## Synchronous Stream Chat\n",
    "\n",
    "Generate a streaming chat response synchronously using the `stream_chat` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sync-stream-chat-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "    ChatMessage(role=\"user\", content=\"Tell me a story.\"),\n",
    "]\n",
    "content = \"\"\n",
    "for chat_response in llm.stream_chat(messages):\n",
    "    content += chat_response.delta\n",
    "    print(chat_response.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "async-chat",
   "metadata": {},
   "source": [
    "## Asynchronous Chat\n",
    "\n",
    "Generate a chat response asynchronously using the `achat` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "async-chat-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_chat():\n",
    "    messages = [\n",
    "        ChatMessage(role=\"user\", content=\"Tell me an async joke.\"),\n",
    "    ]\n",
    "    chat_response = await llm.achat(messages)\n",
    "    print(chat_response.message.content)\n",
    "\n",
    "\n",
    "asyncio.run(async_chat())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "async-stream-chat",
   "metadata": {},
   "source": [
    "## Asynchronous Stream Chat\n",
    "\n",
    "Generate a streaming chat response asynchronously using the `astream_chat` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "async-stream-chat-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_stream_chat():\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "        ChatMessage(role=\"user\", content=\"Tell me an async story.\"),\n",
    "    ]\n",
    "    content = \"\"\n",
    "    response = await llm.astream_chat(messages)\n",
    "    async for chat_response in response:\n",
    "        content += chat_response.delta\n",
    "        print(chat_response.delta, end=\"\")\n",
    "\n",
    "\n",
    "asyncio.run(async_stream_chat())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
