{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDIbNupZt066"
   },
   "source": [
    "# Portkey\n",
    "\n",
    "**Portkey** is a full-stack LLMOps platform that productionizes your Gen AI app reliably and securely.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/examples/llm/portkey.ipynb\" target=\"_blank\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\\\"Open In Colab\\\"/>\n",
    "</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67b-KIp4uRFO"
   },
   "source": [
    "### Key Features of Portkey's Integration with Llamaindex:\n",
    "\n",
    "<img src=\"https://portkey.ai/blog/content/images/2023/09/Colab-Version.png\" alt=\"header\" width=600 />\n",
    "\n",
    "\n",
    "1. **AI Gateway**:\n",
    "    - **Automated Fallbacks & Retries**: Ensure your application remains functional even if a primary service fails.\n",
    "    - **Load Balancing**: Efficiently distribute incoming requests among multiple models.\n",
    "    - **Semantic Caching**: Reduce costs and latency by intelligently caching results.\n",
    "    \n",
    "2. **Observability**:\n",
    "    - **Logging**: Keep track of all requests for monitoring and debugging.\n",
    "    - **Requests Tracing**: Understand the journey of each request for optimization.\n",
    "    - **Custom Tags**: Segment and categorize requests for better insights.\n",
    "\n",
    "To harness these features, let's start with the setup:\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/examples/llm/portkey.ipynb\" target=\"_blank\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\\\"Open In Colab\\\" width=150 />\n",
    "</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Z933R9wuZ4z"
   },
   "outputs": [],
   "source": [
    "# Installing the Rubeus AI gateway developed by the Portkey team\n",
    "!pip install rubeus\n",
    "!pip install llama_index\n",
    "\n",
    "# Importing necessary libraries and modules\n",
    "from llama_index.llms import Portkey, ChatMessage\n",
    "from rubeus import LLMBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Grlj72viug5E"
   },
   "source": [
    "You do not need to install **any** other SDKs or import them in your Llamaindex app.\n",
    "\n",
    "Here's a step-by-step guide to Portkey features and their integration with Llamaindex:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Q8L0IDau60f"
   },
   "source": [
    "#### **Step 1Ô∏è‚É£: Get your Portkey API key**\n",
    "\n",
    "Log into [Portkey here](https://app.portkey.ai/), then click on the profile icon on top left and \"Copy API Key\". Let's also set OpenAI & Anthropic API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vd7elJZHu_jF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PORTKEY_API_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICsp-JDdvLVz"
   },
   "source": [
    "#### **Step 2Ô∏è‚É£: Configure Portkey Features**\n",
    "\n",
    "To harness the full potential of Portkey's integration with Llamaindex, you can configure various features as illustrated above. Here's a guide to all Portkey features and the expected values:\n",
    "\n",
    "| Feature             | Config Key              | Value(Type)                                      | Required    |\n",
    "|---------------------|-------------------------|--------------------------------------------------|-------------|\n",
    "| API Key             | `api_key`               | `string`                                         | ‚úÖ Required (can be set externally) |\n",
    "| Mode                | `mode`                  | `fallback`, `loadbalance`, `single`              | ‚úÖ Required |\n",
    "| Cache Type          | `cache_status`          | `simple`, `semantic`                             | ‚ùî Optional |\n",
    "| Force Cache Refresh | `cache_force_refresh`   | `True`, `False`                                  | ‚ùî Optional |\n",
    "| Cache Age           | `cache_age`             | `integer` (in seconds)                           | ‚ùî Optional |\n",
    "| Trace ID            | `trace_id`              | `string`                                         | ‚ùî Optional |\n",
    "| Retries         | `retry`           | `integer` [0,5]                                  | ‚ùî Optional |\n",
    "| Metadata            | `metadata`              | `json object` [More info](https://docs.portkey.ai/key-features/custom-metadata)          | ‚ùî Optional |\n",
    "| Base URL | `base_url` | `url` | ‚ùî Optional |\n",
    "\n",
    "* `api_key` and `mode` are required values. \n",
    "* You can set your Portkey API key using the Portkey constructor or you can also set it as an environment variable.\n",
    "* There are **3** modes - Single, Fallback, Loadbalance.\n",
    "  * **Single** - This is the standard mode. Use it if you do not want Fallback OR Loadbalance features.\n",
    "  * **Fallback** - Set this mode if you want to enable the Fallback feature. [Check out the guide here](#implementing-fallbacks-and-retries-with-portkey).\n",
    "  * **Loadbalance** - Set this mode if you want to enable the Loadbalance feature. [Check out the guide here](#implementing-load-balancing-with-portkey).\n",
    "\n",
    "Here's an example of how to set up some of these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GfyFAjYvlgg"
   },
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"_environment\": \"production\",\n",
    "    \"_prompt\": \"test\",\n",
    "    \"_user\": \"user\",\n",
    "    \"_organisation\": \"acme\",\n",
    "}\n",
    "\n",
    "pk_llm = Portkey(\n",
    "    mode=\"single\",\n",
    "    cache_status=\"semantic\",\n",
    "    cache_force_refresh=True,\n",
    "    cache_age=1000,\n",
    "    trace_id=\"portkey_llamaindex\",\n",
    "    retry=5,\n",
    "    metadata=metadata,\n",
    ")\n",
    "\n",
    "# Since we have defined the Portkey API Key with os.environ, we do not need to set api_key again here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FuFyuR0vs5t"
   },
   "source": [
    "#### **Step 3Ô∏è‚É£: Constructing the LLM**\n",
    "\n",
    "With the Portkey integration, constructing an LLM is simplified. Use the `LLMBase` function for all providers, with the exact same keys you're accustomed to in your OpenAI or Anthropic constructors. The only new key is `weight`, essential for the load balancing feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4D5p-B7hv_Xm"
   },
   "outputs": [],
   "source": [
    "openai_llm = LLMBase(provider=\"openai\", model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7foKVwDdwBgG"
   },
   "source": [
    "The above code illustrates how to utilize the `LLMBase` function to set up an LLM with the OpenAI provider and the GPT-4 model. This same function can be used for other providers as well, making the integration process streamlined and consistent across various providers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTZZO6viwH2E"
   },
   "source": [
    "#### **Step 4Ô∏è‚É£: Activate the Portkey LLM**\n",
    "\n",
    "Once you've constructed the LLM using the `LLMBase` function, the next step is to activate it with Portkey. This step is essential to ensure that all the Portkey features are available for your LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwsNoWVWwNSX"
   },
   "outputs": [],
   "source": [
    "pk_llm.add_llms(openai_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkjNObFywd3S"
   },
   "source": [
    "And, that's it! In just 4 steps, you have infused your Llamaindex app with sophisticated production capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZztEhMKwNry"
   },
   "source": [
    "#### **üîß Testing the Integration**\n",
    "\n",
    "Let's ensure that everything is set up correctly. Below, we create a simple chat scenario and pass it through our Portkey-enhanced LLM to see the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYpwURe5wakB"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant\"),\n",
    "    ChatMessage(role=\"user\", content=\"What can you do?\"),\n",
    "]\n",
    "print(\"Testing Portkey Llamaindex integration:\")\n",
    "response = pk_llm.chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how your logs will appear on your [Portkey dashboard](https://app.portkey.ai/):\n",
    "\n",
    "<img src=\"https://portkey.ai/blog/content/images/2023/09/Log-1.png\" alt=\"Logs\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnQIen4Hwn52"
   },
   "source": [
    "#### **üîç Recap and References**\n",
    "\n",
    "Congratulations! üéâ You've successfully set up and tested the Portkey integration with Llamaindex. To recap the steps:\n",
    "\n",
    "1. `pip install rubeus`\n",
    "2. `from llama_index.llms import Portkey`\n",
    "3. Grab your Portkey API Key from [here](https://app.portkey.ai/)\n",
    "4. Use Portkey constructor to set trace id, cache, metadata, retry count, and mode: `portkey=Portkey()`\n",
    "5. Use the LLMBase constructor to create your LLM: `llm_a=PortkeyBase(provider=\"openai\",model=\"gpt-4\")`\n",
    "6. Add the LLM to Portkey with `portkey.add_llms(llm_a)`\n",
    "7. Call the portkey like you would any other LLM, with `portkey.chat(messages)`\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/examples/llm/portkey.ipynb\" target=\"_blank\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\\\"Open In Colab\\\" width=150 />\n",
    "</a> \n",
    "\n",
    "Here's the guide to all the functions and their params:\n",
    "- [Portkey LLM Constructor](#step-2-add-all-the-portkey-features-you-want-as-illustrated-below-by-calling-the-portkey-class)\n",
    "- [LLMBase Constructor](https://github.com/Portkey-AI/rubeus-python-sdk/blob/4cf3e17b847225123e92f8e8467b41d082186d60/rubeus/api_resources/utils.py#L179)\n",
    "- [List of Portkey + Llamaindex Features](#portkeys-integration-with-llamaindex-adds-the-following-production-capabilities-to-your-apps-out-of-the-box)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oPzt_a7w_Um"
   },
   "source": [
    "#### **üîÅ Implementing Fallbacks and Retries with Portkey**\n",
    "\n",
    "Fallbacks and retries are essential for building resilient AI applications. With Portkey, implementing these features is straightforward:\n",
    "\n",
    "- **Fallbacks**: If a primary service or model fails, Portkey will automatically switch to a backup model.\n",
    "- **Retries**: If a request fails, Portkey can be configured to retry the request multiple times.\n",
    "\n",
    "Below, we demonstrate how to set up fallbacks and retries using Portkey:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5V8xY7EtxGoG"
   },
   "outputs": [],
   "source": [
    "pk_llm = Portkey(mode=\"fallback\", retry=5)\n",
    "\n",
    "llm1 = LLMBase(provider=\"openai\", model=\"gpt-4\")\n",
    "llm2 = LLMBase(provider=\"openai\", model=\"gpt-3.5-turbo\")\n",
    "\n",
    "pk_llm.add_llms(llm_params=[llm1, llm2])\n",
    "\n",
    "print(\"Testing Fallback & Retry functionality:\")\n",
    "response = pk_llm.chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lv1UOqeFxKMD"
   },
   "source": [
    "#### **‚öñÔ∏è Implementing Load Balancing with Portkey**\n",
    "\n",
    "Load balancing ensures that incoming requests are efficiently distributed among multiple models. This not only enhances the performance but also provides redundancy in case one model fails.\n",
    "\n",
    "With Portkey, implementing load balancing is simple. You need to:\n",
    "\n",
    "- Define the `weight` parameter for each LLM. This weight determines how requests are distributed among the LLMs.\n",
    "- Ensure that the sum of weights for all LLMs equals 1.\n",
    "\n",
    "Here's an example of setting up load balancing with Portkey:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIKHztyuxM8i"
   },
   "outputs": [],
   "source": [
    "pk_llm = Portkey(mode=\"loadbalance\")\n",
    "\n",
    "llm1 = LLMBase(provider=\"openai\", model=\"gpt-4\", weight=0.2)\n",
    "llm2 = LLMBase(provider=\"openai\", model=\"gpt-3.5-turbo\", weight=0.8)\n",
    "\n",
    "pk_llm.add_llms(llm_params=[llm1, llm2])\n",
    "\n",
    "print(\"Testing Loadbalance functionality:\")\n",
    "response = pk_llm.chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0gVEWfVxNX1"
   },
   "source": [
    "#### **üß† Implementing Semantic Caching with Portkey**\n",
    "\n",
    "Semantic caching is a smart caching mechanism that understands the context of a request. Instead of caching based solely on exact input matches, semantic caching identifies similar requests and serves cached results, reducing redundant requests and improving response times as well as saving money.\n",
    "\n",
    "Let's see how to implement semantic caching with Portkey:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrNTROGAy-M-"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "pk_llm = Portkey(mode=\"single\", cache_status=\"semantic\")\n",
    "pk_llm.add_llms(llm1)\n",
    "\n",
    "current_messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant\"),\n",
    "    ChatMessage(role=\"user\", content=\"What are the ingredients of a pizza?\"),\n",
    "]\n",
    "\n",
    "print(\"Testing Portkey Semantic Cache:\")\n",
    "\n",
    "start = time.time()\n",
    "response = pk_llm.chat(current_messages)\n",
    "end = time.time() - start\n",
    "\n",
    "print(response)\n",
    "print(\"\\n--------------------------------------\\n\")\n",
    "print(f\"Served in {end} seconds.\")\n",
    "print(\"\\n--------------------------------------\\n\")\n",
    "\n",
    "new_messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant\"),\n",
    "    ChatMessage(role=\"user\", content=\"Ingredients of pizza\"),\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Testing Portkey Semantic Cache:\")\n",
    "\n",
    "start = time.time()\n",
    "response = pk_llm.chat(new_messages)\n",
    "end = time.time() - start\n",
    "\n",
    "print(response)\n",
    "print(\"\\n--------------------------------------\\n\")\n",
    "print(f\"Served in {end} seconds.\")\n",
    "print(\"\\n--------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJga03BHxcbo"
   },
   "source": [
    "Portkey's cache supports two more cache-critical functions - Force Refresh and Age.\n",
    "\n",
    "`cache_force_refresh`: Force-send a request to your provider instead of serving it from a cache.\n",
    "`cache_age`: Decide the interval at which the cache store for this particular string should get automatically refreshed. The cache age is set in seconds.\n",
    "\n",
    "Here's how you can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7osibv1exhrY"
   },
   "outputs": [],
   "source": [
    "pk_llm = Portkey(\n",
    "    mode=\"single\", cache_status=\"semantic\", cache_age=1000, cache_force_refresh=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujPsSDaJxjy1"
   },
   "source": [
    "#### **üî¨ Observability with Portkey**\n",
    "\n",
    "Having insight into your application's behavior is paramount. Portkey's observability features allow you to monitor, debug, and optimize your AI applications with ease. You can track each request, understand its journey, and segment them based on custom tags. This level of detail can help in identifying bottlenecks, optimizing costs, and enhancing the overall user experience.\n",
    "\n",
    "Here's how to set up observability with Portkey:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tevL0x-uxwyq"
   },
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"_environment\": \"production\",\n",
    "    \"_prompt\": \"test\",\n",
    "    \"_user\": \"user\",\n",
    "    \"_organisation\": \"acme\",\n",
    "}\n",
    "\n",
    "pk_llm = Portkey(\n",
    "    mode=\"single\",\n",
    "    trace_id=\"portkey_llamaindex_test\",\n",
    "    metadata=metadata,\n",
    ")\n",
    "pk_llm.add_llms(llm1)\n",
    "\n",
    "print(\"Testing Observability functionality:\")\n",
    "response = pk_llm.chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeOQL-9uxxQz"
   },
   "source": [
    "#### **üåâ AI Gateway with Rubeus**\n",
    "\n",
    "Rubeus is an open-source AI gateway that powers features like load balancing and fallbacks in Portkey. It acts as an intermediary, ensuring that your requests are processed optimally. One of the advantages of using Rubeus is its flexibility. You can easily customize its behavior, redirect requests to different providers, or even bypass logging to Portkey.\n",
    "\n",
    "Here's an example of customizing the behavior with Rubeus:\n",
    "\n",
    "```py\n",
    "pk_llm.base_url=None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIqlVGz2x9A0"
   },
   "source": [
    "#### **üìù Feedback with Portkey**\n",
    "\n",
    "Continuous improvement is a cornerstone of AI. To ensure your models and applications evolve and serve users better, feedback is vital. Portkey's Feedback API offers a straightforward way to gather weighted feedback from users, allowing you to refine and improve over time.\n",
    "\n",
    "Here's how to utilize the Feedback API with Portkey:\n",
    "\n",
    "Read more about [Feedback here](https://docs.portkey.ai/key-features/feedback-api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TWbS74dylVK"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Endpoint URL\n",
    "url = \"https://api.portkey.ai/v1/feedback\"\n",
    "\n",
    "# Headers\n",
    "headers = {\n",
    "    \"x-portkey-api-key\": \"<YOUR PORTKEY API KEY>\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# Data\n",
    "data = {\"trace_id\": \"REQUEST_TRACE_ID\", \"value\": 1}\n",
    "\n",
    "# Making the request\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "# Print the response\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the feedback with `weight` and `value` for each trace id is available on the Portkey dashboard:\n",
    "\n",
    "<img src=\"https://portkey.ai/blog/content/images/2023/09/feedback.png\" alt=\"Feedback\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHnOP8iSyLJy"
   },
   "source": [
    "#### **‚úÖ Conclusion**\n",
    "\n",
    "Integrating Portkey with Llamaindex simplifies the process of building robust and resilient AI applications. With features like semantic caching, observability, load balancing, feedback, and fallbacks, you can ensure optimal performance and continuous improvement.\n",
    "\n",
    "By following this guide, you've set up and tested the Portkey integration with Llamaindex. As you continue to build and deploy AI applications, remember to leverage the full potential of this integration!\n",
    "\n",
    "For further assistance or questions, reach out to the developers ‚û°Ô∏è <br />\n",
    "<a href=\"https://twitter.com/intent/follow?screen_name=portkeyai\" target=\"_blank\">\n",
    "  <img src=\"https://img.shields.io/twitter/follow/portkeyai?style=social&logo=twitter\" alt=\"Twitter\">\n",
    "</a>\n",
    "\n",
    "Join our community of practitioners putting LLMs into production ‚û°Ô∏è <br />\n",
    "<a href=\"https://discord.gg/sDk9JaNfK8\" target=\"_blank\">\n",
    "  <img src=\"https://img.shields.io/discord/1143393887742861333?logo=discord\" alt=\"Discord\">\n",
    "</a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
