{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use LlamaIndex agent tools as MCP tools\n",
    "\n",
    "We have dozens of agent tools in [LlamaHub](https://llamahub.ai/?tab=tools) and they can all be instantly used as MCP tools! This notebook shows how exactly that's done, using the [Notion Tool](https://llamahub.ai/l/tools/llama-index-tools-notion?from=tools) as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we install our tool, and our MCP server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-tools-notion mcp fastmcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring in our dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP fastMCP server dependencies imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies for Model Context Protocol (MCP) fastMCP server\n",
    "from typing import Any, Dict, List, Optional\n",
    "from fastmcp import FastMCP\n",
    "\n",
    "print(\"MCP fastMCP server dependencies imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate our tools using an API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and configure LlamaIndex Notion Tool Spec\n",
    "from llama_index.tools.notion import NotionToolSpec\n",
    "\n",
    "notion_token = \"xxxx\"\n",
    "tool_spec = NotionToolSpec(integration_token=notion_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what tools are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool 1: load_data\n",
      "Tool 2: search_data\n"
     ]
    }
   ],
   "source": [
    "tools = tool_spec.to_tool_list()\n",
    "\n",
    "for i, tool in enumerate(tools):\n",
    "    print(f\"Tool {i+1}: {tool.metadata.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create and configure the fastMCP server, and register each tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP Server configured with tools\n"
     ]
    }
   ],
   "source": [
    "mcp_server = FastMCP(\"MCP Agent Tools Server\")\n",
    "\n",
    "# Register the tools from the Notion ToolSpec\n",
    "for tool in tools:\n",
    "    mcp_server.tool(\n",
    "        name=tool.metadata.name, description=tool.metadata.description\n",
    "    )(tool.real_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run our MCP server complete with our tools!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[06/27/25 16:27:47] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Starting MCP server <span style=\"color: #008000; text-decoration-color: #008000\">'MCP Agent Tools Server'</span> with transport             <a href=\"file:///Users/seldo/projects/llamaindex/llama_index/docs/.venv/lib/python3.13/site-packages/fastmcp/server/server.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">server.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/seldo/projects/llamaindex/llama_index/docs/.venv/lib/python3.13/site-packages/fastmcp/server/server.py#1358\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1358</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'streamable-http'</span> on <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://127.0.0.1:8000/mcp/</span>                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[06/27/25 16:27:47]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting MCP server \u001b[32m'MCP Agent Tools Server'\u001b[0m with transport             \u001b]8;id=182776;file:///Users/seldo/projects/llamaindex/llama_index/docs/.venv/lib/python3.13/site-packages/fastmcp/server/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=402767;file:///Users/seldo/projects/llamaindex/llama_index/docs/.venv/lib/python3.13/site-packages/fastmcp/server/server.py#1358\u001b\\\u001b[2m1358\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m'streamable-http'\u001b[0m on \u001b[4;94mhttp://127.0.0.1:8000/mcp/\u001b[0m                         \u001b[2m              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [24668]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:54201 - \"POST /mcp/ HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54201 - \"POST /mcp/ HTTP/1.1\" 202 Accepted\n",
      "INFO:     127.0.0.1:54203 - \"GET /mcp/ HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54209 - \"POST /mcp/ HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "ERROR:    ASGI callable returned without completing response.\n",
      "ERROR:    Cancel 0 running task(s), timeout graceful shutdown exceeded\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [24668]\n"
     ]
    }
   ],
   "source": [
    "await mcp_server.run_async(transport=\"streamable-http\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
