{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Hierarchy Node Parser\n",
    "\n",
    "The `CodeHierarchyNodeParser` is useful to split long code files into more reasonable chunks. What this will do is create a \"Hierarchy\" of sorts, where sections of the code are made more reasonable by replacing the scope body with short comments telling the LLM to search for a referenced node if it wants to read that context body. This is called skeletonization, and is toggled by setting `skeleton` to `True` which it is by default. Nodes in this hierarchy will be split based on scope, like function, class, or method scope, and will have links to their children and parents so the LLM can traverse the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Import\n",
    "\n",
    "First be sure to install the necessary [tree-sitter](https://tree-sitter.github.io/tree-sitter/) libraries.\n",
    "\n",
    "`pip install tree-sitter tree-sitter-languages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser.code_hierarchy import CodeHierarchyNodeParser\n",
    "from llama_index.text_splitter.code_splitter import CodeSplitter\n",
    "from llama_index.readers import SimpleDirectoryReader\n",
    "from pathlib import Path\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, choose a directory you want to scan, and glob for all the code files you want to import.\n",
    "\n",
    "In this case I'm going to glob all \"*.py\" files in the `llama_index/node_parser` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = SimpleDirectoryReader(\n",
    "    input_files=Path(\"../../../../llama_index/node_parser\").glob(\"*.py\"),\n",
    "    file_metadata=lambda x: {\"filepath\": x},\n",
    ")\n",
    "nodes = reader.load_data()\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we got 8 files. Lets examine one of these nodes.\n",
    "We see here that the second one is 28756 characters long. That's way too long for most LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33980\n",
      "('from collections import defaultdict\\n'\n",
      " 'from enum import Enum\\n'\n",
      " 'from pprint import pformat\\n'\n",
      " 'from typing import Any, Dict, List, Optional, Sequence, Tuple\\n'\n",
      " '\\n'\n",
      " 'from llama_index.node_parser.extractors.metadata_extractors import '\n",
      " 'MetadataExtractor\\n'\n",
      " 'from llama_index.node_parser.interface import NodeParser\\n'\n",
      " 'from llama_index.node_parser.node_utils import get_nodes_from_node\\n'\n",
      " '\\n'\n",
      " 'try:\\n'\n",
      " '    from pydantic.v1 import BaseModel, Field\\n'\n",
      " 'except ImportError:\\n'\n",
      " '    from pydantic import BaseModel, Field\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'from tree_sitter import Node\\n'\n",
      " '\\n'\n",
      " 'from llama_index.callbacks.base import CallbackManager\\n'\n",
      " 'from llama_index.callbacks.schema import CBEventType, EventPayload\\n'\n",
      " 'from llama_index.schema import BaseNode, Document, NodeRelationship, '\n",
      " 'TextNode\\n'\n",
      " 'from llama_index.text_splitter.code_splitter import CodeSplitter\\n'\n",
      " 'from llama_index.utils import get_tqdm_iterable\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class _SignatureCaptureType(BaseModel):\\n'\n",
      " '    \"\"\"\\n'\n",
      " '    Unfortunately some languages need special options for how to make a '\n",
      " 'signature.\\n'\n",
      " '\\n'\n",
      " '    For example, html element signatures should include their closing >, '\n",
      " 'there is no\\n'\n",
      " '    easy way to include this using an always-exclusive system.\\n'\n",
      " '\\n'\n",
      " \"    However, using an always-inclusive system, python decorators don't \"\n",
      " 'work,\\n'\n",
      " \"    as there isn't an easy to define terminator for decorators that is \"\n",
      " 'inclusive\\n'\n",
      " '    to their signature.\\n'\n",
      " '    \"\"\"\\n'\n",
      " '\\n'\n",
      " '    type: str = Field(description=\"The type string to match on.\")\\n'\n",
      " '    inclusive: bool = Field(\\n'\n",
      " '        description=(\\n'\n",
      " '            \"Whether to include the text of the node matched by this type or '\n",
      " 'not.\"\\n'\n",
      " '        ),\\n'\n",
      " '    )\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class _SignatureCaptureOptions(BaseModel):\\n'\n",
      " '    start_signature_types: Optional[List[_SignatureCaptureType]] = Field(\\n'\n",
      " '        None,\\n'\n",
      " '        description=(\\n'\n",
      " '            \"A list of node types any of which indicate the beginning of the '\n",
      " 'signature.\"\\n'\n",
      " '            \"If this is none or empty, use the start_byte of the node.\"\\n'\n",
      " '        ),\\n'\n",
      " '    )\\n'\n",
      " '    end_signature_types: Optional[List[_SignatureCaptureType]] = Field(\\n'\n",
      " '        None,\\n'\n",
      " '        description=(\\n'\n",
      " '            \"A list of node types any of which indicate the end of the '\n",
      " 'signature.\"\\n'\n",
      " '            \"If this is none or empty, use the end_byte of the node.\"\\n'\n",
      " '        ),\\n'\n",
      " '    )\\n'\n",
      " '    name_identifier: str = Field(\\n'\n",
      " '        description=(\\n'\n",
      " '            \"The node type to use for the signatures \\'name\\'.If retrieving '\n",
      " 'the name is\"\\n'\n",
      " '            \" more complicated than a simple type match, use a function '\n",
      " 'which takes a\"\\n'\n",
      " '            \" node and returns true or false as to whether its the name or '\n",
      " 'not. The\"\\n'\n",
      " '            \" first match is returned.\"\\n'\n",
      " '        )\\n'\n",
      " '    )\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\"\"\"\\n'\n",
      " 'Maps language -> Node Type -> SignatureCaptureOptions\\n'\n",
      " '\\n'\n",
      " 'The best way for a developer to discover these is to put a breakpoint at the '\n",
      " 'TIP\\n'\n",
      " 'tag in _chunk_node, and then create a unit test for some code, and then '\n",
      " 'iterate\\n'\n",
      " 'through the code discovering the node names.\\n'\n",
      " '\"\"\"\\n'\n",
      " '_DEFAULT_SIGNATURE_IDENTIFIERS: Dict[str, Dict[str, '\n",
      " '_SignatureCaptureOptions]] = {\\n'\n",
      " '    \"python\": {\\n'\n",
      " '        \"function_definition\": _SignatureCaptureOptions(\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\"block\", '\n",
      " 'inclusive=False)],\\n'\n",
      " '            name_identifier=\"identifier\",\\n'\n",
      " '        ),\\n'\n",
      " '        \"class_definition\": _SignatureCaptureOptions(\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\"block\", '\n",
      " 'inclusive=False)],\\n'\n",
      " '            name_identifier=\"identifier\",\\n'\n",
      " '        ),\\n'\n",
      " '    },\\n'\n",
      " '    \"html\": {\\n'\n",
      " '        \"element\": _SignatureCaptureOptions(\\n'\n",
      " '            start_signature_types=[_SignatureCaptureType(type=\"<\", '\n",
      " 'inclusive=True)],\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\">\", '\n",
      " 'inclusive=True)],\\n'\n",
      " '            name_identifier=\"tag_name\",\\n'\n",
      " '        )\\n'\n",
      " '    },\\n'\n",
      " '    \"cpp\": {\\n'\n",
      " '        \"class_specifier\": _SignatureCaptureOptions(\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\"{\", '\n",
      " 'inclusive=False)],\\n'\n",
      " '            name_identifier=\"type_identifier\",\\n'\n",
      " '        ),\\n'\n",
      " '        \"function_definition\": _SignatureCaptureOptions(\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\"{\", '\n",
      " 'inclusive=False)],\\n'\n",
      " '            name_identifier=\"function_declarator\",\\n'\n",
      " '        ),\\n'\n",
      " '    },\\n'\n",
      " '    \"typescript\": {\\n'\n",
      " '        \"interface_declaration\": _SignatureCaptureOptions(\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\"{\", '\n",
      " 'inclusive=False)],\\n'\n",
      " '            name_identifier=\"type_identifier\",\\n'\n",
      " '        ),\\n'\n",
      " '        \"lexical_declaration\": _SignatureCaptureOptions(\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\"{\", '\n",
      " 'inclusive=False)],\\n'\n",
      " '            name_identifier=\"identifier\",\\n'\n",
      " '        ),\\n'\n",
      " '        \"function_declaration\": _SignatureCaptureOptions(\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\"{\", '\n",
      " 'inclusive=False)],\\n'\n",
      " '            name_identifier=\"identifier\",\\n'\n",
      " '        ),\\n'\n",
      " '        \"class_declaration\": _SignatureCaptureOptions(\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\"{\", '\n",
      " 'inclusive=False)],\\n'\n",
      " '            name_identifier=\"type_identifier\",\\n'\n",
      " '        ),\\n'\n",
      " '        \"method_definition\": _SignatureCaptureOptions(\\n'\n",
      " '            end_signature_types=[_SignatureCaptureType(type=\"{\", '\n",
      " 'inclusive=False)],\\n'\n",
      " '            name_identifier=\"property_identifier\",\\n'\n",
      " '        ),\\n'\n",
      " '    },\\n'\n",
      " '}\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class _ScopeMethod(Enum):\\n'\n",
      " '    INDENTATION = \"INDENTATION\"\\n'\n",
      " '    BRACKETS = \"BRACKETS\"\\n'\n",
      " '    HTML_END_TAGS = \"HTML_END_TAGS\"\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class _CommentOptions(BaseModel):\\n'\n",
      " '    comment_template: str\\n'\n",
      " '    scope_method: _ScopeMethod\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '_COMMENT_OPTIONS: Dict[str, _CommentOptions] = {\\n'\n",
      " '    \"cpp\": _CommentOptions(\\n'\n",
      " '        comment_template=\"// {}\", scope_method=_ScopeMethod.BRACKETS\\n'\n",
      " '    ),\\n'\n",
      " '    \"html\": _CommentOptions(\\n'\n",
      " '        comment_template=\"<!-- {} -->\", '\n",
      " 'scope_method=_ScopeMethod.HTML_END_TAGS\\n'\n",
      " '    ),\\n'\n",
      " '    \"python\": _CommentOptions(\\n'\n",
      " '        comment_template=\"# {}\", scope_method=_ScopeMethod.INDENTATION\\n'\n",
      " '    ),\\n'\n",
      " '    \"typescript\": _CommentOptions(\\n'\n",
      " '        comment_template=\"// {}\", scope_method=_ScopeMethod.BRACKETS\\n'\n",
      " '    ),\\n'\n",
      " '}\\n'\n",
      " '\\n'\n",
      " 'assert all(\\n'\n",
      " '    language in _DEFAULT_SIGNATURE_IDENTIFIERS for language in '\n",
      " '_COMMENT_OPTIONS\\n'\n",
      " '), \"Not all languages in _COMMENT_OPTIONS are in '\n",
      " '_DEFAULT_SIGNATURE_IDENTIFIERS\"\\n'\n",
      " 'assert all(\\n'\n",
      " '    language in _COMMENT_OPTIONS for language in '\n",
      " '_DEFAULT_SIGNATURE_IDENTIFIERS\\n'\n",
      " '), \"Not all languages in _DEFAULT_SIGNATURE_IDENTIFIERS are in '\n",
      " '_COMMENT_OPTIONS\"\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class _ScopeItem(BaseModel):\\n'\n",
      " '    \"\"\"Like a Node from tree_sitter, but with only the str information we '\n",
      " 'need.\"\"\"\\n'\n",
      " '\\n'\n",
      " '    name: str\\n'\n",
      " '    type: str\\n'\n",
      " '    signature: str\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class _ChunkNodeOutput(BaseModel):\\n'\n",
      " '    \"\"\"The output of a chunk_node call.\"\"\"\\n'\n",
      " '\\n'\n",
      " '    this_document: Optional[TextNode]\\n'\n",
      " '    upstream_children_documents: List[TextNode]\\n'\n",
      " '    all_documents: List[TextNode]\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class CodeHierarchyNodeParser(NodeParser):\\n'\n",
      " '    \"\"\"Split code using a AST parser.\\n'\n",
      " '\\n'\n",
      " '    Add metadata about the scope of the code block and relationships '\n",
      " 'between\\n'\n",
      " '    code blocks.\\n'\n",
      " '    \"\"\"\\n'\n",
      " '\\n'\n",
      " '    @classmethod\\n'\n",
      " '    def class_name(cls) -> str:\\n'\n",
      " '        \"\"\"Get class name.\"\"\"\\n'\n",
      " '        return \"CodeHierarchyNodeParser\"\\n'\n",
      " '\\n'\n",
      " '    language: str = Field(\\n'\n",
      " '        description=\"The programming language of the code being split.\"\\n'\n",
      " '    )\\n'\n",
      " '    signature_identifiers: Dict[str, _SignatureCaptureOptions] = Field(\\n'\n",
      " '        description=(\\n'\n",
      " '            \"A dictionary mapping the type of a split mapped to the first '\n",
      " 'and last type\"\\n'\n",
      " '            \" of itschildren which identify its signature.\"\\n'\n",
      " '        )\\n'\n",
      " '    )\\n'\n",
      " '    min_characters: int = Field(\\n'\n",
      " '        default=80,\\n'\n",
      " '        description=(\\n'\n",
      " '            \"Minimum number of characters per chunk.Defaults to 80 because '\n",
      " 'that\\'s about\"\\n'\n",
      " '            \" how long a replacement comment is in skeleton mode.\"\\n'\n",
      " '        ),\\n'\n",
      " '    )\\n'\n",
      " '    code_splitter: Optional[CodeSplitter] = Field(\\n'\n",
      " '        description=\"The text splitter to use when splitting documents.\"\\n'\n",
      " '    )\\n'\n",
      " '    metadata_extractor: Optional[MetadataExtractor] = Field(\\n'\n",
      " '        default=None, description=\"Metadata extraction pipeline to apply to '\n",
      " 'nodes.\"\\n'\n",
      " '    )\\n'\n",
      " '    callback_manager: CallbackManager = Field(\\n'\n",
      " '        default_factory=CallbackManager, exclude=True\\n'\n",
      " '    )\\n'\n",
      " '    skeleton: bool = Field(\\n'\n",
      " '        True,\\n'\n",
      " '        description=(\\n'\n",
      " '            \"Parent nodes have the text of their child nodes replaced with a '\n",
      " 'signature\"\\n'\n",
      " '            \" and a comment instructing the language model to visit the '\n",
      " 'child node for\"\\n'\n",
      " '            \" the full text of the scope.\"\\n'\n",
      " '        ),\\n'\n",
      " '    )\\n'\n",
      " '\\n'\n",
      " '    def __init__(\\n'\n",
      " '        self,\\n'\n",
      " '        language: str,\\n'\n",
      " '        skeleton: bool = True,\\n'\n",
      " '        signature_identifiers: Optional[Dict[str, _SignatureCaptureOptions]] '\n",
      " '= None,\\n'\n",
      " '        code_splitter: Optional[CodeSplitter] = None,\\n'\n",
      " '        callback_manager: Optional[CallbackManager] = None,\\n'\n",
      " '        metadata_extractor: Optional[MetadataExtractor] = None,\\n'\n",
      " '        min_characters: int = 80,\\n'\n",
      " '    ):\\n'\n",
      " '        callback_manager = callback_manager or CallbackManager([])\\n'\n",
      " '\\n'\n",
      " '        if signature_identifiers is None:\\n'\n",
      " '            try:\\n'\n",
      " '                signature_identifiers = '\n",
      " '_DEFAULT_SIGNATURE_IDENTIFIERS[language]\\n'\n",
      " '            except KeyError:\\n'\n",
      " '                raise ValueError(\\n'\n",
      " '                    f\"Must provide signature_identifiers for language '\n",
      " '{language}.\"\\n'\n",
      " '                )\\n'\n",
      " '\\n'\n",
      " '        super().__init__(\\n'\n",
      " '            language=language,\\n'\n",
      " '            callback_manager=callback_manager,\\n'\n",
      " '            metadata_extractor=metadata_extractor,\\n'\n",
      " '            code_splitter=code_splitter,\\n'\n",
      " '            signature_identifiers=signature_identifiers,\\n'\n",
      " '            min_characters=min_characters,\\n'\n",
      " '            skeleton=skeleton,\\n'\n",
      " '        )\\n'\n",
      " '\\n'\n",
      " '    def _get_node_name(self, node: Node) -> str:\\n'\n",
      " '        \"\"\"Get the name of a node.\"\"\"\\n'\n",
      " '        signature_identifier = self.signature_identifiers[node.type]\\n'\n",
      " '\\n'\n",
      " '        def recur(node: Node) -> str:\\n'\n",
      " '            for child in node.children:\\n'\n",
      " '                if child.type == signature_identifier.name_identifier:\\n'\n",
      " '                    return child.text.decode()\\n'\n",
      " '                if child.children:\\n'\n",
      " '                    out = recur(child)\\n'\n",
      " '                    if out:\\n'\n",
      " '                        return out\\n'\n",
      " '            return \"\"\\n'\n",
      " '\\n'\n",
      " '        return recur(node).strip()\\n'\n",
      " '\\n'\n",
      " '    def _get_node_signature(self, text: str, node: Node) -> str:\\n'\n",
      " '        \"\"\"Get the signature of a node.\"\"\"\\n'\n",
      " '        signature_identifier = self.signature_identifiers[node.type]\\n'\n",
      " '\\n'\n",
      " '        def find_start(node: Node) -> Optional[int]:\\n'\n",
      " '            if not signature_identifier.start_signature_types:\\n'\n",
      " '                signature_identifier.start_signature_types = []\\n'\n",
      " '\\n'\n",
      " '            for st in signature_identifier.start_signature_types:\\n'\n",
      " '                if node.type == st.type:\\n'\n",
      " '                    if st.inclusive:\\n'\n",
      " '                        return node.start_byte\\n'\n",
      " '                    return node.end_byte\\n'\n",
      " '\\n'\n",
      " '            for child in node.children:\\n'\n",
      " '                out = find_start(child)\\n'\n",
      " '                if out is not None:\\n'\n",
      " '                    return out\\n'\n",
      " '\\n'\n",
      " '            return None\\n'\n",
      " '\\n'\n",
      " '        def find_end(node: Node) -> Optional[int]:\\n'\n",
      " '            if not signature_identifier.end_signature_types:\\n'\n",
      " '                signature_identifier.end_signature_types = []\\n'\n",
      " '\\n'\n",
      " '            for st in signature_identifier.end_signature_types:\\n'\n",
      " '                if node.type == st.type:\\n'\n",
      " '                    if st.inclusive:\\n'\n",
      " '                        return node.end_byte\\n'\n",
      " '                    return node.start_byte\\n'\n",
      " '\\n'\n",
      " '            for child in node.children:\\n'\n",
      " '                out = find_end(child)\\n'\n",
      " '                if out is not None:\\n'\n",
      " '                    return out\\n'\n",
      " '\\n'\n",
      " '            return None\\n'\n",
      " '\\n'\n",
      " '        start_byte, end_byte = find_start(node), find_end(node)\\n'\n",
      " '        if start_byte is None:\\n'\n",
      " '            start_byte = node.start_byte\\n'\n",
      " '        if end_byte is None:\\n'\n",
      " '            end_byte = node.end_byte\\n'\n",
      " '        return text[start_byte:end_byte].strip()\\n'\n",
      " '\\n'\n",
      " '    def _chunk_node(\\n'\n",
      " '        self,\\n'\n",
      " '        parent: Node,\\n'\n",
      " '        text: str,\\n'\n",
      " '        _context_list: Optional[List[_ScopeItem]] = None,\\n'\n",
      " '        _root: bool = True,\\n'\n",
      " '    ) -> _ChunkNodeOutput:\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        This is really the \"main\" method of this class. It is recursive and '\n",
      " 'recursively\\n'\n",
      " '        chunks the text by the options identified in '\n",
      " 'self.signature_identifiers.\\n'\n",
      " '\\n'\n",
      " '        It is ran by get_nodes_from_documents.\\n'\n",
      " '\\n'\n",
      " '        Args:\\n'\n",
      " '            parent (Node): The parent node to chunk\\n'\n",
      " '            text (str): The text of the entire document\\n'\n",
      " '            _context_list (Optional[List[_ScopeItem]]): The scope context of '\n",
      " 'the\\n'\n",
      " '                                                        parent node\\n'\n",
      " '            _root (bool): Whether or not this is the root node\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        if _context_list is None:\\n'\n",
      " '            _context_list = []\\n'\n",
      " '\\n'\n",
      " '        upstream_children_documents: List[TextNode] = []\\n'\n",
      " '        all_documents: List[TextNode] = []\\n'\n",
      " '\\n'\n",
      " '        # Capture any whitespace before parent.start_byte\\n'\n",
      " '        # Very important for space sensitive languages like python\\n'\n",
      " '        start_byte = parent.start_byte\\n'\n",
      " '        while start_byte > 0 and text[start_byte - 1] in (\" \", \"\\\\t\"):\\n'\n",
      " '            start_byte -= 1\\n'\n",
      " '\\n'\n",
      " '        # Create this node\\n'\n",
      " '        current_chunk = text[start_byte : parent.end_byte]\\n'\n",
      " '\\n'\n",
      " '        # Return early if the chunk is too small\\n'\n",
      " '        if len(current_chunk) < self.min_characters and not _root:\\n'\n",
      " '            return _ChunkNodeOutput(\\n'\n",
      " '                this_document=None, all_documents=[], '\n",
      " 'upstream_children_documents=[]\\n'\n",
      " '            )\\n'\n",
      " '\\n'\n",
      " '        # TIP: This is a wonderful place to put a debug breakpoint when\\n'\n",
      " '        #      Trying to integrate a new language. Pay attention to '\n",
      " 'parent.type to learn\\n'\n",
      " '        #      all the available node types and their hierarchy.\\n'\n",
      " '        if parent.type in self.signature_identifiers or _root:\\n'\n",
      " '            # Get the new context\\n'\n",
      " '            if not _root:\\n'\n",
      " '                new_context = _ScopeItem(\\n'\n",
      " '                    name=self._get_node_name(parent),\\n'\n",
      " '                    type=parent.type,\\n'\n",
      " '                    signature=self._get_node_signature(text=text, '\n",
      " 'node=parent),\\n'\n",
      " '                )\\n'\n",
      " '                _context_list.append(new_context)\\n'\n",
      " '            this_document = TextNode(\\n'\n",
      " '                text=current_chunk,\\n'\n",
      " '                metadata={\\n'\n",
      " '                    \"inclusive_scopes\": [cl.dict() for cl in '\n",
      " '_context_list],\\n'\n",
      " '                },\\n'\n",
      " '                relationships={\\n'\n",
      " '                    NodeRelationship.CHILD: [],\\n'\n",
      " '                },\\n'\n",
      " '            )\\n'\n",
      " '            all_documents.append(this_document)\\n'\n",
      " '        else:\\n'\n",
      " '            this_document = None\\n'\n",
      " '\\n'\n",
      " '        # Iterate over children\\n'\n",
      " '        for child in parent.children:\\n'\n",
      " '            if child.children:\\n'\n",
      " '                # Recurse on the child\\n'\n",
      " '                next_chunks = self._chunk_node(\\n'\n",
      " '                    child, text, _context_list=_context_list.copy(), '\n",
      " '_root=False\\n'\n",
      " '                )\\n'\n",
      " '\\n'\n",
      " '                # If there is a this_document, then we need\\n'\n",
      " '                # to add the children to this_document\\n'\n",
      " '                # and flush upstream_children_documents\\n'\n",
      " '                if this_document is not None:\\n'\n",
      " \"                    # If we have been given a document, that means it's \"\n",
      " 'children\\n'\n",
      " '                    # are already set, so it needs to become a child of this '\n",
      " 'node\\n'\n",
      " '                    if next_chunks.this_document is not None:\\n'\n",
      " '                        assert not next_chunks.upstream_children_documents, '\n",
      " '(\\n'\n",
      " '                            \"next_chunks.this_document and\"\\n'\n",
      " '                            \" next_chunks.upstream_children_documents are '\n",
      " 'exclusive.\"\\n'\n",
      " '                        )\\n'\n",
      " '                        this_document.relationships[\\n'\n",
      " '                            NodeRelationship.CHILD\\n'\n",
      " '                        ].append(  # type: ignore\\n'\n",
      " '                            '\n",
      " 'next_chunks.this_document.as_related_node_info()\\n'\n",
      " '                        )\\n'\n",
      " '                        next_chunks.this_document.relationships[\\n'\n",
      " '                            NodeRelationship.PARENT\\n'\n",
      " '                        ] = this_document.as_related_node_info()\\n'\n",
      " '                    # Otherwise, we have been given a list of\\n'\n",
      " '                    # upstream_children_documents. We need to make\\n'\n",
      " '                    # them a child of this node\\n'\n",
      " '                    else:\\n'\n",
      " '                        for d in next_chunks.upstream_children_documents:\\n'\n",
      " '                            this_document.relationships[\\n'\n",
      " '                                NodeRelationship.CHILD\\n'\n",
      " '                            ].append(  # type: ignore\\n'\n",
      " '                                d.as_related_node_info()\\n'\n",
      " '                            )\\n'\n",
      " '                            d.relationships[\\n'\n",
      " '                                NodeRelationship.PARENT\\n'\n",
      " '                            ] = this_document.as_related_node_info()\\n'\n",
      " '                # Otherwise we pass the children upstream\\n'\n",
      " '                else:\\n'\n",
      " \"                    # If we have been given a document, that means it's\\n\"\n",
      " '                    # children are already set, so it needs to become a\\n'\n",
      " '                    # child of the next node\\n'\n",
      " '                    if next_chunks.this_document is not None:\\n'\n",
      " '                        assert not next_chunks.upstream_children_documents, '\n",
      " '(\\n'\n",
      " '                            \"next_chunks.this_document and\"\\n'\n",
      " '                            \" next_chunks.upstream_children_documents are '\n",
      " 'exclusive.\"\\n'\n",
      " '                        )\\n'\n",
      " '                        '\n",
      " 'upstream_children_documents.append(next_chunks.this_document)\\n'\n",
      " '                    # Otherwise, we have leftover children, they need\\n'\n",
      " '                    # to become children of the next node\\n'\n",
      " '                    else:\\n'\n",
      " '                        upstream_children_documents.extend(\\n'\n",
      " '                            next_chunks.upstream_children_documents\\n'\n",
      " '                        )\\n'\n",
      " '\\n'\n",
      " '                # Lastly we need to maintain all documents\\n'\n",
      " '                all_documents.extend(next_chunks.all_documents)\\n'\n",
      " '\\n'\n",
      " '        return _ChunkNodeOutput(\\n'\n",
      " '            this_document=this_document,\\n'\n",
      " '            upstream_children_documents=upstream_children_documents,\\n'\n",
      " '            all_documents=all_documents,\\n'\n",
      " '        )\\n'\n",
      " '\\n'\n",
      " '    @staticmethod\\n'\n",
      " '    def get_code_hierarchy_from_nodes(\\n'\n",
      " '        nodes: Sequence[BaseNode],\\n'\n",
      " '    ) -> str:\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        Creates a code hierarchy appropriate to put into a tool description '\n",
      " 'or context\\n'\n",
      " '        to make it easier to search for code.\\n'\n",
      " '\\n'\n",
      " '        Call after `get_nodes_from_documents` and pass that output to this '\n",
      " 'function.\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        out: Dict[str, Any] = defaultdict(dict)\\n'\n",
      " '\\n'\n",
      " '        def get_subdict(keys: list[str]) -> Dict[str, Any]:\\n'\n",
      " '            # Get the dictionary we are operating on\\n'\n",
      " '            this_dict = out\\n'\n",
      " '            for key in keys:\\n'\n",
      " '                if key not in this_dict:\\n'\n",
      " '                    this_dict[key] = defaultdict(dict)\\n'\n",
      " '                this_dict = this_dict[key]\\n'\n",
      " '            return this_dict\\n'\n",
      " '\\n'\n",
      " '        def recur_inclusive_scope(node: BaseNode, i: int, keys: list[str]) '\n",
      " '-> None:\\n'\n",
      " '            if \"inclusive_scopes\" not in node.metadata:\\n'\n",
      " '                raise KeyError(\"inclusive_scopes not in node.metadata\")\\n'\n",
      " '            if i >= len(node.metadata[\"inclusive_scopes\"]):\\n'\n",
      " '                return\\n'\n",
      " '            scope = node.metadata[\"inclusive_scopes\"][i]\\n'\n",
      " '\\n'\n",
      " '            this_dict = get_subdict(keys)\\n'\n",
      " '\\n'\n",
      " '            if scope[\"name\"] not in this_dict:\\n'\n",
      " '                this_dict[scope[\"name\"]] = defaultdict(dict)\\n'\n",
      " '\\n'\n",
      " '            recur_inclusive_scope(node, i+1, keys + [scope[\"name\"]])\\n'\n",
      " '\\n'\n",
      " '        def dict_to_markdown(d, depth=0):\\n'\n",
      " '            markdown = \"\"\\n'\n",
      " '            indent = \"  \" * depth  # Two spaces per depth level\\n'\n",
      " '\\n'\n",
      " '            for key, value in d.items():\\n'\n",
      " '                if isinstance(value, dict):  # Check if value is a dict\\n'\n",
      " '                    # Add the key with a bullet point and increase depth for '\n",
      " 'nested dicts\\n'\n",
      " '                    markdown += f\"{indent}- {key}\\\\n{dict_to_markdown(value, '\n",
      " 'depth + 1)}\"\\n'\n",
      " '                else:\\n'\n",
      " '                    # Handle non-dict items if necessary\\n'\n",
      " '                    markdown += f\"{indent}- {key}: {value}\\\\n\"\\n'\n",
      " '\\n'\n",
      " '            return markdown\\n'\n",
      " '\\n'\n",
      " '        for node in nodes:\\n'\n",
      " '            recur_inclusive_scope(node, 0, '\n",
      " 'node.metadata[\"filepath\"].split(\\'/\\'))\\n'\n",
      " '\\n'\n",
      " '        return dict_to_markdown(out)\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '    def get_nodes_from_documents(\\n'\n",
      " '        self,\\n'\n",
      " '        documents: Sequence[Document],\\n'\n",
      " '        show_progress: bool = False,\\n'\n",
      " '    ) -> List[BaseNode]:\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        The main public method of this class.\\n'\n",
      " '\\n'\n",
      " '        Parse documents into nodes.\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        out: List[BaseNode] = []\\n'\n",
      " '        with self.callback_manager.event(\\n'\n",
      " '            CBEventType.CHUNKING,\\n'\n",
      " '            payload={EventPayload.CHUNKS: [document.text for document in '\n",
      " 'documents]},\\n'\n",
      " '        ) as event:\\n'\n",
      " '            try:\\n'\n",
      " '                import tree_sitter_languages\\n'\n",
      " '            except ImportError:\\n'\n",
      " '                raise ImportError(\\n'\n",
      " '                    \"Please install tree_sitter_languages to use '\n",
      " 'CodeSplitter.\"\\n'\n",
      " '                )\\n'\n",
      " '\\n'\n",
      " '            try:\\n'\n",
      " '                parser = tree_sitter_languages.get_parser(self.language)\\n'\n",
      " '            except Exception as e:\\n'\n",
      " '                print(\\n'\n",
      " '                    f\"Could not get parser for language {self.language}. '\n",
      " 'Check \"\\n'\n",
      " '                    '\n",
      " '\"https://github.com/grantjenks/py-tree-sitter-languages#license \"\\n'\n",
      " '                    \"for a list of valid languages.\"\\n'\n",
      " '                )\\n'\n",
      " '                raise e  # noqa: TRY201\\n'\n",
      " '\\n'\n",
      " '            documents_with_progress = get_tqdm_iterable(\\n'\n",
      " '                documents, show_progress, \"Parsing documents into nodes\"\\n'\n",
      " '            )\\n'\n",
      " '            for document in documents_with_progress:\\n'\n",
      " '                text = document.text\\n'\n",
      " '                tree = parser.parse(bytes(text, \"utf-8\"))\\n'\n",
      " '\\n'\n",
      " '                if (\\n'\n",
      " '                    not tree.root_node.children\\n'\n",
      " '                    or tree.root_node.children[0].type != \"ERROR\"\\n'\n",
      " '                ):\\n'\n",
      " '                    # Chunk the code\\n'\n",
      " '                    _chunks = self._chunk_node(tree.root_node, '\n",
      " 'document.text)\\n'\n",
      " '                    assert (\\n'\n",
      " '                        _chunks.this_document is not None\\n'\n",
      " '                    ), \"Root node must be a chunk\"\\n'\n",
      " '                    chunks = _chunks.all_documents\\n'\n",
      " '\\n'\n",
      " '                    # Add your metadata to the chunks here\\n'\n",
      " '                    for chunk in chunks:\\n'\n",
      " '                        chunk.metadata = {\\n'\n",
      " '                            \"language\": self.language,\\n'\n",
      " '                            **chunk.metadata,\\n'\n",
      " '                            **document.metadata,\\n'\n",
      " '                        }\\n'\n",
      " '                        chunk.relationships[\\n'\n",
      " '                            NodeRelationship.SOURCE\\n'\n",
      " '                        ] = document.as_related_node_info()\\n'\n",
      " '\\n'\n",
      " '                    if self.skeleton:\\n'\n",
      " '                        self._skeletonize_list(chunks)\\n'\n",
      " '\\n'\n",
      " '                    # Now further split the code by lines and characters\\n'\n",
      " '                    # TODO: Test this and the relationships it creates\\n'\n",
      " '                    if self.code_splitter:\\n'\n",
      " '                        new_nodes = []\\n'\n",
      " '                        for original_node in chunks:\\n'\n",
      " '                            new_split_nodes = get_nodes_from_node(\\n'\n",
      " '                                original_node,\\n'\n",
      " '                                text_splitter=self.code_splitter,\\n'\n",
      " '                                include_metadata=True,\\n'\n",
      " '                                include_prev_next_rel=True,\\n'\n",
      " '                            )\\n'\n",
      " '\\n'\n",
      " '                            # Force the first new_split_node to have the\\n'\n",
      " '                            # same id as the original_node\\n'\n",
      " '                            new_split_nodes[0].id_ = original_node.id_\\n'\n",
      " '\\n'\n",
      " '                            # Add the UUID of the next node to the end of '\n",
      " 'all nodes\\n'\n",
      " '                            for i, new_split_node in '\n",
      " 'enumerate(new_split_nodes[:-1]):\\n'\n",
      " '                                new_split_node.text = (\\n'\n",
      " '                                    new_split_node.text\\n'\n",
      " '                                    + \"\\\\n\"\\n'\n",
      " '                                    + '\n",
      " 'self._create_comment_line(new_split_nodes[i + 1])\\n'\n",
      " '                                ).strip()\\n'\n",
      " '\\n'\n",
      " '                            # Add the UUID of the previous node to the '\n",
      " 'beginning of all nodes\\n'\n",
      " '                            for i, new_split_node in '\n",
      " 'enumerate(new_split_nodes[1:]):\\n'\n",
      " '                                new_split_node.text = (\\n'\n",
      " '                                    '\n",
      " 'self._create_comment_line(new_split_nodes[i])\\n'\n",
      " '                                    + new_split_node.text\\n'\n",
      " '                                ).strip()\\n'\n",
      " '\\n'\n",
      " '                            # Add the parent child info to all the '\n",
      " 'new_nodes_\\n'\n",
      " '                            # derived from node\\n'\n",
      " '                            for new_split_node in new_split_nodes:\\n'\n",
      " '                                new_split_node.relationships[\\n'\n",
      " '                                    NodeRelationship.CHILD\\n'\n",
      " '                                ] = original_node.child_nodes  # type: '\n",
      " 'ignore\\n'\n",
      " '                                new_split_node.relationships[\\n'\n",
      " '                                    NodeRelationship.PARENT\\n'\n",
      " '                                ] = original_node.parent_node  # type: '\n",
      " 'ignore\\n'\n",
      " '\\n'\n",
      " '                            # Go through chunks and replace all\\n'\n",
      " '                            # instances of node.node_id in relationships\\n'\n",
      " '                            # with new_nodes_[0].node_id\\n'\n",
      " '                            for old_node in chunks:\\n'\n",
      " '                                # Handle child nodes, which are a list\\n'\n",
      " '                                new_children = []\\n'\n",
      " '                                for old_nodes_child in old_node.child_nodes '\n",
      " 'or []:\\n'\n",
      " '                                    if old_nodes_child.node_id == '\n",
      " 'original_node.node_id:\\n'\n",
      " '                                        new_children.append(\\n'\n",
      " '                                            '\n",
      " 'new_split_nodes[0].as_related_node_info()\\n'\n",
      " '                                        )\\n'\n",
      " '                                    new_children.append(old_nodes_child)\\n'\n",
      " '                                old_node.relationships[\\n'\n",
      " '                                    NodeRelationship.CHILD\\n'\n",
      " '                                ] = new_children\\n'\n",
      " '\\n'\n",
      " '                                # Handle parent node\\n'\n",
      " '                                if (\\n'\n",
      " '                                    old_node.parent_node\\n'\n",
      " '                                    and old_node.parent_node.node_id\\n'\n",
      " '                                    == original_node.node_id\\n'\n",
      " '                                ):\\n'\n",
      " '                                    old_node.relationships[\\n'\n",
      " '                                        NodeRelationship.PARENT\\n'\n",
      " '                                    ] = '\n",
      " 'new_split_nodes[0].as_related_node_info()\\n'\n",
      " '\\n'\n",
      " '                            # Now save new_nodes_\\n'\n",
      " '                            new_nodes += new_split_nodes\\n'\n",
      " '\\n'\n",
      " '                        chunks = new_nodes\\n'\n",
      " '\\n'\n",
      " '                    # Or just extract metadata\\n'\n",
      " '                    if self.metadata_extractor:\\n'\n",
      " '                        chunks = self.metadata_extractor.process_nodes(  # '\n",
      " 'type: ignore\\n'\n",
      " '                            chunks\\n'\n",
      " '                        )\\n'\n",
      " '\\n'\n",
      " '                    event.on_end(\\n'\n",
      " '                        payload={EventPayload.CHUNKS: chunks},\\n'\n",
      " '                    )\\n'\n",
      " '\\n'\n",
      " '                    out += chunks\\n'\n",
      " '                else:\\n'\n",
      " '                    raise ValueError(\\n'\n",
      " '                        f\"Could not parse code with language '\n",
      " '{self.language}.\"\\n'\n",
      " '                    )\\n'\n",
      " '\\n'\n",
      " '        return out\\n'\n",
      " '\\n'\n",
      " '    @staticmethod\\n'\n",
      " '    def _get_indentation(text: str) -> Tuple[str, int, int]:\\n'\n",
      " '        indent_char = None\\n'\n",
      " '        minimum_chain = None\\n'\n",
      " '\\n'\n",
      " '        # Check that text is at least 1 line long\\n'\n",
      " '        text_split = text.splitlines()\\n'\n",
      " '        if len(text_split) == 0:\\n'\n",
      " '            raise ValueError(\"Text should be at least one line long.\")\\n'\n",
      " '\\n'\n",
      " '        for line in text_split:\\n'\n",
      " '            stripped_line = line.lstrip()\\n'\n",
      " '\\n'\n",
      " '            if stripped_line:\\n'\n",
      " \"                # Get whether it's tabs or spaces\\n\"\n",
      " '                spaces_count = line.count(\" \", 0, len(line) - '\n",
      " 'len(stripped_line))\\n'\n",
      " '                tabs_count = line.count(\"\\\\t\", 0, len(line) - '\n",
      " 'len(stripped_line))\\n'\n",
      " '\\n'\n",
      " '                if not indent_char:\\n'\n",
      " '                    if spaces_count:\\n'\n",
      " '                        indent_char = \" \"\\n'\n",
      " '                    if tabs_count:\\n'\n",
      " '                        indent_char = \"\\\\t\"\\n'\n",
      " '\\n'\n",
      " '                # Detect mixed indentation.\\n'\n",
      " '                if spaces_count > 0 and tabs_count > 0:\\n'\n",
      " '                    raise ValueError(\"Mixed indentation found.\")\\n'\n",
      " '                if indent_char == \" \" and tabs_count > 0:\\n'\n",
      " '                    raise ValueError(\"Mixed indentation found.\")\\n'\n",
      " '                if indent_char == \"\\\\t\" and spaces_count > 0:\\n'\n",
      " '                    raise ValueError(\"Mixed indentation found.\")\\n'\n",
      " '\\n'\n",
      " '                # Get the minimum chain of indent_char\\n'\n",
      " '                if indent_char:\\n'\n",
      " '                    char_count = line.count(\\n'\n",
      " '                        indent_char, 0, len(line) - len(stripped_line)\\n'\n",
      " '                    )\\n'\n",
      " '                    if minimum_chain is not None:\\n'\n",
      " '                        if char_count > 0:\\n'\n",
      " '                            minimum_chain = min(char_count, minimum_chain)\\n'\n",
      " '                    else:\\n'\n",
      " '                        if char_count > 0:\\n'\n",
      " '                            minimum_chain = char_count\\n'\n",
      " '\\n'\n",
      " '        # Handle edge case\\n'\n",
      " '        if indent_char is None:\\n'\n",
      " '            indent_char = \" \"\\n'\n",
      " '        if minimum_chain is None:\\n'\n",
      " '            minimum_chain = 4\\n'\n",
      " '\\n'\n",
      " '        # Get the first indent count\\n'\n",
      " '        first_line = text_split[0]\\n'\n",
      " '        first_indent_count = 0\\n'\n",
      " '        for char in first_line:\\n'\n",
      " '            if char == indent_char:\\n'\n",
      " '                first_indent_count += 1\\n'\n",
      " '            else:\\n'\n",
      " '                break\\n'\n",
      " '\\n'\n",
      " '        # Return the default indent level if only one indentation level was '\n",
      " 'found.\\n'\n",
      " '        return indent_char, minimum_chain, first_indent_count // '\n",
      " 'minimum_chain\\n'\n",
      " '\\n'\n",
      " '    @staticmethod\\n'\n",
      " '    def _get_comment_text(node: TextNode) -> str:\\n'\n",
      " '        \"\"\"Gets just the natural language text for a skeletonize '\n",
      " 'comment.\"\"\"\\n'\n",
      " '        return f\"Code replaced for brevity. See node_id {node.node_id}\"\\n'\n",
      " '\\n'\n",
      " '    @classmethod\\n'\n",
      " '    def _create_comment_line(cls, node: TextNode) -> str:\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        Creates a comment line for a node.\\n'\n",
      " '\\n'\n",
      " \"        Sometimes we don't use this in a loop because it requires \"\n",
      " 'recalculating\\n'\n",
      " '        a lot of the same information. But it is handy.\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        # Create the text to replace the child_node.text with\\n'\n",
      " '        language = node.metadata[\"language\"]\\n'\n",
      " '        if language not in _COMMENT_OPTIONS:\\n'\n",
      " '            # TODO: Create a contribution message\\n'\n",
      " '            raise KeyError(\"Language not yet supported. Please '\n",
      " 'contribute!\")\\n'\n",
      " '        comment_options = _COMMENT_OPTIONS[language]\\n'\n",
      " '        (\\n'\n",
      " '            indentation_char,\\n'\n",
      " '            indentation_count_per_lvl,\\n'\n",
      " '            first_indentation_lvl,\\n'\n",
      " '        ) = cls._get_indentation(node.text)\\n'\n",
      " '        return (\\n'\n",
      " '            indentation_char * indentation_count_per_lvl * '\n",
      " '(first_indentation_lvl + 1)\\n'\n",
      " '            + '\n",
      " 'comment_options.comment_template.format(cls._get_comment_text(node))\\n'\n",
      " '            + \"\\\\n\"\\n'\n",
      " '        )\\n'\n",
      " '\\n'\n",
      " '    @classmethod\\n'\n",
      " '    def _get_replacement_text(cls, child_node: TextNode) -> str:\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        Manufactures a the replacement text to use to skeletonize a given '\n",
      " 'child node.\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        signature = '\n",
      " 'child_node.metadata[\"inclusive_scopes\"][-1][\"signature\"]\\n'\n",
      " '        language = child_node.metadata[\"language\"]\\n'\n",
      " '        if language not in _COMMENT_OPTIONS:\\n'\n",
      " '            # TODO: Create a contribution message\\n'\n",
      " '            raise KeyError(\"Language not yet supported. Please '\n",
      " 'contribute!\")\\n'\n",
      " '        comment_options = _COMMENT_OPTIONS[language]\\n'\n",
      " '\\n'\n",
      " '        # Create the text to replace the child_node.text with\\n'\n",
      " '        (\\n'\n",
      " '            indentation_char,\\n'\n",
      " '            indentation_count_per_lvl,\\n'\n",
      " '            first_indentation_lvl,\\n'\n",
      " '        ) = cls._get_indentation(child_node.text)\\n'\n",
      " '\\n'\n",
      " '        # Start with a properly indented signature\\n'\n",
      " '        replacement_txt = (\\n'\n",
      " '            indentation_char * indentation_count_per_lvl * '\n",
      " 'first_indentation_lvl\\n'\n",
      " '            + signature\\n'\n",
      " '        )\\n'\n",
      " '\\n'\n",
      " '        # Add brackets if necessary. Expandable in the\\n'\n",
      " '        # future to other methods of scoping.\\n'\n",
      " '        if comment_options.scope_method == _ScopeMethod.BRACKETS:\\n'\n",
      " '            replacement_txt += \" {\\\\n\"\\n'\n",
      " '            replacement_txt += (\\n'\n",
      " '                indentation_char\\n'\n",
      " '                * indentation_count_per_lvl\\n'\n",
      " '                * (first_indentation_lvl + 1)\\n'\n",
      " '                + comment_options.comment_template.format(\\n'\n",
      " '                    cls._get_comment_text(child_node)\\n'\n",
      " '                )\\n'\n",
      " '                + \"\\\\n\"\\n'\n",
      " '            )\\n'\n",
      " '            replacement_txt += (\\n'\n",
      " '                indentation_char * indentation_count_per_lvl * '\n",
      " 'first_indentation_lvl\\n'\n",
      " '                + \"}\"\\n'\n",
      " '            )\\n'\n",
      " '\\n'\n",
      " '        elif comment_options.scope_method == _ScopeMethod.INDENTATION:\\n'\n",
      " '            replacement_txt += \"\\\\n\"\\n'\n",
      " '            replacement_txt += indentation_char * indentation_count_per_lvl '\n",
      " '* (\\n'\n",
      " '                first_indentation_lvl + 1\\n'\n",
      " '            ) + comment_options.comment_template.format(\\n'\n",
      " '                cls._get_comment_text(child_node)\\n'\n",
      " '            )\\n'\n",
      " '\\n'\n",
      " '        elif comment_options.scope_method == _ScopeMethod.HTML_END_TAGS:\\n'\n",
      " '            tag_name = child_node.metadata[\"inclusive_scopes\"][-1][\"name\"]\\n'\n",
      " '            end_tag = f\"</{tag_name}>\"\\n'\n",
      " '            replacement_txt += \"\\\\n\"\\n'\n",
      " '            replacement_txt += (\\n'\n",
      " '                indentation_char\\n'\n",
      " '                * indentation_count_per_lvl\\n'\n",
      " '                * (first_indentation_lvl + 1)\\n'\n",
      " '                + comment_options.comment_template.format(\\n'\n",
      " '                    cls._get_comment_text(child_node)\\n'\n",
      " '                )\\n'\n",
      " '                + \"\\\\n\"\\n'\n",
      " '            )\\n'\n",
      " '            replacement_txt += (\\n'\n",
      " '                indentation_char * indentation_count_per_lvl * '\n",
      " 'first_indentation_lvl\\n'\n",
      " '                + end_tag\\n'\n",
      " '            )\\n'\n",
      " '\\n'\n",
      " '        else:\\n'\n",
      " '            raise KeyError(f\"Unrecognized enum value '\n",
      " '{comment_options.scope_method}\")\\n'\n",
      " '\\n'\n",
      " '        return replacement_txt\\n'\n",
      " '\\n'\n",
      " '    @classmethod\\n'\n",
      " '    def _skeletonize(cls, parent_node: TextNode, child_node: TextNode) -> '\n",
      " 'None:\\n'\n",
      " '        \"\"\"WARNING: In Place Operation.\"\"\"\\n'\n",
      " '        # Simple protection clauses\\n'\n",
      " '        if child_node.text not in parent_node.text:\\n'\n",
      " '            raise ValueError(\"The child text is not contained inside the '\n",
      " 'parent text.\")\\n'\n",
      " '        if child_node.node_id not in (c.node_id for c in '\n",
      " 'parent_node.child_nodes or []):\\n'\n",
      " '            raise ValueError(\"The child node is not a child of the parent '\n",
      " 'node.\")\\n'\n",
      " '\\n'\n",
      " '        # Now do the replacement\\n'\n",
      " '        replacement_text = cls._get_replacement_text(child_node=child_node)\\n'\n",
      " '        parent_node.text = parent_node.text.replace(child_node.text, '\n",
      " 'replacement_text)\\n'\n",
      " '\\n'\n",
      " '    @classmethod\\n'\n",
      " '    def _skeletonize_list(cls, nodes: List[TextNode]) -> None:\\n'\n",
      " \"        # Create a convenient map for mapping node id's to nodes\\n\"\n",
      " '        node_id_map = {n.node_id: n for n in nodes}\\n'\n",
      " '\\n'\n",
      " '        def recur(node: TextNode) -> None:\\n'\n",
      " '            # If any children exist, skeletonize ourselves, starting at the '\n",
      " 'root DFS\\n'\n",
      " '            for child in node.child_nodes or []:\\n'\n",
      " '                child_node = node_id_map[child.node_id]\\n'\n",
      " '                cls._skeletonize(parent_node=node, child_node=child_node)\\n'\n",
      " '                recur(child_node)\\n'\n",
      " '\\n'\n",
      " '        # Iterate over root nodes and recur\\n'\n",
      " '        for n in nodes:\\n'\n",
      " '            if n.parent_node is None:\\n'\n",
      " '                recur(n)\\n')\n"
     ]
    }
   ],
   "source": [
    "print(len(nodes[1].text))\n",
    "pprint(nodes[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are we to do? Well lets try splitting it. We are going to use the `CodeHierarchyNodeParser` to split the nodes into more reasonable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_nodes = CodeHierarchyNodeParser(\n",
    "    language=\"python\",\n",
    "    # You can further parameterize the CodeSplitter to split the code\n",
    "    # into \"chunks\" that match your context window size using\n",
    "    # chunck_lines and max_chars parameters, here we just use the defaults\n",
    "    code_splitter=CodeSplitter(language=\"python\"),\n",
    ").get_nodes_from_documents(nodes)\n",
    "len(split_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So that split up our data from 8 nodes into 112 nodes! Whats the max length of any of these nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1664"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(n.text) for n in split_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much shorter than before! Let's look at a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\"\"\"Simple node parser.\"\"\"\\n'\n",
      " 'from typing import Callable, List, Optional, Sequence\\n'\n",
      " '\\n'\n",
      " 'from llama_index.bridge.pydantic import Field\\n'\n",
      " 'from llama_index.callbacks.base import CallbackManager\\n'\n",
      " 'from llama_index.callbacks.schema import CBEventType, EventPayload\\n'\n",
      " 'from llama_index.node_parser.extractors.metadata_extractors import '\n",
      " 'MetadataExtractor\\n'\n",
      " 'from llama_index.node_parser.interface import NodeParser\\n'\n",
      " 'from llama_index.node_parser.node_utils import build_nodes_from_splits\\n'\n",
      " 'from llama_index.schema import BaseNode, Document\\n'\n",
      " 'from llama_index.text_splitter.utils import split_by_sentence_tokenizer\\n'\n",
      " 'from llama_index.utils import get_tqdm_iterable\\n'\n",
      " '\\n'\n",
      " 'DEFAULT_WINDOW_SIZE = 3\\n'\n",
      " 'DEFAULT_WINDOW_METADATA_KEY = \"window\"\\n'\n",
      " 'DEFAULT_OG_TEXT_METADATA_KEY = \"original_text\"\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'class SentenceWindowNodeParser(NodeParser):\\n'\n",
      " '    # Code replaced for brevity. See node_id '\n",
      " 'aa2137f3-c798-4d55-82c9-5c3e9be1c770')\n"
     ]
    }
   ],
   "source": [
    "pprint(split_nodes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without even needing a long printout we can see everything this module imported in the first document (which is at the module level) and the single class it defines. However, now instead of the class body, we see a comment: \n",
    "\n",
    "`# Code replaced for brevity. See node_id {node_id}`\n",
    "\n",
    "What if we go to that node_id?\n",
    "\n",
    "Notice below, that node_id is also in it's `child_nodes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('class SentenceWindowNodeParser(NodeParser):\\n'\n",
      " '    # Code replaced for brevity. See node_id '\n",
      " 'fe68e6de-3851-4c77-8acb-445fe45fee6c')\n"
     ]
    }
   ],
   "source": [
    "split_nodes_by_id = {n.node_id: n for n in split_nodes}\n",
    "uuid_from_text = split_nodes[0].text.splitlines()[-1].split(\" \")[-1]\n",
    "pprint(split_nodes_by_id[uuid_from_text].text)\n",
    "\n",
    "assert uuid_from_text in (n.node_id for n in split_nodes[0].child_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an artefact of the `CodeSplitter`. This must have been a big class! But lets look at it's relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='aa2137f3-c798-4d55-82c9-5c3e9be1c770', node_type=<ObjectType.TEXT: '1'>, metadata={'language': 'python', 'inclusive_scopes': [{'name': 'SentenceWindowNodeParser', 'type': 'class_definition', 'signature': 'class SentenceWindowNodeParser(NodeParser):'}], 'filepath': '../../../../llama_index/node_parser/sentence_window.py'}, hash='4278766c3216e33889069110a938de8d8586df6be121586a7eec929470c7d131'),\n",
      " <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='fe68e6de-3851-4c77-8acb-445fe45fee6c', node_type=<ObjectType.TEXT: '1'>, metadata={'language': 'python', 'inclusive_scopes': [{'name': 'SentenceWindowNodeParser', 'type': 'class_definition', 'signature': 'class SentenceWindowNodeParser(NodeParser):'}], 'filepath': '../../../../llama_index/node_parser/sentence_window.py'}, hash='29a4be710fef2a2fe594805008b2dfeafacd7cb5f19e8b10eef65802c3251128'),\n",
      " <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='1a1dd9cf-64f4-4cbf-8bf9-c6d87344fc21', node_type=<ObjectType.TEXT: '1'>, metadata={'language': 'python', 'inclusive_scopes': [], 'filepath': '../../../../llama_index/node_parser/sentence_window.py'}, hash='738a9600b3eddabb858d9ca70b2f92d9b59a88c4f45bb3659ab51ee596bd5a4b'),\n",
      " <NodeRelationship.CHILD: '5'>: [RelatedNodeInfo(node_id='f7a12758-eab1-4cfc-88df-70c3e3d9cd57', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'SentenceWindowNodeParser', 'type': 'class_definition', 'signature': 'class SentenceWindowNodeParser(NodeParser):'}, {'name': '__init__', 'type': 'function_definition', 'signature': 'def __init__(\\n        self,\\n        sentence_splitter: Optional[Callable[[str], List[str]]] = None,\\n        window_size: int = DEFAULT_WINDOW_SIZE,\\n        window_metadata_key: str = DEFAULT_WINDOW_METADATA_KEY,\\n        original_text_metadata_key: str = DEFAULT_OG_TEXT_METADATA_KEY,\\n        include_metadata: bool = True,\\n        include_prev_next_rel: bool = True,\\n        callback_manager: Optional[CallbackManager] = None,\\n        metadata_extractor: Optional[MetadataExtractor] = None,\\n    ) -> None:'}]}, hash='2cc7e93cb979cf389d47f92be0e3dae337ebd7ff6d463769a53821db80f85412'),\n",
      "                                 RelatedNodeInfo(node_id='306070f6-3762-464a-a25e-3648db2a87fa', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'SentenceWindowNodeParser', 'type': 'class_definition', 'signature': 'class SentenceWindowNodeParser(NodeParser):'}, {'name': 'text_splitter', 'type': 'function_definition', 'signature': 'def text_splitter(self) -> Callable[[str], List[str]]:'}]}, hash='bc867bd67606b314632070c946c41fa2f2b123a182ee2e89ae541e9c428464bd'),\n",
      "                                 RelatedNodeInfo(node_id='ecb65036-88bb-4008-806e-b9c629f7d038', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'SentenceWindowNodeParser', 'type': 'class_definition', 'signature': 'class SentenceWindowNodeParser(NodeParser):'}, {'name': 'from_defaults', 'type': 'function_definition', 'signature': 'def from_defaults(\\n        cls,\\n        sentence_splitter: Optional[Callable[[str], List[str]]] = None,\\n        window_size: int = DEFAULT_WINDOW_SIZE,\\n        window_metadata_key: str = DEFAULT_WINDOW_METADATA_KEY,\\n        original_text_metadata_key: str = DEFAULT_OG_TEXT_METADATA_KEY,\\n        include_metadata: bool = True,\\n        include_prev_next_rel: bool = True,\\n        callback_manager: Optional[CallbackManager] = None,\\n        metadata_extractor: Optional[MetadataExtractor] = None,\\n    ) -> \"SentenceWindowNodeParser\":'}]}, hash='6601ea2c560ee8ee2e30815275afed3937d2ca55f735075ca3c03735012fa477'),\n",
      "                                 RelatedNodeInfo(node_id='85b35f34-2e7c-43ea-add4-25704ceaf129', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'SentenceWindowNodeParser', 'type': 'class_definition', 'signature': 'class SentenceWindowNodeParser(NodeParser):'}, {'name': 'get_nodes_from_documents', 'type': 'function_definition', 'signature': 'def get_nodes_from_documents(\\n        self,\\n        documents: Sequence[Document],\\n        show_progress: bool = False,\\n    ) -> List[BaseNode]:'}]}, hash='d0ee8fec1d89a4de0624b1bb8984f34543683dc8d5c0200f0ad1a56d55f01840'),\n",
      "                                 RelatedNodeInfo(node_id='02c7f75f-3d88-4cf2-90d1-b7194a2ce7b5', node_type=<ObjectType.TEXT: '1'>, metadata={'inclusive_scopes': [{'name': 'SentenceWindowNodeParser', 'type': 'class_definition', 'signature': 'class SentenceWindowNodeParser(NodeParser):'}, {'name': 'build_window_nodes_from_documents', 'type': 'function_definition', 'signature': 'def build_window_nodes_from_documents(\\n        self, documents: Sequence[Document]\\n    ) -> List[BaseNode]:'}]}, hash='b47c5fb94581ce6682a8a4a75c800904e5a3963966050e97684e72669cbee210')]}\n"
     ]
    }
   ],
   "source": [
    "pprint(split_nodes_by_id[uuid_from_text].relationships)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that this node has a `NEXT` relationship, and many children.\n",
    "\n",
    "If we view it's `NEXT` relationship we will see things as the `CodeSplitter` sees things. Cutting up the node into chunks that are a certain character length. For more information about the `CodeSplitter` read this:\n",
    "https://docs.sweep.dev/blogs/chunking-2m-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the next node was split from this one because of a big docstring!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('# Code replaced for brevity. See node_id '\n",
      " 'aa2137f3-c798-4d55-82c9-5c3e9be1c770\\n'\n",
      " '\"\"\"Sentence window node parser.\\n'\n",
      " '\\n'\n",
      " '    Splits a document into Nodes, with each node being a sentence.\\n'\n",
      " '    Each node contains a window from the surrounding sentences in the '\n",
      " 'metadata.\\n'\n",
      " '\\n'\n",
      " '    Args:\\n'\n",
      " '        sentence_splitter (Optional[Callable]): splits text into sentences\\n'\n",
      " '        include_metadata (bool): whether to include metadata in nodes\\n'\n",
      " '        include_prev_next_rel (bool): whether to include prev/next '\n",
      " 'relationships\\n'\n",
      " '    \"\"\"\\n'\n",
      " '\\n'\n",
      " '    sentence_splitter: Callable[[str], List[str]] = Field(\\n'\n",
      " '        default_factory=split_by_sentence_tokenizer,\\n'\n",
      " '        description=\"The text splitter to use when splitting documents.\",\\n'\n",
      " '        exclude=True,\\n'\n",
      " '    )\\n'\n",
      " '    window_size: int = Field(\\n'\n",
      " '        default=DEFAULT_WINDOW_SIZE,\\n'\n",
      " '        description=\"The number of sentences on each side of a sentence to '\n",
      " 'capture.\",\\n'\n",
      " '    )\\n'\n",
      " '    window_metadata_key: str = Field(\\n'\n",
      " '        default=DEFAULT_WINDOW_METADATA_KEY,\\n'\n",
      " '        description=\"The metadata key to store the sentence window under.\",\\n'\n",
      " '    )\\n'\n",
      " '    original_text_metadata_key: str = Field(\\n'\n",
      " '        default=DEFAULT_OG_TEXT_METADATA_KEY,\\n'\n",
      " '        description=\"The metadata key to store the original sentence in.\",\\n'\n",
      " '    )\\n'\n",
      " '    include_metadata: bool = Field(\\n'\n",
      " '        default=True, description=\"Whether or not to consider metadata when '\n",
      " 'splitting.\"\\n'\n",
      " '    )\\n'\n",
      " '    include_prev_next_rel: bool = Field(\\n'\n",
      " '        default=True, description=\"Include prev/next node relationships.\"\\n'\n",
      " '    )\\n'\n",
      " '    # Code replaced for brevity. See node_id '\n",
      " 'd8a4b50c-c0d9-4338-870a-1ab058d1e3c8')\n"
     ]
    }
   ],
   "source": [
    "from llama_index.schema import NodeRelationship\n",
    "\n",
    "next_node_relationship_info = split_nodes_by_id[uuid_from_text].relationships[\n",
    "    NodeRelationship.NEXT\n",
    "]\n",
    "next_node = split_nodes_by_id[next_node_relationship_info.node_id]\n",
    "pprint(next_node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of the difference between `NodeRelationship.CHILD`/`NodeRelationship.PARENT` and `NodeRelationship.NEXT`/`NodeRelationship.PREVIOUS` as different dimensions.\n",
    "\n",
    "`CodeHierarchyNodeParser` creates `NodeRelationship.CHILD`/`NodeRelationship.PARENT` between code blocks based on scope hierarchys.\n",
    "\n",
    "Nodes which are then additionally split by `CodeSplitter` based on context length get an additional `NodeRelationship.NEXT`/`NodeRelationship.PREVIOUS`, and the first node in this chain maintains the `NodeRelationship.CHILD`/`NodeRelationship.PARENT` relationships given to it by `CodeHierarchyNodeParser`\n",
    "\n",
    "Now what if we explore that first nodes children (`NodeRelationship.CHILD`) are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('def __init__(\\n'\n",
      " '        self,\\n'\n",
      " '        sentence_splitter: Optional[Callable[[str], List[str]]] = None,\\n'\n",
      " '        window_size: int = DEFAULT_WINDOW_SIZE,\\n'\n",
      " '        window_metadata_key: str = DEFAULT_WINDOW_METADATA_KEY,\\n'\n",
      " '        original_text_metadata_key: str = DEFAULT_OG_TEXT_METADATA_KEY,\\n'\n",
      " '        include_metadata: bool = True,\\n'\n",
      " '        include_prev_next_rel: bool = True,\\n'\n",
      " '        callback_manager: Optional[CallbackManager] = None,\\n'\n",
      " '        metadata_extractor: Optional[MetadataExtractor] = None,\\n'\n",
      " '    ) -> None:\\n'\n",
      " '        \"\"\"Init params.\"\"\"\\n'\n",
      " '        callback_manager = callback_manager or CallbackManager([])\\n'\n",
      " '        sentence_splitter = sentence_splitter or '\n",
      " 'split_by_sentence_tokenizer()\\n'\n",
      " '        super().__init__(\\n'\n",
      " '            sentence_splitter=sentence_splitter,\\n'\n",
      " '            window_size=window_size,\\n'\n",
      " '            window_metadata_key=window_metadata_key,\\n'\n",
      " '            original_text_metadata_key=original_text_metadata_key,\\n'\n",
      " '            include_metadata=include_metadata,\\n'\n",
      " '            include_prev_next_rel=include_prev_next_rel,\\n'\n",
      " '            callback_manager=callback_manager,\\n'\n",
      " '            metadata_extractor=metadata_extractor,\\n'\n",
      " '        )')\n"
     ]
    }
   ],
   "source": [
    "next_node_relationship_info = split_nodes_by_id[uuid_from_text].relationships[\n",
    "    NodeRelationship.CHILD\n",
    "][0]\n",
    "next_node = split_nodes_by_id[next_node_relationship_info.node_id]\n",
    "pprint(next_node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first child of the class is the `__init__` statement! That makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indices\n",
    "\n",
    "Lets explore the use of this node parser in an index. We will be able to use any index which allows search by keyword, which should enable us to search for any node by it's uuid, or by any scope name.\n",
    "\n",
    "Lets use a keyword index to facilitate this kind of operation. We have created a CodeHierarchyKeywordTableIndex which will allow us to search for nodes by their uuid, or by their scope name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.code_hierarchy import (\n",
    "    CodeHierarchyKeywordTableIndex,\n",
    ")\n",
    "\n",
    "idx = CodeHierarchyKeywordTableIndex(\n",
    "    nodes=split_nodes,\n",
    ")\n",
    "retriever = idx.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the same code as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('class SentenceWindowNodeParser(NodeParser):\\n'\n",
      " '    # Code replaced for brevity. See node_id '\n",
      " 'fe68e6de-3851-4c77-8acb-445fe45fee6c')\n"
     ]
    }
   ],
   "source": [
    "pprint(retriever.retrieve(uuid_from_text)[0].node.get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what about getting the rest of the code for this scope?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# Code replaced for brevity. See node_id '\n",
      " 'd8a4b50c-c0d9-4338-870a-1ab058d1e3c8\\n'\n",
      " '@classmethod\\n'\n",
      " '    def from_defaults(\\n'\n",
      " '        cls,\\n'\n",
      " '        sentence_splitter: Optional[Callable[[str], List[str]]] = None,\\n'\n",
      " '        window_size: int = DEFAULT_WINDOW_SIZE,\\n'\n",
      " '        window_metadata_key: str = DEFAULT_WINDOW_METADATA_KEY,\\n'\n",
      " '        original_text_metadata_key: str = DEFAULT_OG_TEXT_METADATA_KEY,\\n'\n",
      " '        include_metadata: bool = True,\\n'\n",
      " '        include_prev_next_rel: bool = True,\\n'\n",
      " '        callback_manager: Optional[CallbackManager] = None,\\n'\n",
      " '        metadata_extractor: Optional[MetadataExtractor] = None,\\n'\n",
      " '    ) -> \"SentenceWindowNodeParser\":\\n'\n",
      " '        # Code replaced for brevity. See node_id '\n",
      " 'ecb65036-88bb-4008-806e-b9c629f7d038\\n'\n",
      " '\\n'\n",
      " '    def get_nodes_from_documents(\\n'\n",
      " '        self,\\n'\n",
      " '        documents: Sequence[Document],\\n'\n",
      " '        show_progress: bool = False,\\n'\n",
      " '    ) -> List[BaseNode]:\\n'\n",
      " '        # Code replaced for brevity. See node_id '\n",
      " '85b35f34-2e7c-43ea-add4-25704ceaf129\\n'\n",
      " '\\n'\n",
      " '    def build_window_nodes_from_documents(\\n'\n",
      " '        self, documents: Sequence[Document]\\n'\n",
      " '    ) -> List[BaseNode]:\\n'\n",
      " '        # Code replaced for brevity. See node_id '\n",
      " '02c7f75f-3d88-4cf2-90d1-b7194a2ce7b5',\n",
      " 'class SentenceWindowNodeParser(NodeParser):\\n'\n",
      " '    # Code replaced for brevity. See node_id '\n",
      " 'fe68e6de-3851-4c77-8acb-445fe45fee6c',\n",
      " '# Code replaced for brevity. See node_id '\n",
      " 'aa2137f3-c798-4d55-82c9-5c3e9be1c770\\n'\n",
      " '\"\"\"Sentence window node parser.\\n'\n",
      " '\\n'\n",
      " '    Splits a document into Nodes, with each node being a sentence.\\n'\n",
      " '    Each node contains a window from the surrounding sentences in the '\n",
      " 'metadata.\\n'\n",
      " '\\n'\n",
      " '    Args:\\n'\n",
      " '        sentence_splitter (Optional[Callable]): splits text into sentences\\n'\n",
      " '        include_metadata (bool): whether to include metadata in nodes\\n'\n",
      " '        include_prev_next_rel (bool): whether to include prev/next '\n",
      " 'relationships\\n'\n",
      " '    \"\"\"\\n'\n",
      " '\\n'\n",
      " '    sentence_splitter: Callable[[str], List[str]] = Field(\\n'\n",
      " '        default_factory=split_by_sentence_tokenizer,\\n'\n",
      " '        description=\"The text splitter to use when splitting documents.\",\\n'\n",
      " '        exclude=True,\\n'\n",
      " '    )\\n'\n",
      " '    window_size: int = Field(\\n'\n",
      " '        default=DEFAULT_WINDOW_SIZE,\\n'\n",
      " '        description=\"The number of sentences on each side of a sentence to '\n",
      " 'capture.\",\\n'\n",
      " '    )\\n'\n",
      " '    window_metadata_key: str = Field(\\n'\n",
      " '        default=DEFAULT_WINDOW_METADATA_KEY,\\n'\n",
      " '        description=\"The metadata key to store the sentence window under.\",\\n'\n",
      " '    )\\n'\n",
      " '    original_text_metadata_key: str = Field(\\n'\n",
      " '        default=DEFAULT_OG_TEXT_METADATA_KEY,\\n'\n",
      " '        description=\"The metadata key to store the original sentence in.\",\\n'\n",
      " '    )\\n'\n",
      " '    include_metadata: bool = Field(\\n'\n",
      " '        default=True, description=\"Whether or not to consider metadata when '\n",
      " 'splitting.\"\\n'\n",
      " '    )\\n'\n",
      " '    include_prev_next_rel: bool = Field(\\n'\n",
      " '        default=True, description=\"Include prev/next node relationships.\"\\n'\n",
      " '    )\\n'\n",
      " '    # Code replaced for brevity. See node_id '\n",
      " 'd8a4b50c-c0d9-4338-870a-1ab058d1e3c8',\n",
      " '# Code replaced for brevity. See node_id '\n",
      " 'fe68e6de-3851-4c77-8acb-445fe45fee6c\\n'\n",
      " 'metadata_extractor: Optional[MetadataExtractor] = Field(\\n'\n",
      " '        default=None, description=\"Metadata extraction pipeline to apply to '\n",
      " 'nodes.\"\\n'\n",
      " '    )\\n'\n",
      " '    callback_manager: CallbackManager = Field(\\n'\n",
      " '        default_factory=CallbackManager, exclude=True\\n'\n",
      " '    )\\n'\n",
      " '\\n'\n",
      " '    def __init__(\\n'\n",
      " '        self,\\n'\n",
      " '        sentence_splitter: Optional[Callable[[str], List[str]]] = None,\\n'\n",
      " '        window_size: int = DEFAULT_WINDOW_SIZE,\\n'\n",
      " '        window_metadata_key: str = DEFAULT_WINDOW_METADATA_KEY,\\n'\n",
      " '        original_text_metadata_key: str = DEFAULT_OG_TEXT_METADATA_KEY,\\n'\n",
      " '        include_metadata: bool = True,\\n'\n",
      " '        include_prev_next_rel: bool = True,\\n'\n",
      " '        callback_manager: Optional[CallbackManager] = None,\\n'\n",
      " '        metadata_extractor: Optional[MetadataExtractor] = None,\\n'\n",
      " '    ) -> None:\\n'\n",
      " '        # Code replaced for brevity. See node_id '\n",
      " 'f7a12758-eab1-4cfc-88df-70c3e3d9cd57\\n'\n",
      " '\\n'\n",
      " '    @classmethod\\n'\n",
      " '    def class_name(cls) -> str:\\n'\n",
      " '        return \"SentenceWindowNodeParser\"\\n'\n",
      " '\\n'\n",
      " '    @property\\n'\n",
      " '    def text_splitter(self) -> Callable[[str], List[str]]:\\n'\n",
      " '        # Code replaced for brevity. See node_id '\n",
      " '306070f6-3762-464a-a25e-3648db2a87fa\\n'\n",
      " '    # Code replaced for brevity. See node_id '\n",
      " 'cf88a5bb-8bcd-46c2-9169-4f6e82b12b78']\n"
     ]
    }
   ],
   "source": [
    "pprint(\n",
    "    [\n",
    "        n.node.get_content()\n",
    "        for n in retriever.retrieve(\"SentenceWindowNodeParser\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difficulty is that these are out of order. The CodeSplitter controls how much overlap there is for each of these documents, and how big they are. You can play with its settings to disambiguate any confusion. However, they do have their uuids for their splits in the text themselves, so an LLM should be able to recursively search these documents to put them in some kind of order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Hierarchy\n",
    "\n",
    "The namesake of this node parser, creates a tree of scope names to use to search the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ..\n",
      "  - ..\n",
      "    - ..\n",
      "      - ..\n",
      "        - llama_index\n",
      "          - node_parser\n",
      "            - sentence_window.py\n",
      "              - SentenceWindowNodeParser\n",
      "                - __init__\n",
      "                - text_splitter\n",
      "                - from_defaults\n",
      "                - get_nodes_from_documents\n",
      "                - build_window_nodes_from_documents\n",
      "            - code_hierarchy.py\n",
      "              - _SignatureCaptureType\n",
      "              - _SignatureCaptureOptions\n",
      "              - _ScopeMethod\n",
      "              - _CommentOptions\n",
      "              - _ScopeItem\n",
      "              - _ChunkNodeOutput\n",
      "              - CodeHierarchyNodeParser\n",
      "                - class_name\n",
      "                - __init__\n",
      "                - _get_node_name\n",
      "                  - recur\n",
      "                - _get_node_signature\n",
      "                  - find_start\n",
      "                  - find_end\n",
      "                - _chunk_node\n",
      "                - get_code_hierarchy_from_nodes\n",
      "                  - get_subdict\n",
      "                  - recur_inclusive_scope\n",
      "                  - dict_to_markdown\n",
      "                - get_nodes_from_documents\n",
      "                - _get_indentation\n",
      "                - _get_comment_text\n",
      "                - _create_comment_line\n",
      "                - _get_replacement_text\n",
      "                - _skeletonize\n",
      "                - _skeletonize_list\n",
      "                  - recur\n",
      "            - loading.py\n",
      "              - load_parser\n",
      "            - node_utils.py\n",
      "              - build_nodes_from_splits\n",
      "              - get_nodes_from_document\n",
      "              - get_nodes_from_node\n",
      "            - unstructured_element.py\n",
      "              - TableColumnOutput\n",
      "                - __str__\n",
      "              - TableOutput\n",
      "              - Element\n",
      "              - html_to_df\n",
      "              - filter_table\n",
      "              - extract_elements\n",
      "              - extract_table_summaries\n",
      "              - get_table_elements\n",
      "              - get_text_elements\n",
      "              - _get_nodes_from_buffer\n",
      "              - get_nodes_from_elements\n",
      "              - UnstructuredElementNodeParser\n",
      "                - __init__\n",
      "                - from_defaults\n",
      "                - get_nodes_from_node\n",
      "                - get_nodes_from_documents\n",
      "                - get_base_nodes_and_mappings\n",
      "            - hierarchical.py\n",
      "              - _add_parent_child_relationship\n",
      "              - get_leaf_nodes\n",
      "              - get_root_nodes\n",
      "              - HierarchicalNodeParser\n",
      "                - from_defaults\n",
      "                - _recursively_get_nodes_from_nodes\n",
      "                - get_nodes_from_documents\n",
      "            - simple_file.py\n",
      "              - SimpleFileNodeParser\n",
      "                - from_defaults\n",
      "                - class_name\n",
      "                - get_nodes_from_documents\n",
      "            - interface.py\n",
      "              - NodeParser\n",
      "                - get_nodes_from_documents\n",
      "              - BaseExtractor\n",
      "                - extract\n",
      "            - simple.py\n",
      "              - SimpleNodeParser\n",
      "                - from_defaults\n",
      "                - get_nodes_from_documents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(CodeHierarchyNodeParser.get_code_hierarchy_from_nodes(split_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Improvements\n",
    "\n",
    "* Creating a scope hierarchy for the context of the llm\n",
    "* More languages\n",
    "\n",
    "Feel free to contribute!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
