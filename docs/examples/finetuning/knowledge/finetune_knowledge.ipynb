{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aff20d1-c522-4e9f-9ed8-3d70c4788601",
   "metadata": {},
   "source": [
    "# Fine-tuning to Memorize Knowledge\n",
    "\n",
    "In this tutorial we experiment with some basic approaches of \"baking in knowledge with fine-tuning.\"\n",
    "\n",
    "- Synthesizing questions from existing context\n",
    "- Trying text completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c4993c-d527-4840-8182-4bfe03bbdb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3492e87-6453-4ad1-b066-57f9d4b5fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6541f7-eeab-4e43-bb53-69bd7ffddec8",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a936769a-4ec7-4c1e-b782-9973fd0c0236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data && wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8432f89c-8fbc-4afc-98c3-ac49d6adb176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_hub.file.pdf.base import PDFReader\n",
    "from llama_hub.file.unstructured.base import UnstructuredReader\n",
    "from llama_hub.file.pymu_pdf.base import PyMuPDFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12efa907-2cf3-46f2-b66f-79f93737c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFReader()\n",
    "docs0 = loader.load(file_path=Path(\"./data/llama2.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "914ef39a-8f5b-4cc9-af62-0bef2c6689d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "doc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\n",
    "metadata = {\"paper_title\": \"Llama 2: Open Foundation and Fine-Tuned Chat Models\"}\n",
    "docs = [Document(text=doc_text, metadata=metadata)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d458d-71b0-4c1d-bddc-bc2ada565c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1663f6d2-ba1b-4184-89c8-1e73cfc6c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.callbacks import CallbackManager\n",
    "\n",
    "callback_manager = CallbackManager([])\n",
    "\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo-0613\", temperature=0.3),\n",
    "    callback_manager=callback_manager,\n",
    ")\n",
    "gpt_4_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-4-0613\", temperature=0.3), callback_manager=callback_manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ffa721-ea43-4a82-bfe3-ad7a159f3b6e",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4d50c8e-6059-4b52-83a0-733e494c8e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import DatasetGenerator\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "# try evaluation modules\n",
    "from llama_index.evaluation import QueryResponseEvaluator, ResponseEvaluator\n",
    "from llama_index import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3d1b721-ce41-41af-aed9-630d1be5a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SimpleNodeParser.from_defaults()\n",
    "nodes = node_parser.get_nodes_from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c92bd0-fb8a-404f-9966-a3fbb7a9b016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "num_questions_per_chunk = 10\n",
    "question_gen_query = (\n",
    "    \"You are a Teacher/ Professor. Your task is to setup \"\n",
    "    \"a quiz/examination. Using the provided context, \"\n",
    "    f\"formulate {num_questions_per_chunk} that captures an important fact from the \"\n",
    "    \"context. \\n\"\n",
    "    \"You MUST obey the following criteria:\\n\"\n",
    "    \"- Restrict the question to the context information provided.\\n\"\n",
    "    \"- Do NOT create a question that cannot be answered from the context.\\n\"\n",
    "    \"- Phrase the question so that it does NOT refer to specific context. \"\n",
    "    'For instance, do NOT put phrases like \"given provided context\" or \"in this work\" in the question, '\n",
    "    \"because if the question is asked elsewhere it wouldn't be provided specific context. Replace these terms \"\n",
    "    \"with specific details.\\n\"\n",
    "    \"BAD questions:\\n\"\n",
    "    \"What did the author do in his childhood\\n\"\n",
    "    \"What were the main findings in this report\\n\\n\"\n",
    "    \"GOOD questions:\\n\"\n",
    "    \"What did Barack Obama do in his childhood\\n\"\n",
    "    \"What were the main findings in the original Transformers paper by Vaswani et al.\\n\\n\"\n",
    "    \"Generate the questions below:\\n\"\n",
    ")\n",
    "\n",
    "# go through each node one at a time -\n",
    "# generate questions, filter using eval modules, and dump to file\n",
    "\n",
    "fp = open(\"data/qa_pairs.jsonl\", \"w\")\n",
    "for idx, node in enumerate(nodes):\n",
    "    dataset_generator = DatasetGenerator(\n",
    "        [node],\n",
    "        question_gen_query=question_gen_query,\n",
    "        service_context=gpt_4_context,\n",
    "        metadata_mode=\"all\",\n",
    "    )\n",
    "    node_questions_0 = dataset_generator.generate_questions_from_nodes(num=10)\n",
    "    print(f\"[Node {idx}] Generated questions:\\n {node_questions_0}\")\n",
    "    # for each question, get a response\n",
    "    for question in tqdm(node_questions_0):\n",
    "        index = SummaryIndex([node], service_context=gpt_35_context)\n",
    "        query_engine = index.as_query_engine()\n",
    "        response = query_engine.query(question)\n",
    "        out_dict = {\"query\": question, \"response\": str(response)}\n",
    "        print(f\"[Node {idx}] Outputs: {out_dict}\")\n",
    "        fp.write(json.dumps(out_dict) + \"\\n\")\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2064aa81-cc4a-413d-ac07-b410c0f8a721",
   "metadata": {},
   "source": [
    "### Filter out questions using QueryResponseEvaluator\n",
    "\n",
    "Do a second pass to make sure only questions that can be answerd by context make it into the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b725d957-329d-4c3f-b253-b1568934d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try evaluation modules\n",
    "from llama_index.evaluation import QueryResponseEvaluator, ResponseEvaluator\n",
    "from llama_index import PromptTemplate\n",
    "from llama_index.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c29c938-f332-4189-9bb9-b359b8a03c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_eval_tmpl = PromptTemplate(\n",
    "    \"Your task is to evaluate the following: If the response for the query isn't able to answer the question provided.\\n\"\n",
    "    \"If query isn't able to answer the question, answer NO.\\n\"\n",
    "    \"Otherwise answer YES.\\n\"\n",
    "    \"To elaborate, you might get an answer like the following: 'The context does not contain the answer to this question.'\"\n",
    "    \"Please return NO in that case. \"\n",
    "    \"You be given the query and response. Return YES or NO as the answer.\\n\"\n",
    "    \"Query: \\n {query_str}\\n\"\n",
    "    \"Response: \\n {response_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "eval_llm = OpenAI(model=\"gpt-4-0613\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55da3785-0ec2-422b-b5a3-b31c1eff2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(path: str, out_path: str):\n",
    "    fp = open(path, \"r\")\n",
    "    out_fp = open(out_path, \"w\")\n",
    "    new_lines = []\n",
    "    for idx, line in enumerate(fp):\n",
    "        qa_pair = json.loads(line)\n",
    "        eval = eval_llm.complete(\n",
    "            query_eval_tmpl.format(\n",
    "                query_str=qa_pair[\"query\"], response_str=qa_pair[\"response\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(f\"[{idx}] QA Pair: {qa_pair} \\n Eval: {eval}\")\n",
    "        if \"NO\" in eval:\n",
    "            continue\n",
    "        else:\n",
    "            # new_lines.append(line)\n",
    "            out_fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d91a70e-9320-4ac8-b029-f74643a05dff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filter_data(\"data/qa_pairs.jsonl\", \"data/qa_pairs_2.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f40ad6-2824-4242-a50c-e595f4b4e368",
   "metadata": {},
   "source": [
    "### Split into Training and Validation Sets\n",
    "\n",
    "We split into training and validation sets.\n",
    "\n",
    "**NOTE**: We shuffle the data before splitting. This helps ensure that the training data has coverage throughout the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "813574f3-db3a-4ed6-89e1-fac1a6be9194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "\n",
    "def split_train_val(path: str, out_train_path: str, out_val_path: str, train_split=0.7):\n",
    "    with open(path, \"r\") as fp:\n",
    "        lines = fp.readlines()\n",
    "\n",
    "        # shuffle the lines to make sure that the \"train questions\" cover most fo the context\n",
    "        shuffled_lines = deepcopy(lines)\n",
    "        random.shuffle(shuffled_lines)\n",
    "\n",
    "        split_idx = int(train_split * len(shuffled_lines))\n",
    "        train_lines = shuffled_lines[:split_idx]\n",
    "        val_lines = shuffled_lines[split_idx:]\n",
    "        with open(out_train_path, \"w\") as out_fp:\n",
    "            out_fp.write(\"\".join(train_lines))\n",
    "\n",
    "        with open(out_val_path, \"w\") as out_fp:\n",
    "            out_fp.write(\"\".join(val_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "094e9373-717f-4456-a4fd-f13582181bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train_val(\n",
    "    \"data/qa_pairs_2.jsonl\", \"data/qa_pairs_train.jsonl\", \"data/qa_pairs_val.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f44ccf-c334-42ae-939c-c64cc4a164e3",
   "metadata": {},
   "source": [
    "### Format into Training Data\n",
    "\n",
    "Format into training data for OpenAI's finetuning endpoints.\n",
    "\n",
    "**NOTE**: We don't use our `OpenAIFinetuningHandler` because that logs the full input prompt including context as the user message. Here we just want to log the query as the user message, because we want to fine-tune gpt-3.5-turbo to \"bake in knowledge\" into the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7baa2821-71de-4887-b9ed-53f474d41118",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"data/qa_pairs_train.jsonl\", \"r\")\n",
    "out_fp = open(\"data/qa_pairs_openai.jsonl\", \"w\")\n",
    "# TODO: try with different system prompts\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful assistant helping to answer questions about the Llama 2 paper.\",\n",
    "}\n",
    "for line in fp:\n",
    "    qa_pair = json.loads(line)\n",
    "    user_prompt = {\"role\": \"user\", \"content\": qa_pair[\"query\"]}\n",
    "    assistant_prompt = {\"role\": \"assistant\", \"content\": qa_pair[\"response\"]}\n",
    "    out_dict = {\n",
    "        \"messages\": [system_prompt, user_prompt, assistant_prompt],\n",
    "    }\n",
    "    out_fp.write(json.dumps(out_dict) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06875df5-9e53-4f2e-a300-0d16881adfde",
   "metadata": {},
   "source": [
    "## Fine-tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0268eeb4-1a72-47b0-8fa8-4dd707edec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning import OpenAIFinetuneEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b04dc433-ca15-4076-a71c-e35df18ff0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_engine = OpenAIFinetuneEngine(\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"data/qa_pairs_openai.jsonl\",\n",
    "    # start_job_id=\"<start-job-id>\"  # if you have an existing job, can specify id here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b36f698f-591c-44c9-9e16-9328102daef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 597\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a helpful assistant helping to answer questions about the Llama 2 paper.'}\n",
      "{'role': 'user', 'content': 'Who were the early reviewers of the paper on \"Llama 2: Open Foundation and Fine-Tuned Chat Models\" who helped improve its quality?'}\n",
      "{'role': 'assistant', 'content': 'Mike Lewis, Joelle Pineau, Laurens van der Maaten, Jason Weston, and Omer Levy were the early reviewers of the paper on \"Llama 2: Open Foundation and Fine-Tuned Chat Models\" who helped improve its quality.'}\n",
      "No errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 50, 637\n",
      "mean / median: 102.51256281407035, 90.0\n",
      "p5 / p95: 66.0, 155.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 2, 588\n",
      "mean / median: 50.45728643216081, 35.0\n",
      "p5 / p95: 18.0, 102.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~61200 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~183600 tokens\n",
      "As of Augest 22, 2023, fine-tuning gpt-3.5-turbo is $0.008 / 1K Tokens.\n",
      "This means your total cost for training will be $0.48960000000000004 per epoch.\n",
      "Waiting for file to be ready...\n"
     ]
    }
   ],
   "source": [
    "finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b332bc43-304c-4883-ad33-b0c1610e5572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-fk0428lntJCRh6x1GKeccv8E at 0x2b95fd6c0> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-fk0428lntJCRh6x1GKeccv8E\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1694406904,\n",
       "  \"finished_at\": 1694409009,\n",
       "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:llamaindex::7xTTW0hT\",\n",
       "  \"organization_id\": \"org-1ZDAvajC6v2ZtAP9hLEIsXRz\",\n",
       "  \"result_files\": [\n",
       "    \"file-Ao1r7cGnYJbHqCG79zAQo6lP\"\n",
       "  ],\n",
       "  \"status\": \"succeeded\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-9ndBjJX0pZ3Do4mPhADcTOef\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 3\n",
       "  },\n",
       "  \"trained_tokens\": 180006,\n",
       "  \"error\": null\n",
       "}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_engine.get_current_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "667218db-a36c-42c8-aea5-33ad08700215",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = finetune_engine.get_finetuned_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "71325911-37f6-4be3-8a18-4ff2cda83cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x2bdaba2f0>, model='ft:gpt-3.5-turbo-0613:llamaindex::7xTTW0hT', temperature=0.1, max_tokens=None, additional_kwargs={}, max_retries=10, class_type='openai')"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdde395f-7a26-48f1-8c7d-038858866313",
   "metadata": {},
   "source": [
    "## Evaluate Results\n",
    "\n",
    "We run evaluations, over both the validation set but also the training set.\n",
    "\n",
    "**Wait, isn't evaluating over the training set cheating?**\n",
    "\n",
    "- It's a sanity check of how much the model was able to memorize information it's trained on.\n",
    "- The training data contains quite a bit of content about the paper, so by answering the training set well the model would at least be well-equipped to answer some questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "12acc83f-c006-4189-953c-5a482a53d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8bd3b708-5b5e-4624-94e9-4cb6a8489abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str):\n",
    "    fp = open(path, \"r\")\n",
    "    data_dicts = []\n",
    "    for line in fp:\n",
    "        d = json.loads(line)\n",
    "        data_dicts.append(d)\n",
    "    return data_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "2dd58d18-37be-4a1e-8a75-bb49ec834f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dicts = load_data(\"data/qa_pairs_train.jsonl\")\n",
    "eval_dicts = load_data(\"data/qa_pairs_val.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "be7cd29c-9797-4376-84b5-52dceec90164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model(model, d):\n",
    "    # print(d)\n",
    "    msgs = [\n",
    "        ChatMessage(\n",
    "            role=\"system\",\n",
    "            content=\"You are a helpful assistant helping to answer questions about the Llama 2 paper.\",\n",
    "        ),\n",
    "        ChatMessage(role=\"user\", content=d[\"query\"]),\n",
    "    ]\n",
    "\n",
    "    # try ft-model\n",
    "    response = model.chat(msgs)\n",
    "    return str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "33b52edd-08f8-4e3f-bd7c-f684b2ddafc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the title of the paper discussed in the context?', 'response': 'The title of the paper discussed in the context is \"Llama 2: Open Foundation and Fine-Tuned Chat Models\".'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'assistant: The title of the paper discussed in the context is \"Llama 2: Open Foundation and Fine-Tuned Chat Models\".'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = query_model(ft_model, eval_dicts[7])\n",
    "print(eval_dicts[7])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "6a391ff4-fd40-4e09-96c7-df862e6c9b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How is the decision made whether to use safety context distillation or not?', 'response': 'The decision to use safety context distillation is made based on the reward model score. The safety reward model is leveraged to determine whether the context-distilled output receives a better reward model score than the original answer. If the context-distilled output gets a better reward model score, it is kept. This approach helps limit the negative impact of context distillation while still utilizing it in cases where it improves the reward model score.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'assistant: The decision to use safety context distillation is made based on the reward model score. If the reward model score is below a certain threshold, safety context distillation is used.'"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_model(ft_model, train_dicts[7])\n",
    "print(train_dicts[7])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee06dd2-17a8-4da7-af59-55b0925aef12",
   "metadata": {},
   "source": [
    "### Setup Baseline RAG system to benchmark\n",
    "\n",
    "We setup a baseline RAG system powered by gpt-3.5-turbo to help benchmark the quality of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a1cc741e-5061-4564-b76d-86a1d3afa60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline RAG system\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "base_index = VectorStoreIndex(nodes, service_context=gpt_35_context)\n",
    "base_query_engine = base_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "4220a3af-fc07-4aa0-bd27-ce1786122b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "base_model = OpenAI(model=\"gpt-4\", temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "6544fe86-eb1c-49f4-9c23-db034891712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How does Llama 2-Chat address the issue of spreading misinformation or conspiracy theories?', 'response': \"Llama 2-Chat addresses the issue of spreading misinformation or conspiracy theories by refuting any misinformation in the prompt immediately. It emphasizes the importance of relying on scientific evidence and credible sources when evaluating historical events. The model does not promote or encourage the spread of false information and instead focuses on sharing accurate and helpful information. It also highlights the importance of fact-checking and critical thinking when assessing the validity of a claim. Llama 2-Chat's programming rules prioritize respect for truth and accuracy in all forms of communication and discourage the spread of misinformation or conspiracy theories.\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"assistant: The Llama 2 paper does not specifically address the issue of spreading misinformation or conspiracy theories. However, it does mention that the model is designed to refuse outputs that are inappropriate or harmful. This could potentially include misinformation or conspiracy theories. It also states that the model's responses are based on a mixture of licensed data, data created by human trainers, and publicly available data. The developers have also used reinforcement learning from human feedback to fine-tune the model, which can help in reducing the spread of false information. However, the specifics of how misinformation or conspiracy theories are handled are not detailed in the paper.\""
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_model(base_model, eval_dicts[80])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdec472e-4e63-46f7-9f7b-3156da34ad38",
   "metadata": {},
   "source": [
    "### Run Evaluations\n",
    "\n",
    "We log the responses from the fine-tuned model, the baseline RAG system, and the baseline model.\n",
    "\n",
    "We then run all responses through a GPT-4 prompt, comparing each against the ground-truth to measure validity of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "71c86f72-bf55-4edf-a911-0935c4a57a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "EVAL_PROMPT_TMPL = PromptTemplate(\n",
    "    \"\"\"\\\n",
    "We provide a question and the 'ground-truth' answer. We also provide \\\n",
    "the predicted answer.\n",
    "\n",
    "Evaluate whether the predicted answer is correct, given its similarity \\\n",
    "to the ground-truth. If details provided in predicted answer are reflected \\\n",
    "in the ground-truth answer, return \"YES\". To return \"YES\", the details don't \\\n",
    "need to exactly match. Be lenient in evaluation if the predicted answer \\\n",
    "is missing a few details. Try to make sure that there are no blatant mistakes. \\\n",
    "Otherwise, return \"NO\".\n",
    "\n",
    "Question: {question}\n",
    "Ground-truth Answer: {gt_answer}\n",
    "Predicted Answer: {pred_answer}\n",
    "Evaluation Result: \\\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def eval_match_gt(query, gt_response, pred_response):\n",
    "    llm = OpenAI(model=\"gpt-4\", temperature=0.0)\n",
    "    fmt_prompt = EVAL_PROMPT_TMPL.format(\n",
    "        question=query,\n",
    "        gt_answer=gt_response,\n",
    "        pred_answer=pred_response,\n",
    "    )\n",
    "    result = llm.complete(fmt_prompt)\n",
    "    if \"yes\" in str(result).lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def run_evals(eval_dicts):\n",
    "    \"\"\"Run evals - fine-tuned model, RAG system, and base model.\"\"\"\n",
    "\n",
    "    raw_responses = []\n",
    "    for eval_dict in tqdm(eval_dicts):\n",
    "        gt_response = eval_dict[\"response\"]\n",
    "        ft_response = str(query_model(ft_model, eval_dict))\n",
    "        rag_response = str(base_query_engine.query(eval_dict[\"query\"]))\n",
    "        base_response = str(query_model(base_model, eval_dict))\n",
    "\n",
    "        # try evaluations\n",
    "        ft_eval = eval_match_gt(eval_dict[\"query\"], gt_response, ft_response)\n",
    "        rag_eval = eval_match_gt(eval_dict[\"query\"], gt_response, rag_response)\n",
    "        base_eval = eval_match_gt(eval_dict[\"query\"], gt_response, base_response)\n",
    "\n",
    "        response_dict = {\n",
    "            \"query\": eval_dict[\"query\"],\n",
    "            \"gt_response\": gt_response,\n",
    "            \"ft_response\": ft_response,\n",
    "            \"rag_response\": rag_response,\n",
    "            \"base_response\": base_response,\n",
    "            \"ft_eval\": ft_eval,\n",
    "            \"rag_eval\": rag_eval,\n",
    "            \"base_eval\": base_eval,\n",
    "        }\n",
    "\n",
    "        raw_responses.append(response_dict)\n",
    "\n",
    "    raw_responses_df = pd.DataFrame(raw_responses)\n",
    "\n",
    "    eval_dict = {\n",
    "        \"ft_score\": raw_responses_df[\"ft_eval\"].mean(),\n",
    "        \"rag_score\": raw_responses_df[\"rag_eval\"].mean(),\n",
    "        \"base_score\": raw_responses_df[\"base_eval\"].mean(),\n",
    "    }\n",
    "\n",
    "    return eval_dict, raw_responses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "76713b7d-1a31-4f65-a324-534f58b70269",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd60d84-cc87-492a-9644-c001058bd1f2",
   "metadata": {},
   "source": [
    "#### Qualitative Evaluations\n",
    "\n",
    "Here we show some qualitative output examples over both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "0076956d-338e-4f22-a10f-253774e5b41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ft_score': 1.0, 'rag_score': 1.0, 'base_score': 0.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>gt_response</th>\n",
       "      <th>ft_response</th>\n",
       "      <th>rag_response</th>\n",
       "      <th>base_response</th>\n",
       "      <th>ft_eval</th>\n",
       "      <th>rag_eval</th>\n",
       "      <th>base_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is the decision made whether to use safety context distillation or not?</td>\n",
       "      <td>The decision to use safety context distillation is made based on the reward model score. The safety reward model is leveraged to determine whether the context-distilled output receives a better reward model score than the original answer. If the context-distilled output gets a better reward model score, it is kept. This approach helps limit the negative impact of context distillation while still utilizing it in cases where it improves the reward model score.</td>\n",
       "      <td>assistant: The decision to use safety context distillation is made based on the reward model score. If the reward model score is above a certain threshold, safety context distillation is used.</td>\n",
       "      <td>The decision to use safety context distillation is made based on the reward model score. The safety reward model is used to evaluate whether the context-distilled output gets a better reward model score than the original answer. If the context-distilled output receives a better reward model score, it is kept. This approach helps limit the negative impact of context distillation while still improving the safety of the model's responses.</td>\n",
       "      <td>assistant: The Llama 2 paper does not provide specific criteria for deciding when to use safety context distillation. The choice to use this method would likely depend on the specific requirements of the task and the potential risks involved. Safety context distillation is used to ensure that the model behaves safely even in situations that were not covered in the training data. If the task involves high-risk decisions or is in a domain where unexpected situations are likely to occur, it might be more important to use safety context distillation. However, this would likely be a decision made on a case-by-case basis, considering factors such as the complexity of the task, the quality and diversity of the training data, and the potential consequences of unsafe behavior.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         query  \\\n",
       "0  How is the decision made whether to use safety context distillation or not?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                      gt_response  \\\n",
       "0  The decision to use safety context distillation is made based on the reward model score. The safety reward model is leveraged to determine whether the context-distilled output receives a better reward model score than the original answer. If the context-distilled output gets a better reward model score, it is kept. This approach helps limit the negative impact of context distillation while still utilizing it in cases where it improves the reward model score.   \n",
       "\n",
       "                                                                                                                                                                                        ft_response  \\\n",
       "0  assistant: The decision to use safety context distillation is made based on the reward model score. If the reward model score is above a certain threshold, safety context distillation is used.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                              rag_response  \\\n",
       "0  The decision to use safety context distillation is made based on the reward model score. The safety reward model is used to evaluate whether the context-distilled output gets a better reward model score than the original answer. If the context-distilled output receives a better reward model score, it is kept. This approach helps limit the negative impact of context distillation while still improving the safety of the model's responses.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                base_response  \\\n",
       "0  assistant: The Llama 2 paper does not provide specific criteria for deciding when to use safety context distillation. The choice to use this method would likely depend on the specific requirements of the task and the potential risks involved. Safety context distillation is used to ensure that the model behaves safely even in situations that were not covered in the training data. If the task involves high-risk decisions or is in a domain where unexpected situations are likely to occur, it might be more important to use safety context distillation. However, this would likely be a decision made on a case-by-case basis, considering factors such as the complexity of the task, the quality and diversity of the training data, and the potential consequences of unsafe behavior.   \n",
       "\n",
       "   ft_eval  rag_eval  base_eval  \n",
       "0        1         1          0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_dict, raw_response_df = run_evals(train_dicts[7:8])\n",
    "display(eval_dict)\n",
    "display(raw_response_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "edc4e854-f8e0-4e58-93ae-a3d6c3d8c0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ft_score': 0.0, 'rag_score': 1.0, 'base_score': 0.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>gt_response</th>\n",
       "      <th>ft_response</th>\n",
       "      <th>rag_response</th>\n",
       "      <th>base_response</th>\n",
       "      <th>ft_eval</th>\n",
       "      <th>rag_eval</th>\n",
       "      <th>base_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What model is used to predict the truthfulness and informativeness of the generated outputs from LLMs?</td>\n",
       "      <td>A fine-tuned GPT-3 model, referred to as \"GPT-judge,\" is used to predict the truthfulness and informativeness of the generated outputs from LLMs.</td>\n",
       "      <td>assistant: The model used to predict the truthfulness and informativeness of the generated outputs from LLMs is called TruthfulQA.</td>\n",
       "      <td>A fine-tuned GPT-3 model, referred to as \"GPT-judge,\" is used to predict the truthfulness and informativeness of the generated outputs from LLMs.</td>\n",
       "      <td>assistant: The Llama 2 paper does not specify a particular model used to predict the truthfulness and informativeness of the generated outputs from LLMs (Language Model). The paper primarily focuses on the limitations and potential risks of large language models. If you're referring to a different paper or model, please provide more details.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                    query  \\\n",
       "0  What model is used to predict the truthfulness and informativeness of the generated outputs from LLMs?   \n",
       "\n",
       "                                                                                                                                         gt_response  \\\n",
       "0  A fine-tuned GPT-3 model, referred to as \"GPT-judge,\" is used to predict the truthfulness and informativeness of the generated outputs from LLMs.   \n",
       "\n",
       "                                                                                                                          ft_response  \\\n",
       "0  assistant: The model used to predict the truthfulness and informativeness of the generated outputs from LLMs is called TruthfulQA.   \n",
       "\n",
       "                                                                                                                                        rag_response  \\\n",
       "0  A fine-tuned GPT-3 model, referred to as \"GPT-judge,\" is used to predict the truthfulness and informativeness of the generated outputs from LLMs.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                             base_response  \\\n",
       "0  assistant: The Llama 2 paper does not specify a particular model used to predict the truthfulness and informativeness of the generated outputs from LLMs (Language Model). The paper primarily focuses on the limitations and potential risks of large language models. If you're referring to a different paper or model, please provide more details.   \n",
       "\n",
       "   ft_eval  rag_eval  base_eval  \n",
       "0        0         1          0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_dict, raw_response_df = run_evals(eval_dicts[6:7])\n",
    "display(eval_dict)\n",
    "display(raw_response_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bececd-287b-4617-80bb-34e3fba0cb13",
   "metadata": {},
   "source": [
    "#### Quantitative Evaluations\n",
    "\n",
    "Here we show quantitative metrics over both the training and eval set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "72f7c8e3-435d-473b-a4a1-faa2667337d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "k = 40\n",
    "\n",
    "train_dicts_sample = random.sample(train_dicts, k)\n",
    "eval_dicts_sample = random.sample(eval_dicts, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "50caca49-396b-471b-a1ec-b78cd357776c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007797956466674805,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22763a7cef1e4d1db40ed89c11c0c0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'ft_score': 0.425, 'rag_score': 0.7, 'base_score': 0.225}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_result, raw_response_df = run_evals(train_dicts_sample)\n",
    "display(eval_result)\n",
    "# display(raw_response_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b737d2a3-a8c3-46f5-9709-cb16b56365a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008716106414794922,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900f482111954273aec557a292681958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_result, raw_response_df = run_evals(eval_dicts_sample)\n",
    "display(eval_result)\n",
    "# display(raw_response_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6295bc-3637-49ab-9c87-060232b6c3da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v2",
   "language": "python",
   "name": "llama_index_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
