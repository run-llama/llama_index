{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "722c13cc-a78a-4037-859e-48c538d00d9b",
   "metadata": {},
   "source": [
    "# Knowledge Distillation For Fine-Tuning A GPT-3.5 Judge\n",
    "\n",
    "There has been recent research that demonstrated GPT-4's ability to closely align to human judges when evaluating LLM generated texts (e.g., see [[1]](https://arxiv.org/abs/2306.05685), [[2]](https://arxiv.org/abs/2303.16634)). In this notebook, we demonstrate how to use the `llama_index` library to distill knowledge from GPT-4 to GPT-3.5 so that the smaller GPT-3.5 becomes closer to GPT-4 performance; and by proxy, closer to human judges.\n",
    "\n",
    "To do so, we take the following steps:\n",
    "\n",
    "1. Generate datasets: `train` and `test`\n",
    "2. Perform knowledge distillation (using `train`)\n",
    "3. Evaluate the distilled model  on `test`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8deb07-ed32-4284-b943-e63867e26288",
   "metadata": {},
   "source": [
    "## 0 Prompt Templates & Asyncio Event Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99afd212-38b0-492c-91ea-a810e126ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "    \"QUESTION_GEN\": (\n",
    "        \"You are a Teacher/ Professor. Your task is to setup \"\n",
    "        \"a quiz/examination. Using the provided context, formulate \"\n",
    "        \"a single question that captures an important fact from the \"\n",
    "        \"context. Restrict the question to the context information provided.\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c89edf1-b359-4370-8b1e-fad279508c68",
   "metadata": {},
   "source": [
    "## 1 Generate datasets: `train` and `test`\n",
    "\n",
    "We should not lose sight on the ultimate goal here, which is to build an LLM judge that closely matches to human judges when evaluating LLM-generated texts. The work we need to do in this step, therefore, is to build a set of generated texts that our LLM judges will judge. More specifically, we will follow the \"pairwise comparison\" evaluation design pattern, where one text generation is passed to an LLM judge that is subsequently prompted to assign a score between 0 and 1 (higher is better).\n",
    "\n",
    "To generate a varied set of texts we'll use the following LLM text-generators:\n",
    "1. HuggingFace: Llama2-7B (chat)\n",
    "2. HuggingFace: Mistral-7B (instruct)\n",
    "3. HuggingFace: Falcon-7B (instruct)\n",
    "\n",
    "The generation task we ask of each of these models will be to generate an abstractive answer to question when provided relevant context (i.e., RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66486ab-38cf-4ed6-bef4-6fe9deee0590",
   "metadata": {},
   "source": [
    "### Using `DatasetGenerator` to build `train` and `test`\n",
    "\n",
    "The specific procedure we will use here involves generating questions against a set of chunks of a given `Document`. With the `<question, chunk>` pairs in hand, (for which we can merely treat as a \"simulated\" retrieval), we pass this information to the three LLM generators and prompt them each to generate an answer.\n",
    "\n",
    "Hang tight, we're almost there (sort of). Since we want to distill GPT-4 abilities for this task to GPT-3.5, we now need to generate GPT-4 judgements on the generated answers. To do this, we will pass the `<question, answer A, answer B>` (where `A` and `B` represent answers from any two of the LLM text-generators) as context to the GPT-4 judge and prompt it to decide the better answer of the two.\n",
    "\n",
    "With all of that we can now build a `dataset` that looks like the one below.\n",
    "| question | context-answer-A-answer-B | gpt-4-evaluation |\n",
    "|----------|---------------------------|------------------|\n",
    "| ...      | ...                       | ...              |\n",
    "\n",
    "And finally, to get `train` and `test` we will simply randomly shuffle `dataset` and split it using a 70/30 ratio. (Phew!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef531ed-8e97-4d8f-8cc3-6f7e0c6ca141",
   "metadata": {},
   "source": [
    "#### Generate Questions and LLM-Generated Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc60af-2ef8-43b6-8b24-f46adc223d03",
   "metadata": {},
   "source": [
    "With all that out of the way, let's spring into action. First, we will download the reference pdf document and create the set of questions against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536754e6-666f-4c43-82b0-d81c9281ed07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 20.7M  100 20.7M    0     0   619k      0  0:00:34  0:00:34 --:--:--  648k  441k      0  0:00:48  0:00:02  0:00:46  441k     0   611k      0  0:00:34  0:00:24  0:00:10  635k616k      0  0:00:34  0:00:32  0:00:02  632k\n"
     ]
    }
   ],
   "source": [
    "# Download the pdf document — Uncomment the line of code below\n",
    "# !curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc37cea-c9d2-4807-ab45-69f8b38db639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from llama_index import SimpleDirectoryReader, ServiceContext\n",
    "\n",
    "# load a document\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"paul_graham_essay.txt\"]\n",
    ").load_data()\n",
    "\n",
    "# Shuffle the documents\n",
    "random.seed(42)\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e10603-a0e0-4c87-a4d5-fdf8f1ca0303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate questions against chunks\n",
    "from llama_index.evaluation import DatasetGenerator\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# set context for llm provider\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    ")\n",
    "\n",
    "# instantiate a DatasetGenerator\n",
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    documents,\n",
    "    question_gen_query=PROMPTS[\"QUESTION_GEN\"],\n",
    "    service_context=gpt_35_context,\n",
    "    num_questions_per_chunk=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e2276-92dc-48c3-8cd0-583655ab7ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use DatasetGenerator to create questions from nodes\n",
    "questions = dataset_generator.generate_questions_from_nodes(num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f3dd3-62ab-4aac-a995-cf16d3306f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What were the two main things the author worked on before college?\n",
      "What programming language was regarded as the language of AI during the time period mentioned in the context?\n",
      "Question: What realization did the author have while visiting the Carnegie Institute?\n",
      "What was the author's experience like as a student at the Accademia?\n",
      "What did the author learn about technology companies while working at Interleaf?\n"
     ]
    }
   ],
   "source": [
    "# let's take a look at a few of these\n",
    "for q in questions[:5]:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b201d9cb-4746-4c71-8728-55e56cb8b76f",
   "metadata": {},
   "source": [
    "Now that we have the questions, the next step is to generate answers using the three LLM text-generators: Llama-2, Mistral, and Falcon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc71805-896d-4bbe-9053-495426a26a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.indices.vector_store.retrievers import VectorIndexRetriever\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents=documents)\n",
    "\n",
    "retriever = VectorIndexRetriever(  # what embeddings are being used?\n",
    "    index=index,\n",
    "    node_ids=list(index.index_struct.nodes_dict.values()),\n",
    "    similarity_top_k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a555bada-a208-418b-9868-6839405bae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HUGGING_FACE_TOKEN = os.getenv(\"HUGGING_FACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bde997-c408-44df-b3d3-e282a4be1abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine.retriever_query_engine import (\n",
    "    RetrieverQueryEngine,\n",
    ")\n",
    "from llama_index.llms import HuggingFaceInferenceAPI\n",
    "from llama_index.llm_predictor import LLMPredictor\n",
    "\n",
    "\n",
    "def create_query_engine(hf_name: str) -> RetrieverQueryEngine:\n",
    "    \"\"\"Create a RetrieverQueryEngine using the HuggingFaceInferenceAPI LLM\"\"\"\n",
    "    if hf_name not in hf_llm_generators:\n",
    "        raise KeyError(\"model not listed in hf_llm_generators\")\n",
    "    llm = HuggingFaceInferenceAPI(\n",
    "        model_name=hf_llm_generators[hf_name],\n",
    "        context_window=2048,  # to use refine\n",
    "        token=HUGGING_FACE_TOKEN,\n",
    "    )\n",
    "    context = ServiceContext.from_defaults(llm_predictor=LLMPredictor(llm=llm))\n",
    "    return RetrieverQueryEngine.from_args(\n",
    "        retriever=retriever, service_context=context\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84331853-ac29-49ca-85c1-e874d26e5f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# define our llm-generators\n",
    "hf_llm_generators = {\n",
    "    \"mistral-7b-instruct\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"llama2-7b-chat\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"falcon-7b-instruct\": \"tiiuae/falcon-7b-instruct\",\n",
    "}\n",
    "\n",
    "query_engines = {\n",
    "    mdl: create_query_engine(mdl) for mdl in hf_llm_generators.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eae846-8fad-4e85-a2a4-4a20a97f6c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mistral-7b-instruct': <llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x2920abf10>,\n",
       " 'llama2-7b-chat': <llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x2920aae60>,\n",
       " 'falcon-7b-instruct': <llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x2920a8100>}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760a173-675a-4db0-8f66-0b396c2a34d7",
   "metadata": {},
   "source": [
    "Now, we can create the master dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1d126f-8fcc-42ea-b143-4533638763a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [03:43<00:00, 22.33s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "dataset = []\n",
    "for q in tqdm.tqdm(questions):\n",
    "    data_entry = {\"question\": q}\n",
    "\n",
    "    responses = {}\n",
    "    for name, engine in query_engines.items():\n",
    "        responses[name] = str(engine.query(q))\n",
    "\n",
    "    data_entry[\"answers\"] = responses\n",
    "    dataset.append(data_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b9601c-78a9-448c-bac0-c78ef1c74cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What were the two main things the author worked on before college?',\n",
       "  'answers': {'mistral-7b-instruct': '\\n\\nBefore college, the two main things I worked on were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something.\\n\\nAfter',\n",
       "   'llama2-7b-chat': '\\nBefore college, the author worked on writing and programming, specifically writing essays and spam filters, and also painted.\\n\\nOriginal Answer: \\nThe two main things the author worked on before college were writing and programming.',\n",
       "   'falcon-7b-instruct': '\\n1. Writing and programming.\\n2. Writing and programming.\\nThe original answer is correct, but it could be improved to better answer the query. The context provided is that the author worked on writing and programming before college, and that they have been working on these skills since then. Adding more information about their work experience and projects could be useful.'}},\n",
       " {'question': 'What programming language was regarded as the language of AI during the time period mentioned in the context?',\n",
       "  'answers': {'mistral-7b-instruct': \"\\n\\nDuring the time period mentioned in the context, Lisp was regarded as the language of AI. However, it's worth noting that the field of AI was still in its early stages during this time, and there were many different programming languages and approaches being used to develop AI systems. Some of the other popular languages for AI development during this time period included Prolog, Python, and C++.\",\n",
       "   'llama2-7b-chat': \" During the time period mentioned in the context (1985), Lisp was the programming language most closely associated with AI research, particularly due to its use in the SHRDLU project.\\n\\nExplanation: While the original answer mentioned Lisp as the language of AI during this time period, the new context provides additional information that highlights Lisp's prominence in AI research during this time. The mention of Terry Winograd using SHRDLU, a project that was built using Lisp, further reinforces this association. Therefore, the refined answer focuses specifically on Lisp's connection to AI research during this time period.\",\n",
       "   'falcon-7b-instruct': '<strong>Lisp</strong> was regarded as the language of AI during the time period mentioned in the context.'}},\n",
       " {'question': 'Question: What realization did the author have while visiting the Carnegie Institute?',\n",
       "  'answers': {'mistral-7b-instruct': ' The author realized that programming was a more powerful and fulfilling field of study than philosophy, as it allowed for the creation of intelligent machines and the exploration of complex problems.',\n",
       "   'llama2-7b-chat': \" The author had a realization while visiting the Carnegie Institute that he could create things that would last, like paintings, which inspired him to pursue art as a career despite his initial interest in computer science and programming. The author was struck by the fact that paintings didn't become obsolete and some of the best ones were hundreds of years old, unlike the technology of the time which seemed to be constantly changing. This realization made him appreciate the durability and timelessness of art, and he decided to dedicate himself to creating works that would stand the test of time.\",\n",
       "   'falcon-7b-instruct': '\\nThe author had realized that they could make a living as an artist.'}}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f8ee56-5738-41b3-9a4c-ff6296910058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save these generations for future use\n",
    "import json\n",
    "\n",
    "with open(\"qa_dataset.jsonl\", \"w\") as outfile:\n",
    "    for entry in dataset:\n",
    "        print(json.dumps(entry), file=outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab67fef-964a-4806-ad01-ec9be4b7a8e1",
   "metadata": {},
   "source": [
    "#### Generate GPT-4 Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e219b1-58be-4a91-a7ca-0a056806d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_TEMPLATE = (\n",
    "    \"Please act as an impartial judge and evaluate the quality of the responses provided by two \"\n",
    "    \"AI assistants to the user question displayed below. You should choose the assistant that \"\n",
    "    \"follows the user’s instructions and answers the user’s question better. Your evaluation \"\n",
    "    \"should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, \"\n",
    "    \"and level of detail of their responses. Begin your evaluation by comparing the two \"\n",
    "    \"responses and provide a short explanation. Avoid any position biases and ensure that the \"\n",
    "    \"order in which the responses were presented does not influence your decision. Do not allow \"\n",
    "    \"the length of the responses to influence your evaluation. Do not favor certain names of \"\n",
    "    \"the assistants. Be as objective as possible. After providing your explanation, output your \"\n",
    "    \"final verdict by strictly following this format: '[[A]]' if assistant A is better, '[[B]]' \"\n",
    "    \"if assistant B is better, and '[[C]]' for a tie.\\n\"\n",
    ")\n",
    "\n",
    "DEFAULT_USER_TEMPLATE = (\n",
    "    \"[User Question]\\n\"\n",
    "    \"{query}\"\n",
    "    \"\\n\\n\"\n",
    "    \"[The Start of Assistant A’s Answer]\\n\"\n",
    "    \"{reference_answer}\\n\"\n",
    "    \"[The End of Assistant A’s Answer]\"\n",
    "    \"\\n\\n\"\n",
    "    \"[The Start of Assistant B’s Answer]\\n\"\n",
    "    \"{generated_answer}\\n\"\n",
    "    \"[The End of Assistant B’s Answer]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe6f3c-0fc5-4af9-b644-1c5219bf9dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading the jsonl file\n",
    "import json\n",
    "\n",
    "with open(\"qa_dataset.jsonl\") as f:\n",
    "    dataset = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e72360-fae5-49e2-adfc-394a504212ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "from llama_index.evaluation import PairwiseComparisonEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe8958f-62fd-41a8-8d21-3055cd80de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the gpt-4 judge\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "gpt_4_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(temperature=0, model=\"gpt-4\")\n",
    ")\n",
    "\n",
    "gpt4_judge = PairwiseComparisonEvaluator(service_context=gpt_4_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4491e0ad-a71e-4438-85b9-45a8c238f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de2c94-e113-4d9a-a87f-b1691411e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = await gpt4_judge.aevaluate(\n",
    "    query=dataset[0][\"question\"],\n",
    "    response=dataset[0][\"answers\"][\"mistral-7b-instruct\"],  # answer 2\n",
    "    reference=dataset[0][\"answers\"][\"llama2-7b-chat\"],  # answer 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4ad489-d588-400e-bb34-5b06fe9a5ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationResult(query='What were the two main things the author worked on before college?', contexts=None, response='\\n\\nBefore college, the two main things I worked on were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something.\\n\\nAfter', passing=None, feedback='It is not clear which answer is better.', score=0.5)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1122ca6d-c4c5-420c-aacc-12ad3663adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# define jupyter display function\n",
    "def display_eval_df(query, answer1, answer2, eval_result) -> None:\n",
    "    eval_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Query\": query,\n",
    "            \"Answer 1\": answer1,\n",
    "            \"Answer 2\": answer2,\n",
    "            \"Score\": eval_result.score,\n",
    "            \"Reason\": eval_result.feedback,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    eval_df = eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"300px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Answer 2\", \"Answer 1\"]\n",
    "    )\n",
    "    display(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfd4601-289e-4fba-bc74-b9a0cfcce09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6655a_row0_col1, #T_6655a_row0_col2 {\n",
       "  inline-size: 300px;\n",
       "  overflow-wrap: break-word;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6655a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6655a_level0_col0\" class=\"col_heading level0 col0\" >Query</th>\n",
       "      <th id=\"T_6655a_level0_col1\" class=\"col_heading level0 col1\" >Answer 1</th>\n",
       "      <th id=\"T_6655a_level0_col2\" class=\"col_heading level0 col2\" >Answer 2</th>\n",
       "      <th id=\"T_6655a_level0_col3\" class=\"col_heading level0 col3\" >Score</th>\n",
       "      <th id=\"T_6655a_level0_col4\" class=\"col_heading level0 col4\" >Reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6655a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_6655a_row0_col0\" class=\"data row0 col0\" >What were the two main things the author worked on before college?</td>\n",
       "      <td id=\"T_6655a_row0_col1\" class=\"data row0 col1\" >\n",
       "Before college, the author worked on writing and programming, specifically writing essays and spam filters, and also painted.\n",
       "\n",
       "Original Answer: \n",
       "The two main things the author worked on before college were writing and programming.</td>\n",
       "      <td id=\"T_6655a_row0_col2\" class=\"data row0 col2\" >\n",
       "\n",
       "Before college, the two main things I worked on were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
       "\n",
       "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n",
       "\n",
       "The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something.\n",
       "\n",
       "After</td>\n",
       "      <td id=\"T_6655a_row0_col3\" class=\"data row0 col3\" >0.500000</td>\n",
       "      <td id=\"T_6655a_row0_col4\" class=\"data row0 col4\" >It is not clear which answer is better.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x13f25b070>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_eval_df(\n",
    "    query=dataset[0][\"question\"],\n",
    "    answer2=dataset[0][\"answers\"][\"mistral-7b-instruct\"],\n",
    "    answer1=dataset[0][\"answers\"][\"llama2-7b-chat\"],\n",
    "    eval_result=eval_result,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42937500-a709-4556-aac2-0e5929f66e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our dataset, and split into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb16d0-15b1-45f3-96b0-3b51574d1626",
   "metadata": {},
   "source": [
    "## 2 Perform knowledge distillation\n",
    "\n",
    "Okay, it's now time to distill some knowledge from GPT-4 to GPT-3.5 To do this, we will make use of `OpenAIFinetuneEngine` class of `llama_index`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_3.10",
   "language": "python",
   "name": "llama_index_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
