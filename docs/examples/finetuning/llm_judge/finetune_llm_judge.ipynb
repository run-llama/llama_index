{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "722c13cc-a78a-4037-859e-48c538d00d9b",
   "metadata": {},
   "source": [
    "# Knowledge Distillation For Fine-Tuning A GPT-3.5 Judge\n",
    "\n",
    "There has been recent research that demonstrated GPT-4's ability to closely align to human judges when evaluating LLM generated texts (e.g., see [[1]](https://arxiv.org/abs/2306.05685), [[2]](https://arxiv.org/abs/2303.16634)). In this notebook, we demonstrate how to use the `llama_index` library to distill knowledge from GPT-4 to GPT-3.5 so that the smaller GPT-3.5 becomes closer to GPT-4 performance; and by proxy, closer to human judges.\n",
    "\n",
    "To do so, we take the following steps:\n",
    "\n",
    "1. Generate datasets: `train` and `test`\n",
    "2. Perform knowledge distillation (using `train`)\n",
    "3. Evaluate the distilled model  on `test`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8deb07-ed32-4284-b943-e63867e26288",
   "metadata": {},
   "source": [
    "## 0 Prompt Templates & Asyncio Event Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99afd212-38b0-492c-91ea-a810e126ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "    \"QUESTION_GEN\": (\n",
    "        \"You are a Teacher/ Professor. Your task is to setup \"\n",
    "        \"a quiz/examination. Using the provided context, formulate \"\n",
    "        \"a single question that captures an important fact from the \"\n",
    "        \"context. Restrict the question to the context information provided.\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c89edf1-b359-4370-8b1e-fad279508c68",
   "metadata": {},
   "source": [
    "## 1 Generate datasets: `train` and `test`\n",
    "\n",
    "We should not lose sight on the ultimate goal here, which is to build an LLM judge that closely matches to human judges when evaluating LLM-generated texts. The work we need to do in this step, therefore, is to build a set of generated texts that our LLM judges will judge. More specifically, we will follow the \"pairwise comparison\" evaluation design pattern, where one text generation is passed to an LLM judge that is subsequently prompted to assign a score between 0 and 1 (higher is better).\n",
    "\n",
    "To generate a varied set of texts we'll use the following LLM text-generators:\n",
    "1. HuggingFace: Llama2-7B (chat)\n",
    "2. HuggingFace: Mistral-7B (instruct)\n",
    "3. HuggingFace: Falcon-7B (instruct)\n",
    "\n",
    "The generation task we ask of each of these models will be to generate an abstractive answer to question when provided relevant context (i.e., RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66486ab-38cf-4ed6-bef4-6fe9deee0590",
   "metadata": {},
   "source": [
    "### Using `DatasetGenerator` to build `train` and `test`\n",
    "\n",
    "The specific procedure we will use here involves generating questions against a set of chunks of a given `Document`. With the `<question, chunk>` pairs in hand, (for which we can merely treat as a \"simulated\" retrieval), we pass this information to the three LLM generators and prompt them each to generate an answer.\n",
    "\n",
    "Hang tight, we're almost there (sort of). Since we want to distill GPT-4 abilities for this task to GPT-3.5, we now need to generate GPT-4 judgements on the generated answers. To do this, we will pass the `<question, answer A, answer B>` (where `A` and `B` represent answers from any two of the LLM text-generators) as context to the GPT-4 judge and prompt it to decide the better answer of the two.\n",
    "\n",
    "With all of that we can now build a `dataset` that looks like the one below.\n",
    "| question | context-answer-A-answer-B | gpt-4-evaluation |\n",
    "|----------|---------------------------|------------------|\n",
    "| ...      | ...                       | ...              |\n",
    "\n",
    "And finally, to get `train` and `test` we will simply randomly shuffle `dataset` and split it using a 70/30 ratio. (Phew!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef531ed-8e97-4d8f-8cc3-6f7e0c6ca141",
   "metadata": {},
   "source": [
    "#### Generate Questions and LLM-Generated Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc60af-2ef8-43b6-8b24-f46adc223d03",
   "metadata": {},
   "source": [
    "With all that out of the way, let's spring into action. First, we will download the reference pdf document and create the set of questions against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536754e6-666f-4c43-82b0-d81c9281ed07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 20.7M  100 20.7M    0     0   619k      0  0:00:34  0:00:34 --:--:--  648k  441k      0  0:00:48  0:00:02  0:00:46  441k     0   611k      0  0:00:34  0:00:24  0:00:10  635k616k      0  0:00:34  0:00:32  0:00:02  632k\n"
     ]
    }
   ],
   "source": [
    "# Download the pdf document — Uncomment the line of code below\n",
    "# !curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc37cea-c9d2-4807-ab45-69f8b38db639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from llama_index import SimpleDirectoryReader, ServiceContext\n",
    "\n",
    "# load a document\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"paul_graham_essay.txt\"]\n",
    ").load_data()\n",
    "\n",
    "# Shuffle the documents\n",
    "random.seed(42)\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e10603-a0e0-4c87-a4d5-fdf8f1ca0303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate questions against chunks\n",
    "from llama_index.evaluation import DatasetGenerator\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# set context for llm provider\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    ")\n",
    "\n",
    "# instantiate a DatasetGenerator\n",
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    documents,\n",
    "    question_gen_query=PROMPTS[\"QUESTION_GEN\"],\n",
    "    service_context=gpt_35_context,\n",
    "    num_questions_per_chunk=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e2276-92dc-48c3-8cd0-583655ab7ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use DatasetGenerator to create questions from nodes\n",
    "questions = dataset_generator.generate_questions_from_nodes(num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f3dd3-62ab-4aac-a995-cf16d3306f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was the author's initial experience with programming on the IBM 1401?\n",
      "What language was regarded as the language of AI during the time the author was learning AI at Cornell?\n",
      "Question: What prompted the author to consider pursuing a career in art?\n",
      "What was the author's experience like studying at the Accademia in Florence?\n",
      "What important lesson did the author learn from their experience at Interleaf?\n"
     ]
    }
   ],
   "source": [
    "# let's take a look at a few of these\n",
    "for q in questions[:5]:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b201d9cb-4746-4c71-8728-55e56cb8b76f",
   "metadata": {},
   "source": [
    "Now that we have the questions, the next step is to generate answers using the three LLM text-generators: Llama-2, Mistral, and Falcon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc71805-896d-4bbe-9053-495426a26a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.indices.vector_store.retrievers import VectorIndexRetriever\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents=documents)\n",
    "\n",
    "retriever = VectorIndexRetriever(  # what embeddings are being used?\n",
    "    index=index,\n",
    "    node_ids=list(index.index_struct.nodes_dict.values()),\n",
    "    similarity_top_k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a555bada-a208-418b-9868-6839405bae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HUGGING_FACE_TOKEN = os.getenv(\"HUGGING_FACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bde997-c408-44df-b3d3-e282a4be1abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine.retriever_query_engine import (\n",
    "    RetrieverQueryEngine,\n",
    ")\n",
    "from llama_index.llms import HuggingFaceInferenceAPI\n",
    "from llama_index.llm_predictor import LLMPredictor\n",
    "\n",
    "\n",
    "def create_query_engine(hf_name: str) -> RetrieverQueryEngine:\n",
    "    \"\"\"Create a RetrieverQueryEngine using the HuggingFaceInferenceAPI LLM\"\"\"\n",
    "    if hf_name not in hf_llm_generators:\n",
    "        raise KeyError(\"model not listed in hf_llm_generators\")\n",
    "    llm = HuggingFaceInferenceAPI(\n",
    "        model_name=hf_llm_generators[hf_name],\n",
    "        context_window=2048,  # to use refine\n",
    "        token=HUGGING_FACE_TOKEN,\n",
    "    )\n",
    "    context = ServiceContext.from_defaults(llm_predictor=LLMPredictor(llm=llm))\n",
    "    return RetrieverQueryEngine.from_args(\n",
    "        retriever=retriever, service_context=context\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84331853-ac29-49ca-85c1-e874d26e5f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# define our llm-generators\n",
    "hf_llm_generators = {\n",
    "    \"mistral-7b-instruct\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"llama2-7b-chat\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"falcon-7b-instruct\": \"tiiuae/falcon-7b-instruct\",\n",
    "}\n",
    "\n",
    "query_engines = {\n",
    "    mdl: create_query_engine(mdl) for mdl in hf_llm_generators.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eae846-8fad-4e85-a2a4-4a20a97f6c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mistral-7b-instruct': <llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x17fe05000>,\n",
       " 'llama2-7b-chat': <llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x17fe04820>,\n",
       " 'falcon-7b-instruct': <llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x17fe07160>}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760a173-675a-4db0-8f66-0b396c2a34d7",
   "metadata": {},
   "source": [
    "Now, we can create the master dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1d126f-8fcc-42ea-b143-4533638763a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [03:41<00:00, 22.17s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "dataset = []\n",
    "for q in tqdm.tqdm(questions):\n",
    "    data_entry = {\"question\": q}\n",
    "\n",
    "    responses = {}\n",
    "    for name, engine in query_engines.items():\n",
    "        responses[name] = str(engine.query(q))\n",
    "\n",
    "    data_entry[\"answers\"] = responses\n",
    "    dataset.append(data_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b9601c-78a9-448c-bac0-c78ef1c74cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': \"Question: What was the author's initial experience with programming on the IBM 1401?\",\n",
       "  'answers': {'mistral-7b-instruct': \"\\n\\nThe author's initial experience with programming on the IBM 1401 was that he couldn't figure out what to do with it. He couldn't understand the language he was using and couldn't think of any interesting problems to solve. He was puzzled by the machine and its capabilities. However, he was able to salvage some of the wreckage of his plans and focus on Lisp, which he found interesting for its own sake and not just for its association with AI. He decided to write a book about Lisp hacking, which helped him learn the language better. He was drawn to systems work, but he realized that any program he wrote would be obsolete in a couple of decades. He briefly considered using surplus Xerox Dandelions, but they were too slow by present standards.\",\n",
       "   'llama2-7b-chat': \"\\nThe author's initial experience with programming on the IBM 1401 was not memorable for them. They were around 13 or 14 years old when they first worked on the machine, but they didn't write any programs that they remember. The author found the experience of programming on the IBM 1401 to be unremarkable and didn't have a strong connection to the machine. The author's interest in programming was sparked later on, when they began working with Lisp, which they found more exciting and engaging.\",\n",
       "   'falcon-7b-instruct': \"\\nThe author's initial experience with programming on the IBM 1401 was limited. They were impressed by the machine's ability to store and process data, but they found the programming language to be primitive and limited. They were excited to learn Lisp, which expanded their understanding of programming concepts and allowed them to create more complex programs.\"}},\n",
       " {'question': 'What language was regarded as the language of AI during the time the author was learning AI at Cornell?',\n",
       "  'answers': {'mistral-7b-instruct': '\\n\\nDuring the time the author was learning AI at Cornell, Lisp was regarded as the language of AI. The author was drawn to AI due to a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. The author was convinced that Mike would become a reality soon and that teaching SHRDLU more words would be a matter of time.',\n",
       "   'llama2-7b-chat': ' Lisp\\n\\nThe author of the passage was learning AI at Cornell in the mid-1980s, and during that time, Lisp was the language of AI. The author mentions Terry Winograd using SHRDLU, which was a Lisp program, and this further supports the answer.',\n",
       "   'falcon-7b-instruct': '\\nThe original answer is correct, but it is possible to provide a more specific answer. The author was learning AI at Cornell in the mid-1980s, and the language of AI at that time was Lisp.'}},\n",
       " {'question': 'Question: What prompted the author to consider pursuing a career in art?',\n",
       "  'answers': {'mistral-7b-instruct': \"\\n\\nThe author was prompted to consider pursuing a career in art by a visit to the Carnegie Institute in 1988, where he realized that paintings could last and could be made as a living. He had always liked looking at paintings but had never imagined it was possible to make them. He started taking art classes at Harvard and decided to pursue a career in art. However, the author's decision to become rich and his interest in the World Wide Web led him to start a company to put art galleries online. Despite his efforts, the company failed to gain traction, and the author realized that the online art market was not as lucrative as he had initially thought.\",\n",
       "   'llama2-7b-chat': \"\\nThe author was prompted to consider pursuing a career in art after visiting the Carnegie Institute and realizing that paintings didn't become obsolete and that it was possible to make a living doing something they loved. They also wanted to build things that would last, and after visiting Rich Draves at CMU, they became interested in building things that would last. Additionally, the author was dissatisfied with their work in computer science and wanted to make something that would last. The author was also influenced by the thought of becoming rich, as they heard about a famous fund manager on the radio who was super rich and the author wondered why they couldn't do the same. However, the author soon realized that their idea of starting a company to put art galleries online was not a good one, as art galleries didn't want to be online and the author's software for generating web sites for galleries was not successful in signing up galleries. Instead, the author realized that online stores were similar to the sites they had been generating for galleries and that they could build an internet storefront, which was something they already knew how to do.\",\n",
       "   'falcon-7b-instruct': '\\nThe author was inspired to pursue a career in art by the fact that he was able to make art while working on a PhD in computer science. He was also inspired by the fact that he was able to make a living as a freelance Lisp hacker.'}},\n",
       " {'question': \"What was the author's experience like studying at the Accademia in Florence?\",\n",
       "  'answers': {'mistral-7b-instruct': \"\\n\\nThe author's experience studying at the Accademia di Belli Arti in Florence was not positive. Despite the prestige of the institution, the author found the curriculum to be lacking in focus on actual learning. The students would spend their time imitating things they'd seen in American art magazines, rather than studying and practicing their craft. The author was disappointed by this, as they had high hopes for the school. However, the author was able to get into the school despite not having any prior experience in Italian, and they were able to complete their studies and graduate.\",\n",
       "   'llama2-7b-chat': \"\\n\\nThe author's experience at the Accademia in Florence was not satisfactory, and they decided to leave after a year. They were disappointed with the lack of rigor in the painting department, where students were not required to learn anything and instead focused on painting still lives. The author found this experience to be disappointing as they wanted to learn something new. They also mention that they started painting still lives in their bedroom at night, and they found it to be a good way to observe and learn about what they were seeing.\\n\\nWhen the author received an invitation to take the entrance exam at the Accademia in Florence, they were tempted to continue studying, but the school was slow, and they didn't see the point. The author also mentions that they wanted to build things that would last, and they found the idea of making paintings that would last to be appealing. They started taking art classes at Harvard that fall, and they were genuinely in love with Lisp hacking and working on multiple projects, including their dissertation.\\n\\nIn retrospect, the author wishes they had focused on macros and embedded languages, as there is a whole world in that area that has barely been\",\n",
       "   'falcon-7b-instruct': \"\\nThe author's experience at the Accademia in Florence was a mix of frustration and inspiration. The frustration came from the fact that the school was not very well-equipped to teach modern art, and the author felt that he was not learning much. However, the inspiration came from the fact that he was surrounded by beautiful art and architecture, and he was able to learn a lot from the masters of the Renaissance. He also found that the school was very supportive of his work, and he was able to get a lot of help from his classmates. Overall, the author's experience at the Accademia was a positive one, and it helped him to develop his skills as an artist.\"}},\n",
       " {'question': 'What important lesson did the author learn from their experience at Interleaf?',\n",
       "  'answers': {'mistral-7b-instruct': '\\n\\nThe author learned that it\\'s better for technology companies to be run by product people than sales people, that it leads to bugs when code is edited by too many people, that cheap office space is no bargain if it\\'s depressing, that planned meetings are inferior to corridor conversations, that big, bureaucratic customers are a dangerous source of money, and that there\\'s not much overlap between conventional office hours and the optimal time for hacking, or conventional offices and the optimal place for it. Additionally, the author learned that the low end eats the high end, meaning it\\'s good to be the \"entry level\" option, even though that will be less prestigious, because if you\\'re not, someone else will be, and will squash you against the ceiling. The author also learned that the exponential growth in the power of commodity (i.e. Intel) processors rolled up high-end, special-purpose hardware and software companies like a bulldozer in the 1990s, and that money and coolness are tightly coupled in the art world. The author lived at Piazza San Felice 4, so their walk to the Accademia went straight down the',\n",
       "   'llama2-7b-chat': '\\nThe author learned that the low end can eat the high end, which means that it\\'s better to be the \"entry level\" option, even though it may be less prestigious, because if you\\'re not, someone else will be, and will squash you against the ceiling. This lesson was useful in both Viaweb and Y Combinator, and the author used it in their work.\\n\\nOriginal Answer: \\nWhat important lesson did the author learn from their experience at Interleaf?\\nThe author learned that the low end eats the high end, which means that it\\'s better to be the \"entry level\" option, even though it may be less prestigious, because if you\\'re not, someone else will be, and will squash you against the ceiling. This lesson was useful in both Viaweb and Y Combinator, and the author used it in their work.',\n",
       "   'falcon-7b-instruct': '\\nThe author learned that low-end software can eat high-end software.'}},\n",
       " {'question': \"Question: What was the author's motivation for wanting to become rich?\",\n",
       "  'answers': {'mistral-7b-instruct': \"\\n\\nThe author's motivation for wanting to become rich is not explicitly stated in the provided context information. However, based on the new context, it appears that the author's primary motivation was to graduate from graduate school and move on to the next stage of his career. In order to achieve this goal, the author wrote a dissertation in a short amount of time and applied to art schools. The author's desire to become rich may have been a secondary motivation, as it is mentioned in the context that the author had some money saved from consulting work he'd done in grad school and could potentially last a year if he lived cheaply.\",\n",
       "   'llama2-7b-chat': \"\\nThe author's motivation for wanting to become rich is not explicitly stated in the passage. However, based on the context, it can be inferred that the author's primary motivation was a desire for financial independence and the ability to create something that would last. The author mentions that they were drawn to work on things that weren't prestigious, but had the potential to be meaningful and lucrative. They also express a desire to build things that would last, rather than focusing solely on short-term financial gain. Therefore, while the author may have had some financial motivations, their primary drive seems to have been a desire to create something of lasting value.\",\n",
       "   'falcon-7b-instruct': \"\\nThe author's motivation for wanting to become rich was to have the freedom to work on things that were important to them, and to be able to provide for themselves and their family.\"}},\n",
       " {'question': 'What was the main goal of an online store builder according to the author?',\n",
       "  'answers': {'mistral-7b-instruct': \"\\n\\nAccording to the author, the main goal of an online store builder was to be easy to use and inexpensive. This was in line with the company's determination to be the Microsoft Word, not the Interleaf, of ecommerce software. The author also mentions that the main goal of an online store builder is to make users look legit, and the key to looking legit is high production values. The author states that if you get page layouts and fonts and colors right, you can make a guy running a store out of his bedroom look more legit than a big company.\",\n",
       "   'llama2-7b-chat': ' According to the author, the main goal of an online store builder was to make users look legitimate and professional, with high production values to make a small business owner look more legitimate than a big company.',\n",
       "   'falcon-7b-instruct': '\\nThe main goal of an online store builder according to the author is to build a web app that users can control through the browser, without any client software or typing anything into the command line on the server.'}},\n",
       " {'question': 'What were the three main parts of the software developed for the ecommerce startup?',\n",
       "  'answers': {'mistral-7b-instruct': \"\\n\\nThe three main parts of the software developed for the ecommerce startup were the editor, the shopping cart, and the manager. The editor was one of the best general-purpose site builders and kept the code tight. The shopping cart was developed by Robert, who was a graduate student at the time, and the manager, which kept track of orders and statistics, was developed by Trevor Blackwell, who was also a graduate student. The software was developed with the goal of making users look legit, and the key to looking legit was high production values. The site builder was designed to make it easy for users to create pages that looked like static ones, but instead of leading to static pages, the links all referred to closures stored in a hash table on the server. The software was developed in September, but it wasn't finished until almost 3 years later.\",\n",
       "   'llama2-7b-chat': '\\nThe three main parts of the software developed for the ecommerce startup were:\\n\\n1. The editor, which was written by the author and used to build sites.\\n2. The shopping cart, which was written by Robert.\\n3. The manager, which kept track of orders and statistics, and was written by Trevor Blackwell.\\n\\nThe author mentions that the software was initially developed with a WYSIWYG (What You See Is What You Get) site builder, which allowed users to create pages that looked exactly like the static pages that would be generated later. However, this approach was later abandoned in favor of a more traditional database-driven approach, which allowed for more flexibility and scalability.\\n\\nThe author also notes that Robert rebelled against the project after a month, feeling that it was not yet complete. In response, the author recruited more programmers, including Trevor Blackwell, who proved to be a valuable addition to the team.\\n\\nOverall, the software developed for the ecommerce startup consisted of three main parts: the editor, the shopping cart, and the manager. These components worked together to create a functional and user-friendly online store.',\n",
       "   'falcon-7b-instruct': '\\n1. Editor: a tool for creating and editing HTML documents.\\n2. Shopping cart: a tool for handling online orders.\\n3. Manager: a tool for managing the database and server.\\n\\nThe original answer was:\\n1. Editor: a tool for creating and editing HTML documents.\\n2. Shopping cart: a tool for handling online orders.\\n3. Manager: a tool for managing the database and server.'}},\n",
       " {'question': \"What was the author's initial perception of their company before it was acquired by Yahoo?\",\n",
       "  'answers': {'mistral-7b-instruct': \"\\nThe author's initial perception of their company before it was acquired by Yahoo was that it was amateurish and they didn't reach breakeven until about when Yahoo bought them in the summer of 1998. They were at the mercy of investors for the entire life of the company and since both they and their investors were noobs at startups, the result was a mess even by startup standards. However, the author later realized that growth rate is the ultimate test of a startup and that they were doing fine in terms of growth. They mistakenly thought the absolute number of users mattered, but in the long term the growth rate takes care of the absolute number. The author also learned about retail and user experience through building stores for users.\",\n",
       "   'llama2-7b-chat': \" The author initially perceived their company as amateurish, as they were focused on building stores for users and didn't prioritize the absolute number of users. They were more concerned with growth rate and believed that a company with just a handful of employees would seem amateurish during the Internet Bubble.\",\n",
       "   'falcon-7b-instruct': \"\\nWhat was the author's initial perception of their company before it was acquired by Yahoo?\\nThe author's initial perception of their company was that it was a mess. They were overwhelmed by the number of users and the amount of work they had to do. They were unsure of the company's future and whether they would be able to keep up with the demand. They were also unsure of their own ability to keep up with the demand. They were overwhelmed by the amount of work they had to do and the number of users they had to scan. They were unsure of the company's future and whether they would be able to keep up with the demand. They were also unsure of their own ability to keep up with the demand.\"}},\n",
       " {'question': 'What was the name of the new company that the author started to build web applications?',\n",
       "  'answers': {'mistral-7b-instruct': ' The new company that the author started to build web applications was called Aspra.',\n",
       "   'llama2-7b-chat': ' The name of the new company that the author started to build web applications was Aspra, which was co-founded by the author and Robert Morris.',\n",
       "   'falcon-7b-instruct': '<strong>Aspra</strong>'}}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f8ee56-5738-41b3-9a4c-ff6296910058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save these generations for future use\n",
    "import json\n",
    "\n",
    "with open(\"qa_dataset.jsonl\", \"w\") as outfile:\n",
    "    for entry in dataset:\n",
    "        print(json.dumps(entry), file=outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab67fef-964a-4806-ad01-ec9be4b7a8e1",
   "metadata": {},
   "source": [
    "#### Generate GPT-4 Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e72360-fae5-49e2-adfc-394a504212ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe6f3c-0fc5-4af9-b644-1c5219bf9dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for loading the jsonl file\n",
    "# with open('dataset.jsonl') as f:\n",
    "#     loaded_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42937500-a709-4556-aac2-0e5929f66e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our dataset, and split into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb16d0-15b1-45f3-96b0-3b51574d1626",
   "metadata": {},
   "source": [
    "## 2 Perform knowledge distillation\n",
    "\n",
    "Okay, it's now time to distill some knowledge from GPT-4 to GPT-3.5 To do this, we will make use of `OpenAIFinetuneEngine` class of `llama_index`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_3.10",
   "language": "python",
   "name": "llama_index_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
