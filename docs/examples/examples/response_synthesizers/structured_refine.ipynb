{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def266be",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/response_synthesizers/structured_refine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "540ff471-dcea-4b3e-9c0c-a3173f1c640e",
   "metadata": {},
   "source": [
    "# Refine with Structured Answer Filtering\n",
    "When using our Refine response synthesizer for response synthesis, it's crucial to filter out non-answers. An issue often encountered is the propagation of a single unhelpful response like \"I don't have the answer\", which can persist throughout the synthesis process and lead to a final answer of the same nature. This can occur even when there are actual answers present in other, more relevant sections.\n",
    "\n",
    "These unhelpful responses can be filtered out by setting `structured_answer_filtering` to `True`. It is set to `False` by default since this currently only works best if you are using an OpenAI model that supports function calling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d14c2",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9a84d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9584f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "158b08a8-32d3-4397-ad37-75870416226b",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b6f5c-5852-41be-8ce8-d94c520e0e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"The president in the year 2040 is John Cena.\",\n",
    "    \"The president in the year 2050 is Florence Pugh.\",\n",
    "    'The president in the year 2060 is Dwayne \"The Rock\" Johnson.',\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efed56ee-fcd3-439c-a1b2-53c643f15c8e",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903f9dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4dad4-6044-4c9c-becd-4e2908b54a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c48278-f5b2-47bb-a240-6b66a191c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "summarizer = get_response_synthesizer(\n",
    "    response_mode=\"refine\", llm=llm, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ac725-54ce-4243-bc09-4a50e2590b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Refine context: The president in the year 2050 is Florence Pugh.\n",
      "> Refine context: The president in the year 2060 is Dwayne \"The R...\n"
     ]
    }
   ],
   "source": [
    "response = summarizer.get_response(\"who is president in the year 2050?\", texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc2744b",
   "metadata": {},
   "source": [
    "### Failed Result\n",
    "As you can see, we weren't able to get the correct answer from the input `texts` strings since the initial \"I don't know\" answer propogated through till the end of the response synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a600aa73-74b8-4a20-8f56-1b273417f788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have access to information about the future.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218b85d5",
   "metadata": {},
   "source": [
    "Now we'll try again with `structured_answer_filtering=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27488623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "summarizer = get_response_synthesizer(\n",
    "    response_mode=\"refine\",\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    structured_answer_filtering=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac8681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function call: StructuredRefineResponse with args: {\n",
      "  \"answer\": \"It is not possible to determine who the president is in the year 2050 based on the given context information.\",\n",
      "  \"query_satisfied\": false\n",
      "}\n",
      "> Refine context: The president in the year 2050 is Florence Pugh.\n",
      "Function call: StructuredRefineResponse with args: {\n",
      "  \"answer\": \"Florence Pugh\",\n",
      "  \"query_satisfied\": true\n",
      "}\n",
      "> Refine context: The president in the year 2060 is Dwayne \"The R...\n",
      "Function call: StructuredRefineResponse with args: {\n",
      "  \"answer\": \"Florence Pugh\",\n",
      "  \"query_satisfied\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = summarizer.get_response(\"who is president in the year 2050?\", texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed92fb",
   "metadata": {},
   "source": [
    "### Successful Result\n",
    "As you can see, we were able to determine the correct answer from the given context by filtering the `texts` strings for the ones that actually contained the answer to our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0503c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florence Pugh\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6668e7",
   "metadata": {},
   "source": [
    "## Non Function-calling LLMs\n",
    "You may want to make use of this filtering functionality with an LLM that doesn't offer a function calling API.\n",
    "\n",
    "In that case, the `Refine` module will automatically switch to using a structured output `Program` that doesn't rely on an external function calling API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f6f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll stick with OpenAI but use an older model that does not support function calling\n",
    "instruct_llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fddd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "summarizer = get_response_synthesizer(\n",
    "    response_mode=\"refine\",\n",
    "    llm=instruct_llm,\n",
    "    verbose=True,\n",
    "    structured_answer_filtering=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90911bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florence Pugh\n"
     ]
    }
   ],
   "source": [
    "response = summarizer.get_response(\"who is president in the year 2050?\", texts)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e39730",
   "metadata": {},
   "source": [
    "### `CompactAndRefine`\n",
    "Since `CompactAndRefine` is built on top of `Refine`, this response mode also supports structured answer filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf1c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "summarizer = get_response_synthesizer(\n",
    "    response_mode=\"compact\",\n",
    "    llm=instruct_llm,\n",
    "    verbose=True,\n",
    "    structured_answer_filtering=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf9213a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florence Pugh\n"
     ]
    }
   ],
   "source": [
    "response = summarizer.get_response(\"who is president in the year 2050?\", texts)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
