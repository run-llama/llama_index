{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG: Multimodal Search on Videos and Stream Video Results üì∫\n",
    "\n",
    "Constructing a RAG pipeline for text is relatively straightforward, thanks to the tools developed for parsing, indexing, and retrieving text data. \n",
    "\n",
    "However, adapting RAG models for video content presents a greater challenge. Videos combine visual, auditory, and textual elements, requiring more processing power and sophisticated video pipelines.\n",
    "\n",
    "While Large Language Models (LLMs) excel with text, they fall short in helping you consume or create video clips. `VideoDB` provides a sophisticated database abstraction for your MP4 files, enabling the use of LLMs on your video data. With VideoDB, you can not only analyze but also `instantly watch video streams` of your search results.\n",
    "\n",
    "> [VideoDB](https://videodb.io) is a serverless database designed to streamline the storage, search, editing, and streaming of video content. VideoDB offers random access to sequential video data by building indexes and developing interfaces for querying and browsing video content. Learn more at [docs.videodb.io](https://docs.videodb.io).\n",
    "\n",
    "\n",
    "In this notebook, we introduce `VideoDBRetriever`, a tool specifically designed to simplify the creation of RAG pipelines for video content, without any hassle of dealing with complex video infrastructure.\n",
    "\n",
    "![](https://raw.githubusercontent.com/video-db/videodb-cookbook-assets/main/images/guides/multimodal_llama_index_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## üõ†Ô∏èÔ∏è Setup \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîë Requirements\n",
    "\n",
    "To connect to VideoDB, simply get the API key and create a connection. This can be done by setting the `VIDEO_DB_API_KEY` environment variable. You can get it from üëâüèº [VideoDB Console](https://console.videodb.io). ( Free for first 50 uploads, **No credit card required!** )\n",
    "\n",
    "Get your `OPENAI_API_KEY` from OpenAI platform for `llama_index` response synthesizer.\n",
    "\n",
    "<!-- > Set the `OPENAI_API_KEY` & `VIDEO_DB_API_KEY` environment variable with your API keys. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"VIDEO_DB_API_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Installing Dependencies\n",
    "\n",
    "To get started, we'll need to install the following packages:\n",
    "\n",
    "- `llama-index`\n",
    "- `llama-index-retrievers-videodb`\n",
    "- `videodb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install videodb\n",
    "%pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-retrievers-videodb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ† Using VideoDBRetriever to Build RAG for Single Video\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìã Step 1: Connect to VideoDB and Ingest Data\n",
    "\n",
    "Let's upload a our video file first.\n",
    "\n",
    "You can use any `public url`, `Youtube link` or `local file` on your system. \n",
    "\n",
    "> ‚ú® First 50 uploads are free!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Video\n",
      "Video uploaded with ID: m-a758f9bb-f769-484a-9d54-02417ccfe7e6\n"
     ]
    }
   ],
   "source": [
    "from videodb import connect\n",
    "\n",
    "# connect to VideoDB\n",
    "conn = connect()\n",
    "coll = conn.create_collection(\n",
    "    name=\"VideoDB Retrievers\", description=\"VideoDB Retrievers\"\n",
    ")\n",
    "\n",
    "# upload videos to default collection in VideoDB\n",
    "print(\"Uploading Video\")\n",
    "video = coll.upload(url=\"https://www.youtube.com/watch?v=aRgP3n0XiMc\")\n",
    "print(f\"Video uploaded with ID: {video.id}\")\n",
    "\n",
    "# video = coll.get_video(\"m-b6230808-307d-468a-af84-863b2c321f05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * `coll = conn.get_collection()` : Returns default collection object.\n",
    "> * `coll.get_videos()` : Returns list of all the videos in a collections.\n",
    "> * `coll.get_video(video_id)`: Returns Video object from given`video_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üó£Ô∏è Step 2: Indexing & Search from Spoken Content  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video can be viewed as data with different modalities. First, we will work with the `spoken content`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üó£Ô∏è Indexing Spoken Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing spoken content in Video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:48<00:00,  2.08it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Indexing spoken content in Video...\")\n",
    "video.index_spoken_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üó£Ô∏è Retrieving Relevant Nodes from Spoken Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `VideoDBRetriever` to retrieve relevant nodes from our indexed content. The video ID should be passed as a parameter, and the `index_type` should be set to `IndexType.spoken_word`.\n",
    "\n",
    "You can configure the `score_threshold` and `result_threshold` after experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.videodb import VideoDBRetriever\n",
    "from videodb import SearchType, IndexType\n",
    "\n",
    "spoken_retriever = VideoDBRetriever(\n",
    "    collection=coll.id,\n",
    "    video=video.id,\n",
    "    search_type=SearchType.semantic,\n",
    "    index_type=IndexType.spoken_word,\n",
    "    score_threshold=0.1,\n",
    ")\n",
    "\n",
    "spoken_query = \"Nationwide exams\"\n",
    "nodes_spoken_index = spoken_retriever.retrieve(spoken_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üó£Ô∏èÔ∏èÔ∏è Viewing the result : üí¨ Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the relevant nodes and synthesize the response using llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of the nationwide exams were eagerly awaited all day.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "response = response_synthesizer.synthesize(\n",
    "    spoken_query, nodes=nodes_spoken_index\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üó£Ô∏è Viewing the result : üé• Video Clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each retrieved node that is relevant to the query, the `start` and `end` fields in the metadata represent the time interval covered by the node.\n",
    "\n",
    "We will use VideoDB's Programmable Stream to generate a stream of relevant video clips based on the timestamps of these nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://console.videodb.io/player?url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/3c108acd-e459-494a-bc17-b4768c78e5df.m3u8'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from videodb import play_stream\n",
    "\n",
    "results = [\n",
    "    (node.metadata[\"start\"], node.metadata[\"end\"])\n",
    "    for node in nodes_spoken_index\n",
    "]\n",
    "\n",
    "stream_link = video.generate_stream(results)\n",
    "play_stream(stream_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì∏Ô∏è Step3 : Index & Search from Visual Content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üì∏ Indexing Visual Content\n",
    "To learn more about Scene Index, explore the following guides:\n",
    "\n",
    "- [Quickstart Guide](https://github.com/video-db/videodb-cookbook/blob/main/quickstart/Scene%20Index%20QuickStart.ipynb) guide provides a step-by-step introduction to Scene Index. It's ideal for getting started quickly and understanding the primary functions.\n",
    "\n",
    "- [Scene Extraction Options Guide](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/playground_scene_extraction.ipynb) delves deeper into the various options available for scene extraction within Scene Index. It covers advanced settings, customization features, and tips for optimizing scene extraction based on different needs and preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Visual content in Video...\n",
      "Scene Index successful with ID: 990733050d6fd4f5\n"
     ]
    }
   ],
   "source": [
    "from videodb import SceneExtractionType\n",
    "\n",
    "print(\"Indexing Visual content in Video...\")\n",
    "\n",
    "# Index scene content\n",
    "index_id = video.index_scenes(\n",
    "    extraction_type=SceneExtractionType.shot_based,\n",
    "    extraction_config={\"frame_count\": 3},\n",
    "    prompt=\"Describe the scene in detail\",\n",
    ")\n",
    "video.get_scene_index(index_id)\n",
    "\n",
    "print(f\"Scene Index successful with ID: {index_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üì∏Ô∏è Retrieving Relevant Nodes from Scene Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we used `VideoDBRetriever` for the spoken index, we will use it for the scene index. Here, we will need to set `index_type` to `IndexType.scene` and pass the `scene_index_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.videodb import VideoDBRetriever\n",
    "from videodb import SearchType, IndexType\n",
    "\n",
    "\n",
    "scene_retriever = VideoDBRetriever(\n",
    "    collection=coll.id,\n",
    "    video=video.id,\n",
    "    search_type=SearchType.semantic,\n",
    "    index_type=IndexType.scene,\n",
    "    scene_index_id=index_id,\n",
    "    score_threshold=0.1,\n",
    ")\n",
    "\n",
    "scene_query = \"accident scenes\"\n",
    "nodes_scene_index = scene_retriever.retrieve(scene_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üì∏Ô∏èÔ∏èÔ∏è Viewing the result : üí¨ Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scenes described do not depict accidents but rather dynamic and intense scenarios involving motion, urgency, and tension in urban settings at night.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "response = response_synthesizer.synthesize(\n",
    "    scene_query, nodes=nodes_scene_index\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üì∏ Ô∏è Viewing the result : üé• Video Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://console.videodb.io/player?url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/ae74e9da-13bf-4056-8cfa-0267087b74d7.m3u8'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from videodb import play_stream\n",
    "\n",
    "results = [\n",
    "    (node.metadata[\"start\"], node.metadata[\"end\"])\n",
    "    for node in nodes_scene_index\n",
    "]\n",
    "\n",
    "stream_link = video.generate_stream(results)\n",
    "play_stream(stream_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Step4: Simple Multimodal RAG - Combining Results of Both modalities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to unlock in multimodal queries in our video library like this: \n",
    "\n",
    "> üì∏üó£Ô∏è \"*Show me 1.Accident Scene 2.Discussion about nationwide exams*\"\n",
    "\n",
    "There are lots of way to do create a multimodal RAG, for the sake of simplicity we are choosing a simple approach:\n",
    "\n",
    "1. üß© **Query Transformation**: Divide query into two parts that can be used with respective scene and spoken indexes.\n",
    "2. üîé **Finding Relevant nodes for each modality**: Using `VideoDBRetriever` find relevant nodes from Spoken Index and Scene Index \n",
    "3. ‚úèÔ∏è **Viewing the result : Text**: Use Relevant Nodes to sythesize a text reponse Integrating the results from both indexes for precise video segment identification. \n",
    "4. üé• **Viewing the result : Video Clip**: Integrating the results from both indexes for precise video segment identification. \n",
    "\n",
    "> To checkout more advanced multimodal techniques, checkout out [advnaced multimodal guides](https://docs.videodb.io/multimodal-guide-90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß© Query Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query for Spoken retriever :  Discussion about nationwide exams\n",
      "Query for Scene retriever :  Accident Scene\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "def split_spoken_visual_query(query):\n",
    "    transformation_prompt = \"\"\"\n",
    "    Divide the following query into two distinct parts: one for spoken content and one for visual content. The spoken content should refer to any narration, dialogue, or verbal explanations and The visual content should refer to any images, videos, or graphical representations. Format the response strictly as:\\nSpoken: <spoken_query>\\nVisual: <visual_query>\\n\\nQuery: {query}\n",
    "    \"\"\"\n",
    "    prompt = transformation_prompt.format(query=query)\n",
    "    response = OpenAI(model=\"gpt-4\").complete(prompt)\n",
    "    divided_query = response.text.strip().split(\"\\n\")\n",
    "    spoken_query = divided_query[0].replace(\"Spoken:\", \"\").strip()\n",
    "    scene_query = divided_query[1].replace(\"Visual:\", \"\").strip()\n",
    "    return spoken_query, scene_query\n",
    "\n",
    "\n",
    "query = \"Show me 1.Accident Scene 2.Discussion about nationwide exams \"\n",
    "spoken_query, scene_query = split_spoken_visual_query(query)\n",
    "print(\"Query for Spoken retriever : \", spoken_query)\n",
    "print(\"Query for Scene retriever : \", scene_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üîé Finding Relevant nodes for each modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import SearchType, IndexType\n",
    "\n",
    "# Retriever for Spoken Index\n",
    "spoken_retriever = VideoDBRetriever(\n",
    "    collection=coll.id,\n",
    "    video=video.id,\n",
    "    search_type=SearchType.semantic,\n",
    "    index_type=IndexType.spoken_word,\n",
    "    score_threshold=0.1,\n",
    ")\n",
    "\n",
    "# Retriever for Scene Index\n",
    "scene_retriever = VideoDBRetriever(\n",
    "    collection=coll.id,\n",
    "    video=video.id,\n",
    "    search_type=SearchType.semantic,\n",
    "    index_type=IndexType.scene,\n",
    "    scene_index_id=index_id,\n",
    "    score_threshold=0.1,\n",
    ")\n",
    "\n",
    "# Fetch relevant nodes for Spoken index\n",
    "nodes_spoken_index = spoken_retriever.retrieve(spoken_query)\n",
    "\n",
    "# Fetch relevant nodes for Scene index\n",
    "nodes_scene_index = scene_retriever.retrieve(scene_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ô∏èüí¨Ô∏è Viewing the result : Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first scene depicts a dynamic and intense scenario in an urban setting at night, involving a motorcycle chase with a figure possibly dodging away. The second scene portrays a dramatic escape or rescue situation with characters in motion alongside a large truck. The discussion about nationwide exams involves a conversation between a character and their mother about exam results and studying.\n"
     ]
    }
   ],
   "source": [
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "response = response_synthesizer.synthesize(\n",
    "    query, nodes=nodes_scene_index + nodes_spoken_index\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üé• Viewing the result : Video Clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From each modality, we have retrieved results that are relevant to the query within that specific modality (semantic and scene/visual, in this case).\n",
    "\n",
    "Each node has start and end fields in the metadata, which represent the time interval the node covers.\n",
    "\n",
    "There are lots of way to sythesize there results, For now we will use a simple method : \n",
    "\n",
    "- `Union`: This method takes all the timestamps from every node, creating a comprehensive list that includes every relevant time, even if some timestamps appear in only one modality.\n",
    "\n",
    "One of the other ways can be `Intersection`:\n",
    "\n",
    "- `Intersection`: This method only includes timestamps that are present in every node, resulting in a smaller list with times that are universally relevant across all modalities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://console.videodb.io/player?url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/91b08b39-c72f-4e33-ad1c-47a2ea11ac17.m3u8'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from videodb import play_stream\n",
    "\n",
    "\n",
    "def merge_intervals(intervals):\n",
    "    if not intervals:\n",
    "        return []\n",
    "    intervals.sort(key=lambda x: x[0])\n",
    "    merged = [intervals[0]]\n",
    "    for interval in intervals[1:]:\n",
    "        if interval[0] <= merged[-1][1]:\n",
    "            merged[-1][1] = max(merged[-1][1], interval[1])\n",
    "        else:\n",
    "            merged.append(interval)\n",
    "    return merged\n",
    "\n",
    "\n",
    "# Extract timestamps from both relevant nodes\n",
    "results = [\n",
    "    [node.metadata[\"start\"], node.metadata[\"end\"]]\n",
    "    for node in nodes_spoken_index + nodes_scene_index\n",
    "]\n",
    "merged_results = merge_intervals(results)\n",
    "\n",
    "# Use Videodb to create a stream of relevant clips\n",
    "stream_link = video.generate_stream(merged_results)\n",
    "play_stream(stream_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ† Using VideoDBRetriever to Build RAG for Collection of Videos\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding More videos to our collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_2 = coll.upload(url=\"https://www.youtube.com/watch?v=kMRX3EA68g4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üó£Ô∏è Indexing Spoken Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_2.index_spoken_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üì∏ Indexing Scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Video(id=m-b6230808-307d-468a-af84-863b2c321f05, collection_id=c-4882e4a8-9812-4921-80ff-b77c9c4ab4e7, stream_url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/528623c2-3a8e-4c84-8f05-4dd74f1a9977.m3u8, player_url=https://console.dev.videodb.io/player?url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/528623c2-3a8e-4c84-8f05-4dd74f1a9977.m3u8, name=Death note - episode 1 (english dubbed) | HD, description=None, thumbnail_url=None, length=1366.006712),\n",
       " Video(id=m-f5b86106-4c28-43f1-b753-fa9b3f839dfe, collection_id=c-4882e4a8-9812-4921-80ff-b77c9c4ab4e7, stream_url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/4273851a-46f3-4d57-bc1b-9012ce330da8.m3u8, player_url=https://console.dev.videodb.io/player?url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/4273851a-46f3-4d57-bc1b-9012ce330da8.m3u8, name=Death note - episode 5 (english dubbed) | HD, description=None, thumbnail_url=None, length=1366.099592)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from videodb import SceneExtractionType\n",
    "\n",
    "print(\"Indexing Visual content in Video...\")\n",
    "\n",
    "# Index scene content\n",
    "index_id = video_2.index_scenes(\n",
    "    extraction_type=SceneExtractionType.shot_based,\n",
    "    extraction_config={\"frame_count\": 3},\n",
    "    prompt=\"Describe the scene in detail\",\n",
    ")\n",
    "video_2.get_scene_index(index_id)\n",
    "\n",
    "print(f\"Scene Index successful with ID: {index_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß© Query Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query for Spoken retriever :  Kiara is speaking\n",
      "Query for Scene retriever :  Show me Accident Scene\n"
     ]
    }
   ],
   "source": [
    "query = \"Show me 1.Accident Scene 2.Kiara is speaking \"\n",
    "spoken_query, scene_query = split_spoken_visual_query(query)\n",
    "print(\"Query for Spoken retriever : \", spoken_query)\n",
    "print(\"Query for Scene retriever : \", scene_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîé Finding relevant nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import SearchType, IndexType\n",
    "\n",
    "# Retriever for Spoken Index\n",
    "spoken_retriever = VideoDBRetriever(\n",
    "    collection=coll.id,\n",
    "    search_type=SearchType.semantic,\n",
    "    index_type=IndexType.spoken_word,\n",
    "    score_threshold=0.2,\n",
    ")\n",
    "\n",
    "# Retriever for Scene Index\n",
    "scene_retriever = VideoDBRetriever(\n",
    "    collection=coll.id,\n",
    "    search_type=SearchType.semantic,\n",
    "    index_type=IndexType.scene,\n",
    "    score_threshold=0.2,\n",
    ")\n",
    "\n",
    "# Fetch relevant nodes for Spoken index\n",
    "nodes_spoken_index = spoken_retriever.retrieve(spoken_query)\n",
    "\n",
    "# Fetch relevant nodes for Scene index\n",
    "nodes_scene_index = scene_retriever.retrieve(scene_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ô∏èüí¨Ô∏è Viewing the result : Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kira is speaking about his plans and intentions regarding the agent from the bus. The accident scene captures a distressing moment where an individual is urgently making a phone call near a damaged car, with a victim lying motionless on the ground. The chaotic scene includes a bus in the background, emphasizing the severity of the tragic incident.\n"
     ]
    }
   ],
   "source": [
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "response = response_synthesizer.synthesize(\n",
    "    \"What is kaira speaking. And tell me about accident scene\",\n",
    "    nodes=nodes_scene_index + nodes_spoken_index,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üé• Viewing the result : Video Clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with an editing workflow involving multiple videos, we need to create a `Timeline` of `VideoAsset` and then compile them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://codaio.imgix.net/docs/_s5lUnUCIU/blobs/bl-n4vT_dFztl/e664f43dbd4da89c3a3bfc92e3224c8a188eb19d2d458bebe049e780f72506ca6b19421c7168205f7ad307187e73da60c73cdbb9a0ef3fec77cc711927ad26a29a92cd13691fa9375c231f1c006853bacf28e09b3bf0bbcb5f7b76462b354a180fb437ad?auto=format%2Ccompress&fit=max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://console.videodb.io/player?url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/2810827b-4d80-44af-a26b-ded2a7a586f6.m3u8'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from videodb import connect, play_stream\n",
    "from videodb.timeline import Timeline\n",
    "from videodb.asset import VideoAsset\n",
    "\n",
    "# Create a new timeline Object\n",
    "timeline = Timeline(conn)\n",
    "\n",
    "for node_obj in nodes_scene_index + nodes_spoken_index:\n",
    "    node = node_obj.node\n",
    "\n",
    "    # Create a Video asset for each node\n",
    "    node_asset = VideoAsset(\n",
    "        asset_id=node.metadata[\"video_id\"],\n",
    "        start=node.metadata[\"start\"],\n",
    "        end=node.metadata[\"end\"],\n",
    "    )\n",
    "\n",
    "    # Add the asset to timeline\n",
    "    timeline.add_inline(node_asset)\n",
    "\n",
    "# Generate stream for the compiled timeline\n",
    "stream_url = timeline.generate_stream()\n",
    "play_stream(stream_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## Configuring `VideoDBRetriever`\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Retriever for only one Video\n",
    "You can pass the `id` of the video object to search in only that video. \n",
    "```python\n",
    "VideoDBRetriever(video=\"my_video.id\")\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è Retriever for a set of Video/ Collection\n",
    "You can pass the `id` of the Collection to search in only that Collection. \n",
    "```python\n",
    "VideoDBRetriever(collection=\"my_coll.id\")\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è Retriever for different type of Indexes\n",
    "```python\n",
    "from videodb import IndexType\n",
    "spoken_word = VideoDBRetriever(index_type=IndexType.spoken_word)\n",
    "\n",
    "scene_retriever = VideoDBRetriever(index_type=IndexType.scene, scene_index_id=\"my_index_id\")\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è Configuring Search Type of Retriever \n",
    "`search_type` determines the search method used to retrieve nodes against given query \n",
    "```python\n",
    "from videodb import SearchType, IndexType\n",
    "\n",
    "keyword_spoken_search = VideoDBRetriever(\n",
    "    search_type=SearchType.keyword,\n",
    "    index_type=IndexType.spoken_word\n",
    ")\n",
    "\n",
    "semantic_scene_search = VideoDBRetriever(\n",
    "    search_type=SearchType.semantic,\n",
    "    index_type=IndexType.spoken_word\n",
    ")\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è Configure threshold parameters  \n",
    "- `result_threshold`: is the threshold for number of results returned by retriever; the default value is `5`\n",
    "- `score_threshold`: only nodes with score higher than `score_threshold` will be returned by retriever; the default value is `0.2`  \n",
    "\n",
    "```python\n",
    "custom_retriever = VideoDBRetriever(result_threshold=2, score_threshold=0.5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ú® Configuring Indexing and Chunking\n",
    "---\n",
    "\n",
    "In this example, we utilize the VideoDB's Indexing for video retrieval. However, you have the flexibility to load both Transcript and Scene Data and apply your own indexing techniques using llamaindex.\n",
    "\n",
    "For more detailed guidance, refer to this [guide](https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/multi_modal/multi_modal_videorag_videodb.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è Next Steps\n",
    "---\n",
    "\n",
    "In this guide, we built a Simple Multimodal RAG for Videos Using VideoDB, Llamaindex, and OpenAI\n",
    "\n",
    "You can optimize the pipeline by incorporating more advanced techniques like\n",
    "- Optimize Query Transformation\n",
    "- More methods to combine retrieved nodes from different modalities\n",
    "- Experiment with Different RAG pipelines like Knowledge Graph\n",
    "\n",
    "To learn more about Programable Stream feature that we used to create relevant clip checkout [Dynamic Video Stream Guide](https://docs.videodb.io/dynamic-video-stream-guide-44)\n",
    "\n",
    "\n",
    "To learn more about Scene Index, explore the following guides:\n",
    "\n",
    "- [Quickstart Guide](https://github.com/video-db/videodb-cookbook/blob/main/quickstart/Scene%20Index%20QuickStart.ipynb) \n",
    "- [Scene Extraction Options](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/playground_scene_extraction.ipynb)\n",
    "- [Advanced Visual Search](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/advanced_visual_search.ipynb)\n",
    "- [Custom Annotation Pipelines](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/custom_annotations.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Support & Community\n",
    "---\n",
    "\n",
    "If you have any questions or feedback. Feel free to reach out to us üôåüèº\n",
    "\n",
    "* [Discord](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fdiscord.gg%2Fpy9P639jGz)\n",
    "* [GitHub](https://github.com/video-db)\n",
    "* [Email](mailto:ashu@videodb.io)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
