{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/prometheus2_cookbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prometheus-2 Cookbook\n",
    "\n",
    "In this notebook we will demonstrate usage of [Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models](https://arxiv.org/abs/2405.01535).\n",
    "\n",
    "#### Abstract from the paper:\n",
    "\n",
    "Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs.\n",
    "\n",
    "#### Note: The base models for building Prometheus-2 are Mistral-7B and Mixtral8x7B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will demonstrate the usage of Prometheus-2 as evaluator for the following evaluators available with LlamaIndex:\n",
    "\n",
    "1. Pairwise Evaluator - Assesses whether the LLM would favor one response over another from two different query engines.\n",
    "2. Faithfulness Evaluator - Determines if the answer remains faithful to the retrieved contexts, indicating the absence of hallucination.\n",
    "3. Correctness Evaluator - Determines whether the generated answer matches the reference answer provided for the query, which requires labels.\n",
    "4. Relevancy Evaluator - Evaluates the relevance of retrieved contexts and the response to a query.\n",
    "\n",
    "*   If you're unfamiliar with the above evaluators, please refer to our [Evaluation Guide](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/) for more information.\n",
    "\n",
    "*   The prompts for the demonstration are inspired/ taken from the [promethues-eval](https://github.com/prometheus-eval/prometheus-eval/blob/main/libs/prometheus-eval/prometheus_eval/prompts.py) repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-llms-huggingface-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"  # OPENAI API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to the same event-loop\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from typing import Tuple, Optional\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data\n",
    "\n",
    "For the demonstration, we will utilize the PaulGrahamEssay dataset and define a sample query along with a reference answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import download_llama_dataset\n",
    "\n",
    "paul_graham_rag_dataset, paul_graham_documents = download_llama_dataset(\n",
    "    \"PaulGrahamEssayDataset\", \"./data/paul_graham\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Query and Reference(Ground truth) answer for the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = paul_graham_rag_dataset[0].query\n",
    "reference = paul_graham_rag_dataset[0].reference_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup LLM and Embedding model.\n",
    "\n",
    "You need to deploy the model on huggingface or can load it locally. Here we deployed it using HF Inference Endpoints.\n",
    "\n",
    "We will use OpenAI Embedding model and LLM for building Index, prometheus LLM for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "HF_TOKEN = \"YOUR HF TOKEN\"\n",
    "HF_ENDPOINT_URL = \"YOUR HF ENDPOINT URL\"\n",
    "\n",
    "prometheus_llm = HuggingFaceInferenceAPI(\n",
    "    model_name=HF_ENDPOINT_URL,\n",
    "    token=HF_TOKEN,\n",
    "    temperature=0.0,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1,\n",
    "    num_output=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "Settings.llm = OpenAI()\n",
    "Settings.embed_model = OpenAIEmbedding()\n",
    "Settings.chunk_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build two QueryEngines for pairwise evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import LabelledRagDataset\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "\n",
    "dataset_path = \"./data/paul_graham\"\n",
    "rag_dataset = LabelledRagDataset.from_json(f\"{dataset_path}/rag_dataset.json\")\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=f\"{dataset_path}/source_files\"\n",
    ").load_data()\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents=documents)\n",
    "\n",
    "query_engine1 = index.as_query_engine(similarity_top_k=1)\n",
    "\n",
    "query_engine2 = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = str(query_engine1.query(query))\n",
    "response2 = str(query_engine2.query(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The author mentions using the IBM 1401 computer for programming in his early experiences. The language he used was an early version of Fortran. One of the challenges he faced was the limited input options for programs, as the only form of input was data stored on punched cards, which he did not have access to. This limitation made it difficult for him to create programs that required specific input data.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The author mentions using the IBM 1401 computer for programming in his early experiences. The language he used was an early version of Fortran. One of the challenges he faced was the limited input options for programs, as the only form of input was data stored on punched cards, which he did not have access to. This limitation made it difficult for him to create programs that required specific input data, leading to a lack of meaningful programming experiences on the IBM 1401.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABS_SYSTEM_PROMPT = \"You are a fair judge assistant tasked with providing clear, objective feedback based on specific criteria, ensuring each assessment reflects the absolute standards set for performance.\"\n",
    "REL_SYSTEM_PROMPT = \"You are a fair judge assistant assigned to deliver insightful feedback that compares individual performances, highlighting how each stands relative to others within the same cohort.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prometheus_pairwise_eval_prompt_template = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of two responses strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, choose a better response between Response A and Response B. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (A or B)\"\n",
    "4. Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "###Instruction:\n",
    "Your task is to compare response A and Response B and give Feedback and score [RESULT] based on Rubric for the following query.\n",
    "{query}\n",
    "\n",
    "###Response A:\n",
    "{answer_1}\n",
    "\n",
    "###Response B:\n",
    "{answer_2}\n",
    "\n",
    "###Score Rubric:\n",
    "A: If Response A is better than Response B.\n",
    "B: If Response B is better than Response A.\n",
    "\n",
    "###Feedback: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_function(\n",
    "    outputs: str,\n",
    ") -> Tuple[Optional[bool], Optional[float], Optional[str]]:\n",
    "    parts = outputs.split(\"[RESULT]\")\n",
    "    if len(parts) == 2:\n",
    "        feedback, result = parts[0].strip(), parts[1].strip()\n",
    "        if result == \"A\":\n",
    "            return True, 0.0, feedback\n",
    "        elif result == \"B\":\n",
    "            return True, 1.0, feedback\n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import PairwiseComparisonEvaluator\n",
    "\n",
    "prometheus_pairwise_evaluator = PairwiseComparisonEvaluator(\n",
    "    llm=prometheus_llm,\n",
    "    parser_function=parser_function,\n",
    "    enforce_consensus=False,\n",
    "    eval_template=REL_SYSTEM_PROMPT\n",
    "    + \"\\n\\n\"\n",
    "    + prometheus_pairwise_eval_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_result = await prometheus_pairwise_evaluator.aevaluate(\n",
    "    query,\n",
    "    response=response1,\n",
    "    second_response=response2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationResult(query='In the essay, the author mentions his early experiences with programming. Describe the first computer he used for programming, the language he used, and the challenges he faced.', contexts=None, response=\"\\nBoth responses accurately describe the first computer the author used for programming, the language he used, and the challenges he faced. However, Response B provides a more comprehensive understanding of the challenges faced by the author. It not only mentions the limited input options but also connects this limitation to the author's lack of meaningful programming experiences on the IBM 1401. This additional context in Response B enhances the reader's understanding of the author's experiences and the impact of the challenges he faced. Therefore, based on the score rubric, Response B is better than Response A as it offers a more detailed and insightful analysis of the author's early programming experiences. \\n[RESULT] B\", passing=True, feedback=\"\\nBoth responses accurately describe the first computer the author used for programming, the language he used, and the challenges he faced. However, Response B provides a more comprehensive understanding of the challenges faced by the author. It not only mentions the limited input options but also connects this limitation to the author's lack of meaningful programming experiences on the IBM 1401. This additional context in Response B enhances the reader's understanding of the author's experiences and the impact of the challenges he faced. Therefore, based on the score rubric, Response B is better than Response A as it offers a more detailed and insightful analysis of the author's early programming experiences. \\n[RESULT] B\", score=1.0, pairwise_source='original', invalid_result=False, invalid_reason=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_result.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Both responses accurately describe the first computer the author used for programming, the language he used, and the challenges he faced. However, Response B provides a more comprehensive understanding of the challenges faced by the author. It not only mentions the limited input options but also connects this limitation to the author's lack of meaningful programming experiences on the IBM 1401. This additional context in Response B enhances the reader's understanding of the author's experiences and the impact of the challenges he faced. Therefore, based on the score rubric, Response B is better than Response A as it offers a more detailed and insightful analysis of the author's early programming experiences. \n",
       "[RESULT] B</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{pairwise_result.feedback}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "According to the feedback, the second response is preferred over the first response, with a score of 1.0 as per our parser function.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prometheus_correctness_eval_prompt_template = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a query, a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assesses the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is either 1 or 2 or 3 or 4 or 5. You should refer to the score rubric.\n",
    "3. The output format should only look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\"\n",
    "4. Please do not generate any other opening, closing, and explanations.\n",
    "5. Only evaluate on common things between generated answer and reference answer. Don't evaluate on things which are present in reference answer but not in generated answer.\n",
    "\n",
    "###Instruction:\n",
    "Your task is to evaluate the generated answer and reference answer for the following query:\n",
    "{query}\n",
    "\n",
    "###Generate answer to evaluate:\n",
    "{generated_answer}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "Score 1: If the generated answer is not relevant to the user query and reference answer.\n",
    "Score 2: If the generated answer is according to reference answer but not relevant to user query.\n",
    "Score 3: If the generated answer is relevant to the user query and reference answer but contains mistakes.\n",
    "Score 4: If the generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise.\n",
    "Score 5: If the generated answer is relevant to the user query and fully correct according to the reference answer.\n",
    "\n",
    "###Feedback:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import re\n",
    "\n",
    "\n",
    "def parser_function(output_str: str) -> Tuple[float, str]:\n",
    "    # Print result to backtrack\n",
    "    display(Markdown(f\"<b>{output_str}</b>\"))\n",
    "\n",
    "    # Pattern to match the feedback and response\n",
    "    # This pattern looks for any text ending with '[RESULT]' followed by a number\n",
    "    pattern = r\"(.+?) \\[RESULT\\] (\\d)\"\n",
    "\n",
    "    # Using regex to find all matches\n",
    "    matches = re.findall(pattern, output_str)\n",
    "\n",
    "    # Check if any match is found\n",
    "    if matches:\n",
    "        # Assuming there's only one match in the text, extract feedback and response\n",
    "        feedback, score = matches[0]\n",
    "        score = float(score.strip()) if score is not None else score\n",
    "        return score, feedback.strip()\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    ")\n",
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
    "\n",
    "\n",
    "# CorrectnessEvaluator with Prometheus model\n",
    "prometheus_correctness_evaluator = CorrectnessEvaluator(\n",
    "    llm=prometheus_llm,\n",
    "    parser_function=parser_function,\n",
    "    eval_template=ABS_SYSTEM_PROMPT\n",
    "    + \"\\n\\n\"\n",
    "    + prometheus_correctness_eval_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "The generated answer is relevant to the user query and the reference answer, as it correctly identifies the IBM 1401 as the first computer used for programming, the early version of Fortran as the programming language, and the challenge of limited input options. However, the response lacks the depth and detail found in the reference answer. For instance, it does not mention the specific age of the author when he started using the IBM 1401, nor does it provide examples of the types of programs he could not create due to the lack of input data. These omissions make the response less comprehensive than the reference answer. Therefore, while the generated answer is accurate and relevant, it is not as thorough as the reference answer. So the score is 4. [RESULT] 4</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "correctness_result = prometheus_correctness_evaluator.evaluate(\n",
    "    query=query,\n",
    "    response=response1,\n",
    "    reference=reference,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>4.0</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{correctness_result.score}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>True</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{correctness_result.passing}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The generated answer is relevant to the user query and the reference answer, as it correctly identifies the IBM 1401 as the first computer used for programming, the early version of Fortran as the programming language, and the challenge of limited input options. However, the response lacks the depth and detail found in the reference answer. For instance, it does not mention the specific age of the author when he started using the IBM 1401, nor does it provide examples of the types of programs he could not create due to the lack of input data. These omissions make the response less comprehensive than the reference answer. Therefore, while the generated answer is accurate and relevant, it is not as thorough as the reference answer. So the score is 4.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{correctness_result.feedback}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "Based on the feedback, the generated answer is relevant to the user query and matches the metrics of the reference answer precisely. However, it is not as concise, resulting in a score of 4.0. Despite this, the answer passes as True based on the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prometheus_faithfulness_eval_prompt_template = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), an information, a context, and a score rubric representing evaluation criteria are given.\n",
    "1. You are provided with evaluation task with the help of information, context information to give result based on score rubrics.\n",
    "2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general.\n",
    "3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric.\n",
    "4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)”\n",
    "5. Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "###The instruction to evaluate: Your task is to evaluate if the given piece of information is supported by context.\n",
    "\n",
    "###Information:\n",
    "{query_str}\n",
    "\n",
    "###Context:\n",
    "{context_str}\n",
    "\n",
    "###Score Rubrics:\n",
    "Score YES: If the given piece of information is supported by context.\n",
    "Score NO: If the given piece of information is not supported by context\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "prometheus_faithfulness_refine_prompt_template = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a information, a context information, an existing answer, and a score rubric representing a evaluation criteria are given.\n",
    "1. You are provided with evaluation task with the help of information, context information and an existing answer.\n",
    "2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general.\n",
    "3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric.\n",
    "4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\"\n",
    "5. Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "###The instruction to evaluate: If the information is present in the context and also provided with an existing answer.\n",
    "\n",
    "###Existing answer:\n",
    "{existing_answer}\n",
    "\n",
    "###Information:\n",
    "{query_str}\n",
    "\n",
    "###Context:\n",
    "{context_msg}\n",
    "\n",
    "###Score Rubrics:\n",
    "Score YES: If the existing answer is already YES or If the Information is present in the context.\n",
    "Score NO: If the existing answer is NO and If the Information is not present in the context.\n",
    "\n",
    "###Feedback: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FaithfulnessEvaluator with Prometheus model\n",
    "prometheus_faithfulness_evaluator = FaithfulnessEvaluator(\n",
    "    llm=prometheus_llm,\n",
    "    eval_template=ABS_SYSTEM_PROMPT\n",
    "    + \"\\n\\n\"\n",
    "    + prometheus_faithfulness_eval_prompt_template,\n",
    "    refine_template=ABS_SYSTEM_PROMPT\n",
    "    + \"\\n\\n\"\n",
    "    + prometheus_faithfulness_refine_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_vector = query_engine1.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_result = prometheus_faithfulness_evaluator.evaluate_response(\n",
    "    response=response_vector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faithfulness_result.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faithfulness_result.passing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "The score and passing denotes there is no hallucination observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevancy Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prometheus_relevancy_eval_prompt_template = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a query with response, context, and a score rubric representing evaluation criteria are given.\n",
    "1. You are provided with evaluation task with the help of a query with response and context.\n",
    "2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general.\n",
    "3. After writing a feedback, write a score that is A or B. You should refer to the score rubric.\n",
    "4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)”\n",
    "5. Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "###The instruction to evaluate: Your task is to evaluate if the response for the query is in line with the context information provided.\n",
    "\n",
    "###Query and Response:\n",
    "{query_str}\n",
    "\n",
    "###Context:\n",
    "{context_str}\n",
    "\n",
    "###Score Rubrics:\n",
    "Score YES: If the response for the query is in line with the context information provided.\n",
    "Score NO: If the response for the query is not in line with the context information provided.\n",
    "\n",
    "###Feedback: \"\"\"\n",
    "\n",
    "prometheus_relevancy_refine_prompt_template = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a query with response, context, an existing answer, and a score rubric representing a evaluation criteria are given.\n",
    "1. You are provided with evaluation task with the help of a query with response and context and an existing answer.\n",
    "2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general.\n",
    "3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric.\n",
    "4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\"\n",
    "5. Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "###The instruction to evaluate: Your task is to evaluate if the response for the query is in line with the context information provided.\n",
    "\n",
    "###Query and Response:\n",
    "{query_str}\n",
    "\n",
    "###Context:\n",
    "{context_str}\n",
    "\n",
    "###Score Rubrics:\n",
    "Score YES: If the existing answer is already YES or If the response for the query is in line with the context information provided.\n",
    "Score NO: If the existing answer is NO and If the response for the query is in line with the context information provided.\n",
    "\n",
    "###Feedback: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RelevancyEvaluator with Prometheus model\n",
    "prometheus_relevancy_evaluator = RelevancyEvaluator(\n",
    "    llm=prometheus_llm,\n",
    "    eval_template=ABS_SYSTEM_PROMPT\n",
    "    + \"\\n\\n\"\n",
    "    + prometheus_relevancy_eval_prompt_template,\n",
    "    refine_template=ABS_SYSTEM_PROMPT\n",
    "    + \"\\n\\n\"\n",
    "    + prometheus_relevancy_refine_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevancy_result = prometheus_relevancy_evaluator.evaluate_response(\n",
    "    query=query, response=response_vector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevancy_result.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevancy_result.passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "The response provided is in line with the context information given. It accurately describes the first computer used for programming, the language used, and the challenges faced by the author. The IBM 1401 computer is correctly identified as the first computer used for programming, and the early version of Fortran is mentioned as the language used. The challenges faced by the author, such as the limited input options and the difficulty of creating meaningful programs, are also accurately described. The response is concise and directly addresses the query, providing a clear and relevant answer. Therefore, based on the score rubric, the response is in line with the context information provided. \n",
       "[RESULT] YES</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{relevancy_result.feedback}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "The feedback indicates that the response to the query aligns well with the provided context information, resulting in a score of 1.0 and passing status of True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "Exploring Prometheus-2 for OSS evaluation is interesting.\n",
    "\n",
    "The feedback is in the expected format, making parsing and decision-making easier.\n",
    "\n",
    "It's valuable to compare with GPT-4 for evaluation purposes and consider using Prometheus-2 in evaluations.\n",
    "\n",
    "You can refer to our [guide](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/evaluation/prometheus_evaluation.ipynb) on comparing GPT-4 as an evaluator with the OSS evaluation model for experimentation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
