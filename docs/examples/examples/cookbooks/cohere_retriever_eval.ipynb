{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a5706df",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/cohere_retriever_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36129c05-81f2-466c-a507-b62a577199d8",
   "metadata": {},
   "source": [
    "# Cohere init8 and binary Embeddings Retrieval Evaluation\n",
    "\n",
    "Cohere Embed is the first embedding model that natively supports float, int8, binary and ubinary embeddings. Refer to their [main blog post](https://txt.cohere.com/int8-binary-embeddings/) for more details on Cohere int8 & binary Embeddings.\n",
    "\n",
    "This notebook helps you to evaluate these different embedding types and pick one for your RAG pipeline. It uses our `RetrieverEvaluator` to evaluate the quality of the embeddings using the Retriever module LlamaIndex.\n",
    "\n",
    "Observed Metrics:\n",
    "\n",
    "1. Hit-Rate\n",
    "2. MRR (Mean-Reciprocal-Rank)\n",
    "\n",
    "For any given question, these will compare the quality of retrieved results from the ground-truth context. The eval dataset is created using our synthetic dataset generation module. We will use GPT-4 for dataset generation to avoid bias.\n",
    "\n",
    "# Note: The results shown at the end of the notebook are very specific to dataset, and various other parameters considered. We recommend you to use the notebook as reference to experiment on your dataset and evaluate the usage of different embedding types in your RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f54aff-edaf-4f99-9aa7-9c91a6cabaf8",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df86266",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-openai\n",
    "%pip install llama-index-embeddings-cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e98d46-1ac6-4fad-8004-64e4d4dcf47e",
   "metadata": {},
   "source": [
    "## Setup API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a03e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI KEY\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"YOUR COHEREAI API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8659681a-7141-4f80-9bbe-8eddc061a134",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Here we load in data (PG essay), parse into Nodes. We then index this data using our simple vector index and get a retriever for the following different embedding types.\n",
    "\n",
    "1. `float`\n",
    "2. `int8`\n",
    "3. `binary`\n",
    "4. `ubinary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6fecf4-7215-4ae9-b02b-3cb7c6000f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63b16c-6a83-4ef0-a451-43c2c3d9c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import generate_question_context_pairs\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.cohere import CohereEmbedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e3b3f28",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c112d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-27 20:26:33--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n",
      "\n",
      "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2024-03-27 20:26:34 (2.18 MB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af83cac0-2d08-467e-a88c-7370472eec87",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab50ac91-e9d4-4fae-a519-db5711a13210",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1beff2-c9e9-4b67-b5f8-4530df0d356f",
   "metadata": {},
   "source": [
    "## Create Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66499039-d76d-4914-b03a-bcbd10c8c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SentenceSplitter(chunk_size=512)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af31b424-02c4-4731-beca-e88ef4f202ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default, the node ids are set to random uuids. To ensure same id's per run, we manually set them.\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"node_{idx}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4d0a3-7ab3-48a0-89c3-6e7abc6d82de",
   "metadata": {},
   "source": [
    "## Create retrievers for different embedding types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1268ad2-3b29-49dc-92b3-6894900b4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm for question generation\n",
    "# Take any other llm other than from cohereAI to avoid bias.\n",
    "llm = OpenAI(model=\"gpt-4\")\n",
    "\n",
    "\n",
    "# Function to return embedding model\n",
    "def cohere_embedding(\n",
    "    model_name: str, input_type: str, embedding_type: str\n",
    ") -> CohereEmbedding:\n",
    "    return CohereEmbedding(\n",
    "        api_key=os.environ[\"COHERE_API_KEY\"],\n",
    "        model_name=model_name,\n",
    "        input_type=input_type,\n",
    "        embedding_type=embedding_type,\n",
    "    )\n",
    "\n",
    "\n",
    "# Function to return retriver for different embedding type embedding model\n",
    "def retriver(nodes, embedding_type=\"float\", model_name=\"embed-english-v3.0\"):\n",
    "    vector_index = VectorStoreIndex(\n",
    "        nodes,\n",
    "        embed_model=cohere_embedding(\n",
    "            model_name, \"search_document\", embedding_type\n",
    "        ),\n",
    "    )\n",
    "    retriever = vector_index.as_retriever(\n",
    "        similarity_top_k=2,\n",
    "        embed_model=cohere_embedding(\n",
    "            model_name, \"search_query\", embedding_type\n",
    "        ),\n",
    "    )\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11836d3f-136d-45fe-bad8-f0480751ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build retriever for float embedding type\n",
    "retriver_float = retriver(nodes)\n",
    "\n",
    "# Build retriever for int8 embedding type\n",
    "retriver_int8 = retriver(nodes, \"int8\")\n",
    "\n",
    "# Build retriever for binary embedding type\n",
    "retriver_binary = retriver(nodes, \"binary\")\n",
    "\n",
    "# Build retriever for ubinary embedding type\n",
    "retriver_ubinary = retriver(nodes, \"ubinary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf75ec6-7419-4975-83df-6d6fa08adb77",
   "metadata": {},
   "source": [
    "### Try out Retrieval\n",
    "\n",
    "We'll try out retrieval over a sample query with `float` retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd260990-0aea-490b-99e0-d7517f668020",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = retriver_float.retrieve(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b0823-8be4-4dd4-8486-8e73d79590fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** node_2<br>**Similarity:** 0.3641554823852197<br>**Text:** I remember vividly how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.\n",
       "\n",
       "Computers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he'd write 2 pages at a time and then print them out, but it was a lot better than a typewriter.\n",
       "\n",
       "Though I liked programming, I didn't plan to study it in college. In college I was going to study philosophy, which sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to which the things studied in other fields would be mere domain knowledg...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** node_0<br>**Similarity:** 0.36283154406791923<br>**Text:** What I Worked On\n",
       "\n",
       "February 2021\n",
       "\n",
       "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
       "\n",
       "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n",
       "\n",
       "The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in ...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "for node in retrieved_nodes:\n",
    "    display_source_node(node, source_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5371db56-2b1c-497a-8fd0-a1a69b2ce773",
   "metadata": {},
   "source": [
    "## Evaluation dataset - Synthetic Dataset Generation of (query, context) pairs\n",
    "\n",
    "Here we build a simple evaluation dataset over the existing text corpus.\n",
    "\n",
    "We use our `generate_question_context_pairs` to generate a set of (question, context) pairs over a given unstructured text corpus. This uses the LLM to auto-generate questions from each context chunk.\n",
    "\n",
    "We get back a `EmbeddingQAFinetuneDataset` object. At a high-level this contains a set of ids mapping to queries and relevant doc chunks, as well as the corpus itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25924cf-7eeb-4160-a035-4a69ee1e46de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29a159-9a4f-4d44-9c0d-1cd683f8bb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [04:10<00:00,  4.24s/it]\n"
     ]
    }
   ],
   "source": [
    "qa_dataset = generate_question_context_pairs(\n",
    "    nodes, llm=llm, num_questions_per_chunk=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32d458b-50ad-426c-a969-e9fe8fb5861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Describe the author's initial experiences with programming on the IBM 1401. What were some of the challenges he faced and how did these experiences shape his understanding of programming?\"\n"
     ]
    }
   ],
   "source": [
    "queries = qa_dataset.queries.values()\n",
    "print(list(queries)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a900650-38ed-405e-936c-08e48e0fb8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [optional] save\n",
    "qa_dataset.save_json(\"pg_eval_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713c1b71-2ab6-42a0-bde3-ab3bfe880f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [optional] load\n",
    "qa_dataset = EmbeddingQAFinetuneDataset.from_json(\"pg_eval_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3267fd6-187c-4e11-9f80-cfce08d98f1f",
   "metadata": {},
   "source": [
    "## Use `RetrieverEvaluator` for Retrieval Evaluation\n",
    "\n",
    "We're now ready to run our retrieval evals. We'll run our `RetrieverEvaluator` over the eval dataset that we generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d9ac28-672a-4a5b-be81-fb97e92a1e64",
   "metadata": {},
   "source": [
    "### Define `RetrieverEvaluator` for different embedding_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fa6f9c-a6da-43c4-b4c6-748ada393490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "\n",
    "metrics = [\"mrr\", \"hit_rate\"]\n",
    "\n",
    "# Retrieval evaluator for float embedding type\n",
    "retriever_evaluator_float = RetrieverEvaluator.from_metric_names(\n",
    "    metrics, retriever=retriver_float\n",
    ")\n",
    "\n",
    "# Retrieval evaluator for int8 embedding type\n",
    "retriever_evaluator_int8 = RetrieverEvaluator.from_metric_names(\n",
    "    metrics, retriever=retriver_int8\n",
    ")\n",
    "\n",
    "# Retrieval evaluator for binary embedding type\n",
    "retriever_evaluator_binary = RetrieverEvaluator.from_metric_names(\n",
    "    metrics, retriever=retriver_binary\n",
    ")\n",
    "\n",
    "# Retrieval evaluator for ubinary embedding type\n",
    "retriever_evaluator_ubinary = RetrieverEvaluator.from_metric_names(\n",
    "    metrics, retriever=retriver_ubinary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f3351-d745-46b3-b53b-916f7c244def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: \"Describe the author's initial experiences with programming on the IBM 1401. What were some of the challenges he faced and how did these experiences shape his understanding of programming?\"\n",
      "Metrics: {'mrr': 0.5, 'hit_rate': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try it out on a sample query\n",
    "sample_id, sample_query = list(qa_dataset.queries.items())[0]\n",
    "sample_expected = qa_dataset.relevant_docs[sample_id]\n",
    "\n",
    "eval_result = retriever_evaluator_float.evaluate(sample_query, sample_expected)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3963d146-c4a3-4b00-8b53-52c6ec03d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on the entire dataset\n",
    "\n",
    "# float embedding type\n",
    "eval_results_float = await retriever_evaluator_float.aevaluate_dataset(\n",
    "    qa_dataset\n",
    ")\n",
    "\n",
    "# int8 embedding type\n",
    "eval_results_int8 = await retriever_evaluator_int8.aevaluate_dataset(\n",
    "    qa_dataset\n",
    ")\n",
    "\n",
    "# binary embedding type\n",
    "eval_results_binary = await retriever_evaluator_binary.aevaluate_dataset(\n",
    "    qa_dataset\n",
    ")\n",
    "\n",
    "# ubinary embedding type\n",
    "eval_results_ubinary = await retriever_evaluator_ubinary.aevaluate_dataset(\n",
    "    qa_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bcf25d-f6ed-4e1d-b298-b7e318791865",
   "metadata": {},
   "source": [
    "#### Define `display_results` to get the display the results in dataframe with each retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc38c7-660e-451b-8305-e8a7f23510a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def display_results(name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    hit_rate = full_df[\"hit_rate\"].mean()\n",
    "    mrr = full_df[\"mrr\"].mean()\n",
    "    columns = {\"Embedding Type\": [name], \"hit_rate\": [hit_rate], \"mrr\": [mrr]}\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b184508e-c57a-4f07-ab27-e51ff78ed8be",
   "metadata": {},
   "source": [
    "## Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d059d5ee-d2aa-4edf-8b9b-e09d29ff17b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics for float embedding type\n",
    "metrics_float = display_results(\"float\", eval_results_float)\n",
    "\n",
    "# metrics for int8 embedding type\n",
    "metrics_int8 = display_results(\"int8\", eval_results_int8)\n",
    "\n",
    "# metrics for binary embedding type\n",
    "metrics_binary = display_results(\"binary\", eval_results_binary)\n",
    "\n",
    "# metrics for ubinary embedding type\n",
    "metrics_ubinary = display_results(\"ubinary\", eval_results_ubinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb224e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_metrics = pd.concat(\n",
    "    [metrics_float, metrics_int8, metrics_binary, metrics_ubinary]\n",
    ")\n",
    "combined_metrics.set_index([\"Embedding Type\"], append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c0c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Embedding Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">0</th>\n",
       "      <th>float</th>\n",
       "      <td>0.805085</td>\n",
       "      <td>0.665254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int8</th>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.673729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>binary</th>\n",
       "      <td>0.491525</td>\n",
       "      <td>0.394068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ubinary</th>\n",
       "      <td>0.449153</td>\n",
       "      <td>0.377119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  hit_rate       mrr\n",
       "  Embedding Type                    \n",
       "0 float           0.805085  0.665254\n",
       "  int8            0.813559  0.673729\n",
       "  binary          0.491525  0.394068\n",
       "  ubinary         0.449153  0.377119"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658e5e99-921c-4c5b-9dd9-3c3439a88b2c",
   "metadata": {},
   "source": [
    "# Note: The results shown above are very specific to dataset, and various other parameters considered. We recommend you to use the notebook as reference to experiment on your dataset and evaluate the usage of different embedding types in your RAG pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
