{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/codestral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codestral from MistralAI Cookbook\n",
    "\n",
    "MistralAI released [codestral-latest](https://mistral.ai/news/codestral/) - a code model.\n",
    "\n",
    "Codestral is a new code model from mistralai tailored for code generation, fluent in over 80 programming languages. It simplifies coding tasks by completing functions, writing tests, and filling in code snippets, enhancing developer efficiency and reducing errors. Codestral operates through a unified API endpoint, making it a versatile tool for software development.\n",
    "\n",
    "This cookbook showcases how to use the `codestral-latest` model with llama-index. It guides you through using the Codestral fill-in-the-middle and instruct endpoints."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"<YOUR MISTRAL API KEY>\"\n",
    "\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "\n",
    "llm = MistralAI(model=\"codestral-latest\", temperature=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruct mode usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a function for fibonacci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Sure, here is a simple Python function that calculates the nth number in the Fibonacci sequence:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        print(\"Input should be positive integer.\")\n",
      "    elif n == 1:\n",
      "        return 0\n",
      "    elif n == 2:\n",
      "        return 1\n",
      "    else:\n",
      "        a, b = 0, 1\n",
      "        for i in range(2, n):\n",
      "            a, b = b, a + b\n",
      "        return b\n",
      "```\n",
      "\n",
      "You can use this function to find the nth number in the Fibonacci sequence by calling `fibonacci(n)`, where `n` is the position of the number you want to find. For example, `fibonacci(10)` will return the 10th number in the Fibonacci sequence.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [ChatMessage(role=\"user\", content=\"Write a function for fibonacci\")]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a function to build RAG pipeline using LlamaIndex.\n",
    "\n",
    "Note: The output is mostly accurate, but it is based on an older LlamaIndex package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Sure, I can help you with that. Here's a basic example of how you can build a Retrieval Augmented Generation (RAG) pipeline using LlamaIndex. This example assumes that you have a list of documents.\n",
      "\n",
      "```python\n",
      "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
      "\n",
      "def build_rag_pipeline(documents_path):\n",
      "    # Load documents\n",
      "    documents = SimpleDirectoryReader(documents_path).load_data()\n",
      "\n",
      "    # Create index\n",
      "    index = VectorStoreIndex.from_documents(documents)\n",
      "\n",
      "    # Create query engine\n",
      "    query_engine = index.as_query_engine()\n",
      "\n",
      "    return query_engine\n",
      "\n",
      "# Usage\n",
      "query_engine = build_rag_pipeline(\"path_to_your_documents\")\n",
      "response = query_engine.query(\"Your query here\")\n",
      "print(response)\n",
      "```\n",
      "\n",
      "In this code:\n",
      "\n",
      "1. We first import the necessary classes from LlamaIndex.\n",
      "2. We define a function `build_rag_pipeline` that takes a path to a directory of documents as input.\n",
      "3. We load the documents using `SimpleDirectoryReader`.\n",
      "4. We create an index from the documents using `VectorStoreIndex.from_documents`.\n",
      "5. We create a query engine from the index using `index.as_query_engine`.\n",
      "6. Finally, we return the query engine.\n",
      "\n",
      "You can use the query engine to ask questions about the documents. The query engine will use the index to retrieve relevant documents and then generate a response based on those documents.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        content=\"Write a function to build RAG pipeline using LlamaIndex.\",\n",
    "    )\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill-in-the-middle\n",
    "\n",
    "This feature allows users to set a starting point with a prompt and an optional ending with a suffix and stop. The Codestral model then generates the intervening code, perfect for tasks requiring specific code generation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill the code with start and end of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def multiply(\n",
      "a, b):\n",
      "  \"\"\"\n",
      "  This function multiplies two numbers\n",
      "  \"\"\"\n",
      "  \n",
      "return a*b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"def multiply(\"\n",
    "suffix = \"return a*b\"\n",
    "\n",
    "response = llm.fill_in_middle(prompt, suffix)\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "{prompt}\n",
    "{response.text}\n",
    "{suffix}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill the code with start, end of the code and stop tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def multiply(a,\n",
      " b):\n",
      "\n",
      "    return a * b\n",
      "\n",
      "# test the function\n",
      "print(multiply(2, 3))  # should print 6\n",
      "print(multiply(-1, 5))  # should print -5\n",
      "print(multiply(0, 99))  # should print 0\n",
      "\n",
      "# we can also test the function with large numbers\n",
      "print(multiply(123456789, 987654321))  # should print 121932631132635269\n",
      "\n",
      "# the function should also work with floating point numbers\n",
      "print(multiply(3.14, 2.71))  # should print approximately 8.5392\n",
      "\n",
      "# the function should also work with negative floating point numbers\n",
      "print(multiply(-3.14, 2.71))  # should print approximately -8.5392\n",
      "\n",
      "# the function should also work with mixed types (integer and floating point)\n",
      "print(multiply(2, 3.14))  # should print approximately 6.28\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"def multiply(a,\"\n",
    "suffix = \"\"\n",
    "stop = [\"\\n\\n\\n\"]\n",
    "\n",
    "response = llm.fill_in_middle(prompt, suffix, stop)\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "{prompt}\n",
    "{response.text}\n",
    "{suffix}\n",
    "\"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anthropic_env",
   "language": "python",
   "name": "anthropic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
