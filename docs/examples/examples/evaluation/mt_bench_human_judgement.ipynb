{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc26779",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/evaluation/mt_bench_human_judgement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44792f-924f-4aaf-b414-30e23f491454",
   "metadata": {},
   "source": [
    "# Benchmarking LLM Evaluators On The MT-Bench Human Judgement `LabelledPairwiseEvaluatorDataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfad227-fe26-4cdb-b181-2384e8d1bba0",
   "metadata": {},
   "source": [
    "In this notebook guide, we benchmark Gemini and GPT models as LLM evaluators using a slightly adapted version of the MT-Bench Human Judgements dataset. For this dataset, human evaluators compare two llm model responses to a given query and rank them according to their own preference. In the original version, there can be more than one human evaluator for a given example (query, two model responses). In the adapted version that we consider however, we aggregate these 'repeated' entries and convert the 'winner' column of the original schema to instead represent the proportion of times 'model_a' wins across all of the human evaluators. To adapt this to a llama-dataset, and to better consider ties (albeit with small samples) we set an uncertainty threshold for this proportion in that if it is between [0.4, 0.6] then we consider there to be no winner between the two models. We download this dataset from [llama-hub](https://llamahub.ai). Finally, the LLMs that we benchmark are listed below:\n",
    "\n",
    "1. GPT-3.5 (OpenAI)\n",
    "2. GPT-4 (OpenAI)\n",
    "3. Gemini-Pro (Google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a734fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-openai\n",
    "%pip install llama-index-llms-cohere\n",
    "%pip install llama-index-llms-gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95120632-50cf-4844-bedd-7d31567a0d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"google-generativeai\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e0eaca-4eeb-408d-ac89-5eb6338c7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d117804-1c19-48ca-b995-7e6fbadbb10c",
   "metadata": {},
   "source": [
    "### Load In Dataset\n",
    "\n",
    "Let's load in the llama-dataset from llama-hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a168fe64-dd85-4113-8f04-95a221696ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import download_llama_dataset\n",
    "\n",
    "# download dataset\n",
    "pairwise_evaluator_dataset, _ = download_llama_dataset(\n",
    "    \"MtBenchHumanJudgementDataset\", \"./mt_bench_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941607dd-99ec-443a-9b46-5e8842a20828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>second_answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth_answer</th>\n",
       "      <th>query_by</th>\n",
       "      <th>answer_by</th>\n",
       "      <th>second_answer_by</th>\n",
       "      <th>ground_truth_answer_by</th>\n",
       "      <th>reference_feedback</th>\n",
       "      <th>reference_score</th>\n",
       "      <th>reference_evaluation_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Compose an engaging travel blog post about a r...</td>\n",
       "      <td>I recently had the pleasure of visiting Hawaii...</td>\n",
       "      <td>Aloha! I recently had the pleasure of embarkin...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>human</td>\n",
       "      <td>ai (alpaca-13b)</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Compose an engaging travel blog post about a r...</td>\n",
       "      <td>I recently had the pleasure of visiting Hawaii...</td>\n",
       "      <td>Aloha and welcome to my travel blog post about...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>human</td>\n",
       "      <td>ai (alpaca-13b)</td>\n",
       "      <td>ai (vicuna-13b-v1.2)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Compose an engaging travel blog post about a r...</td>\n",
       "      <td>Here is a draft travel blog post about a recen...</td>\n",
       "      <td>I recently had the pleasure of visiting Hawaii...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>human</td>\n",
       "      <td>ai (claude-v1)</td>\n",
       "      <td>ai (alpaca-13b)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Compose an engaging travel blog post about a r...</td>\n",
       "      <td>Here is a draft travel blog post about a recen...</td>\n",
       "      <td>Here is a travel blog post about a recent trip...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>human</td>\n",
       "      <td>ai (claude-v1)</td>\n",
       "      <td>ai (llama-13b)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Compose an engaging travel blog post about a r...</td>\n",
       "      <td>Aloha! I recently had the pleasure of embarkin...</td>\n",
       "      <td>I recently had the pleasure of visiting Hawaii...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>human</td>\n",
       "      <td>ai (gpt-3.5-turbo)</td>\n",
       "      <td>ai (alpaca-13b)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  Compose an engaging travel blog post about a r...   \n",
       "1  Compose an engaging travel blog post about a r...   \n",
       "2  Compose an engaging travel blog post about a r...   \n",
       "3  Compose an engaging travel blog post about a r...   \n",
       "4  Compose an engaging travel blog post about a r...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  I recently had the pleasure of visiting Hawaii...   \n",
       "1  I recently had the pleasure of visiting Hawaii...   \n",
       "2  Here is a draft travel blog post about a recen...   \n",
       "3  Here is a draft travel blog post about a recen...   \n",
       "4  Aloha! I recently had the pleasure of embarkin...   \n",
       "\n",
       "                                       second_answer contexts  \\\n",
       "0  Aloha! I recently had the pleasure of embarkin...     None   \n",
       "1  Aloha and welcome to my travel blog post about...     None   \n",
       "2  I recently had the pleasure of visiting Hawaii...     None   \n",
       "3  Here is a travel blog post about a recent trip...     None   \n",
       "4  I recently had the pleasure of visiting Hawaii...     None   \n",
       "\n",
       "  ground_truth_answer query_by           answer_by      second_answer_by  \\\n",
       "0                None    human     ai (alpaca-13b)    ai (gpt-3.5-turbo)   \n",
       "1                None    human     ai (alpaca-13b)  ai (vicuna-13b-v1.2)   \n",
       "2                None    human      ai (claude-v1)       ai (alpaca-13b)   \n",
       "3                None    human      ai (claude-v1)        ai (llama-13b)   \n",
       "4                None    human  ai (gpt-3.5-turbo)       ai (alpaca-13b)   \n",
       "\n",
       "  ground_truth_answer_by reference_feedback  reference_score  \\\n",
       "0                   None               None              0.0   \n",
       "1                   None               None              0.0   \n",
       "2                   None               None              1.0   \n",
       "3                   None               None              1.0   \n",
       "4                   None               None              1.0   \n",
       "\n",
       "  reference_evaluation_by  \n",
       "0                   human  \n",
       "1                   human  \n",
       "2                   human  \n",
       "3                   human  \n",
       "4                   human  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_evaluator_dataset.to_pandas()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f234425b-1e8d-44e9-ab57-5678df1a112d",
   "metadata": {},
   "source": [
    "### Define Our Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa0a9d-6005-49fa-9854-e093935657a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import PairwiseComparisonEvaluator\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.llms.cohere import Cohere\n",
    "\n",
    "\n",
    "llm_gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "llm_gpt35 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "llm_gemini = Gemini(model=\"models/gemini-pro\", temperature=0)\n",
    "\n",
    "evaluators = {\n",
    "    \"gpt-4\": PairwiseComparisonEvaluator(llm=llm_gpt4),\n",
    "    \"gpt-3.5\": PairwiseComparisonEvaluator(llm=llm_gpt35),\n",
    "    \"gemini-pro\": PairwiseComparisonEvaluator(llm=llm_gemini),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1af05-113f-4886-aa0e-8664f271d8fa",
   "metadata": {},
   "source": [
    "### Benchmark With `EvaluatorBenchmarkerPack` (llama-pack)\n",
    "\n",
    "To compare our four evaluators we will benchmark them against `MTBenchHumanJudgementDataset`, wherein references are provided by human evaluators. The benchmarks will return the following quantites:\n",
    "\n",
    "- `number_examples`: The number of examples the dataset consists of.\n",
    "- `invalid_predictions`: The number of evaluations that could not yield a final evaluation (e.g., due to inability to parse the evaluation output, or an exception thrown by the LLM evaluator)\n",
    "- `inconclusives`: Since this is a pairwise comparison, to mitigate the risk for \"position bias\" we conduct two evaluations — one with original order of presenting the two model answers, and another with the order in which these answers are presented to the evaluator LLM is flipped. A result is inconclusive if the LLM evaluator in the second ordering flips its vote in relation to the first vote.\n",
    "- `ties`: A `PairwiseComparisonEvaluator` can also return a \"tie\" result. This is the number of examples for which it gave a tie result.\n",
    "- `agreement_rate_with_ties`: The rate at which the LLM evaluator agrees with the reference (in this case human) evaluator, when also including ties. The denominator used to compute this metric is given by: `number_examples - invalid_predictions - inconclusives`.\n",
    "- `agreement_rate_without_ties`: The rate at which the LLM evaluator agress with the reference (in this case human) evaluator, when excluding and ties. The denominator used to compute this metric is given by: `number_examples - invalid_predictions - inconclusives - ties`.\n",
    "\n",
    "To compute these metrics, we'll make use of the `EvaluatorBenchmarkerPack`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8296a18a-5119-4271-a74e-4e17c03355fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "\n",
    "EvaluatorBenchmarkerPack = download_llama_pack(\n",
    "    \"EvaluatorBenchmarkerPack\", \"./pack\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592153a3-a082-4391-b071-6402b9ee797a",
   "metadata": {},
   "source": [
    "#### GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cde4d3-aeb1-4d60-88bd-9f058283551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_benchmarker = EvaluatorBenchmarkerPack(\n",
    "    evaluator=evaluators[\"gpt-3.5\"],\n",
    "    eval_dataset=pairwise_evaluator_dataset,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c98d4cd-353f-4753-b520-c4a9f6df9350",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_3p5_benchmark_df = await evaluator_benchmarker.arun(\n",
    "    batch_size=100, sleep_time_in_seconds=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a5acd5-bd8b-409e-94b0-7a3e6a8bc5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_examples</th>\n",
       "      <th>invalid_predictions</th>\n",
       "      <th>inconclusives</th>\n",
       "      <th>ties</th>\n",
       "      <th>agreement_rate_with_ties</th>\n",
       "      <th>agreement_rate_without_ties</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-3.5</th>\n",
       "      <td>1204</td>\n",
       "      <td>82</td>\n",
       "      <td>393</td>\n",
       "      <td>56</td>\n",
       "      <td>0.736626</td>\n",
       "      <td>0.793462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         number_examples  invalid_predictions  inconclusives  ties  \\\n",
       "gpt-3.5             1204                   82            393    56   \n",
       "\n",
       "         agreement_rate_with_ties  agreement_rate_without_ties  \n",
       "gpt-3.5                  0.736626                     0.793462  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_3p5_benchmark_df.index = [\"gpt-3.5\"]\n",
    "gpt_3p5_benchmark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c3b1f3-50db-419c-987e-6f74a9e93b46",
   "metadata": {},
   "source": [
    "#### GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4122606e-6490-4d65-aaab-a55d4702f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_benchmarker = EvaluatorBenchmarkerPack(\n",
    "    evaluator=evaluators[\"gpt-4\"],\n",
    "    eval_dataset=pairwise_evaluator_dataset,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7381359-ce0b-4a1a-941d-829ce1151e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_benchmark_df = await evaluator_benchmarker.arun(\n",
    "    batch_size=100, sleep_time_in_seconds=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d858adbe-1140-4da9-85fc-5c6f009cb733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_examples</th>\n",
       "      <th>invalid_predictions</th>\n",
       "      <th>inconclusives</th>\n",
       "      <th>ties</th>\n",
       "      <th>agreement_rate_with_ties</th>\n",
       "      <th>agreement_rate_without_ties</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-4</th>\n",
       "      <td>1204</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>103</td>\n",
       "      <td>0.701087</td>\n",
       "      <td>0.77023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       number_examples  invalid_predictions  inconclusives  ties  \\\n",
       "gpt-4             1204                    0            100   103   \n",
       "\n",
       "       agreement_rate_with_ties  agreement_rate_without_ties  \n",
       "gpt-4                  0.701087                      0.77023  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_4_benchmark_df.index = [\"gpt-4\"]\n",
    "gpt_4_benchmark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b145b017-ebc8-4bf9-8e39-5e6b5b0de8d4",
   "metadata": {},
   "source": [
    "### Gemini Pro\n",
    "\n",
    "NOTE: The rate limit for Gemini models is still very constraining, which is understandable given that they've just been released at the time of writing this notebook. So, we use a very small `batch_size` and moderately high `sleep_time_in_seconds` to reduce risk of getting rate-limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af501e5-9c83-4bc2-91c0-85f1f9561506",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_benchmarker = EvaluatorBenchmarkerPack(\n",
    "    evaluator=evaluators[\"gemini-pro\"],\n",
    "    eval_dataset=pairwise_evaluator_dataset,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730b655-917a-4b31-be57-e2397d811567",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_pro_benchmark_df = await evaluator_benchmarker.arun(\n",
    "    batch_size=5, sleep_time_in_seconds=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853944a-4d88-44cb-8bab-91dcfd99d900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_examples</th>\n",
       "      <th>invalid_predictions</th>\n",
       "      <th>inconclusives</th>\n",
       "      <th>ties</th>\n",
       "      <th>agreement_rate_with_ties</th>\n",
       "      <th>agreement_rate_without_ties</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gemini-pro</th>\n",
       "      <td>1204</td>\n",
       "      <td>2</td>\n",
       "      <td>295</td>\n",
       "      <td>60</td>\n",
       "      <td>0.742007</td>\n",
       "      <td>0.793388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            number_examples  invalid_predictions  inconclusives  ties  \\\n",
       "gemini-pro             1204                    2            295    60   \n",
       "\n",
       "            agreement_rate_with_ties  agreement_rate_without_ties  \n",
       "gemini-pro                  0.742007                     0.793388  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_pro_benchmark_df.index = [\"gemini-pro\"]\n",
    "gemini_pro_benchmark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38099fe-27c6-4f69-b0ce-8f5a853a7679",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_benchmarker.prediction_dataset.save_json(\"gemini_predictions.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef51ad4-2f63-4b04-aba3-05676b49d6af",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "For convenience, let's put all the results in a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52b919-739f-46b9-812d-715b86f08656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_examples</th>\n",
       "      <th>invalid_predictions</th>\n",
       "      <th>inconclusives</th>\n",
       "      <th>ties</th>\n",
       "      <th>agreement_rate_with_ties</th>\n",
       "      <th>agreement_rate_without_ties</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-3.5</th>\n",
       "      <td>1204</td>\n",
       "      <td>82</td>\n",
       "      <td>393</td>\n",
       "      <td>56</td>\n",
       "      <td>0.736626</td>\n",
       "      <td>0.793462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4</th>\n",
       "      <td>1204</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>103</td>\n",
       "      <td>0.701087</td>\n",
       "      <td>0.770230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini-pro</th>\n",
       "      <td>1204</td>\n",
       "      <td>2</td>\n",
       "      <td>295</td>\n",
       "      <td>60</td>\n",
       "      <td>0.742007</td>\n",
       "      <td>0.793388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            number_examples  invalid_predictions  inconclusives  ties  \\\n",
       "gpt-3.5                1204                   82            393    56   \n",
       "gpt-4                  1204                    0            100   103   \n",
       "gemini-pro             1204                    2            295    60   \n",
       "\n",
       "            agreement_rate_with_ties  agreement_rate_without_ties  \n",
       "gpt-3.5                     0.736626                     0.793462  \n",
       "gpt-4                       0.701087                     0.770230  \n",
       "gemini-pro                  0.742007                     0.793388  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "final_benchmark = pd.concat(\n",
    "    [\n",
    "        gpt_3p5_benchmark_df,\n",
    "        gpt_4_benchmark_df,\n",
    "        gemini_pro_benchmark_df,\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "final_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54852ee-204d-4395-b193-6002a185daf4",
   "metadata": {},
   "source": [
    "From the results above, we make the following observations:\n",
    "- In terms of agreement rates, all three models seem quite close, with perhaps a slight edge given to the Gemini models\n",
    "- Gemini Pro and GPT-3.5 seem to be a bit more assertive than GPT-4 resulting in only 50-60 ties to GPT-4's 100 ties.\n",
    "- However, perhaps related to the previous point, GPT-4 yields the least amount of inconclusives, meaning that it suffers the least from position bias.\n",
    "- Overall, it seems that Gemini Pro is up to snuff with GPT models, and would say that it outperforms GPT-3.5 — looks like Gemini can be legit alternatives to GPT models for evaluation tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
