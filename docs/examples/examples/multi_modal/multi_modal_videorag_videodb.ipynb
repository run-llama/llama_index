{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/multi_modal/multi_modal_videorag_videodb.ipynb\n",
    "\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "# Multimodal RAG with VideoDB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG: Multimodal Search on Videos and Stream Video Results üì∫\n",
    "\n",
    "Constructing a RAG pipeline for text is relatively straightforward, thanks to the tools developed for parsing, indexing, and retrieving text data. \n",
    "\n",
    "However, adapting RAG models for video content presents a greater challenge. Videos combine visual, auditory, and textual elements, requiring more processing power and sophisticated video pipelines.\n",
    "\n",
    "> [VideoDB](https://videodb.io) is a serverless database designed to streamline the storage, search, editing, and streaming of video content. VideoDB offers random access to sequential video data by building indexes and developing interfaces for querying and browsing video content. Learn more at [docs.videodb.io](https://docs.videodb.io).\n",
    "\n",
    "To build a truly Multimodal search for Videos, you need to work with different modalities of a video like Spoken Content, Visual.\n",
    "\n",
    "In this notebook, we will develop a multimodal RAG for video using VideoDB and Llama-Index ‚ú®.\n",
    "\n",
    "![](https://raw.githubusercontent.com/video-db/videodb-cookbook-assets/main/images/guides/multimodal_llama_index_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## üõ†Ô∏èÔ∏è Setup \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîë Requirements\n",
    "\n",
    "To connect to VideoDB, simply get the API key and create a connection. This can be done by setting the `VIDEO_DB_API_KEY` environment variable. You can get it from üëâüèº [VideoDB Console](https://console.videodb.io). ( Free for first 50 uploads, **No credit card required!** )\n",
    "\n",
    "Get your `OPENAI_API_KEY` from OpenAI platform for `llama_index` response synthesizer.\n",
    "\n",
    "<!-- > Set the `OPENAI_API_KEY` & `VIDEO_DB_API_KEY` environment variable with your API keys. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"VIDEO_DB_API_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Installing Dependencies\n",
    "\n",
    "To get started, we'll need to install the following packages:\n",
    "\n",
    "- `llama-index`\n",
    "- `videodb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install videodb\n",
    "%pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ† Building Multimodal RAG\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìã Step 1: Connect to VideoDB and Upload Video\n",
    "\n",
    "Let's upload a our video file first.\n",
    "\n",
    "You can use any `public url`, `Youtube link` or `local file` on your system. \n",
    "\n",
    "> ‚ú® First 50 uploads are free!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Video\n",
      "Video uploaded with ID: m-0ccadfc8-bc8c-4183-b83a-543946460e2a\n"
     ]
    }
   ],
   "source": [
    "from videodb import connect\n",
    "\n",
    "# connect to VideoDB\n",
    "conn = connect()\n",
    "coll = conn.get_collection()\n",
    "\n",
    "# upload videos to default collection in VideoDB\n",
    "print(\"Uploading Video\")\n",
    "video = conn.upload(url=\"https://www.youtube.com/watch?v=libKVRa01L8\")\n",
    "print(f\"Video uploaded with ID: {video.id}\")\n",
    "\n",
    "\n",
    "# video = coll.get_video(\"m-56f55058-62b6-49c4-bbdc-43c0badf4c0b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * `coll = conn.get_collection()` : Returns default collection object.\n",
    "> * `coll.get_videos()` : Returns list of all the videos in a collections.\n",
    "> * `coll.get_video(video_id)`: Returns Video object from given`video_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì∏üó£Ô∏è Step 2: Extract Scenes from Video  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to extract scenes from the video and then use vLLM to obtain a description of each scene.\n",
    "\n",
    "To learn more about Scene Extraction options, explore the following guides:\n",
    "- [Scene Extraction Options Guide](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/playground_scene_extraction.ipynb) delves deeper into the various options available for scene extraction within Scene Index. It covers advanced settings, customization features, and tips for optimizing scene extraction based on different needs and preferences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Visual content in Video...\n",
      "Scene Index successful with ID: f3eef7aee2a0ff58\n"
     ]
    }
   ],
   "source": [
    "from videodb import SceneExtractionType\n",
    "\n",
    "\n",
    "# Specify Scene Extraction algorithm\n",
    "index_id = video.index_scenes(\n",
    "    extraction_type=SceneExtractionType.time_based,\n",
    "    extraction_config={\"time\": 2, \"select_frames\": [\"first\", \"last\"]},\n",
    "    prompt=\"Describe the scene in detail\",\n",
    ")\n",
    "video.get_scene_index(index_id)\n",
    "\n",
    "print(f\"Scene Extraction successful with ID: {index_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ú® Step 3 : Incorporating VideoDB in your existing Llamaindex RAG Pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To develop a thorough multimodal search for videos, you need to handle different video modalities, including spoken content and visual elements.\n",
    "\n",
    "You can retrieve all Transcript Nodes and Visual Nodes of a video using VideoDB and then incorporate them into your LlamaIndex pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üó£ Fetching Transcript Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can fetch transcript nodes using `Video.get_transcript()`\n",
    "\n",
    "To configure the segmenter, use the `segmenter` and `length` arguments.\n",
    "\n",
    "Possible values for segmenter are:\n",
    "- `Segmenter.time`: Segments the video based on the specified `length` in seconds.\n",
    "- `Segmenter.word`: Segments the video based on the word count specified by `length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import Segmenter\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "\n",
    "# Fetch all Transcript Nodes\n",
    "nodes_transcript_raw = video.get_transcript(\n",
    "    segmenter=Segmenter.time, length=60\n",
    ")\n",
    "\n",
    "# Convert the raw transcript nodes to TextNode objects\n",
    "nodes_transcript = [\n",
    "    TextNode(\n",
    "        text=node[\"text\"],\n",
    "        metadata={key: value for key, value in node.items() if key != \"text\"},\n",
    "    )\n",
    "    for node in nodes_transcript_raw\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üì∏ Fetching Scene Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all Scenes\n",
    "scenes = video.get_scene_index(index_id)\n",
    "\n",
    "# Convert the scenes to TextNode objects\n",
    "nodes_scenes = [\n",
    "    TextNode(\n",
    "        text=node[\"description\"],\n",
    "        metadata={\n",
    "            key: value for key, value in node.items() if key != \"description\"\n",
    "        },\n",
    "    )\n",
    "    for node in scenes\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Simple RAG Pipeline with Transcript + Scene Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We index both our Transcript Nodes and Scene Node\n",
    "\n",
    "üîç‚ú® For simplicity, we are using a basic RAG pipeline. However, you can integrate more advanced LlamaIndex RAG pipelines here for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The narrator discusses the location of our Solar System within the Milky Way galaxy, emphasizing its position in one of the minor spiral arms known as the Orion Spur. The images provided offer visual representations of the Milky Way's structure, with labels indicating the specific location of the Solar System within the galaxy.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Index both Transcript and Scene Nodes\n",
    "index = VectorStoreIndex(nodes_scenes + nodes_transcript)\n",
    "q = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ô∏èüí¨Ô∏è Viewing the result : Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = q.query(\n",
    "    \"Show me where the narrator discusses the formation of the solar system and visualize the milky way galaxy\"\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üé• Viewing the result : Video Clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our nodes' metadata includes `start` and `end` fields, which represent the start and end times relative to the beginning of the video.\n",
    "\n",
    "Using this information from the relevant nodes, we can create Video Clips corresponding to these nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import play_stream\n",
    "\n",
    "\n",
    "# Helper function to merge overlapping intervals\n",
    "def merge_intervals(intervals):\n",
    "    if not intervals:\n",
    "        return []\n",
    "    intervals.sort(key=lambda x: x[0])\n",
    "    merged = [intervals[0]]\n",
    "    for interval in intervals[1:]:\n",
    "        if interval[0] <= merged[-1][1]:\n",
    "            merged[-1][1] = max(merged[-1][1], interval[1])\n",
    "        else:\n",
    "            merged.append(interval)\n",
    "    return merged\n",
    "\n",
    "\n",
    "# Extract relevant timestamps from the source nodes\n",
    "relevant_timestamps = [\n",
    "    [node.metadata[\"start\"], node.metadata[\"end\"]] for node in res.source_nodes\n",
    "]\n",
    "\n",
    "# Create a compilation of all relevant timestamps\n",
    "stream_url = video.generate_stream(merge_intervals(relevant_timestamps))\n",
    "play_stream(stream_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è Next Steps\n",
    "---\n",
    "\n",
    "In this guide, we built a Simple Multimodal RAG for Videos Using VideoDB, Llamaindex, and OpenAI\n",
    "\n",
    "You can optimize the pipeline by incorporating more advanced techniques like\n",
    "- Build a Search on Video Collection\n",
    "- Optimize Query Transformation\n",
    "- More methods to combine retrieved nodes from different modalities\n",
    "- Experiment with Different RAG pipelines like Knowledge Graph\n",
    "\n",
    "\n",
    "To learn more about Scene Index, explore the following guides:\n",
    "\n",
    "- [Quickstart Guide](https://github.com/video-db/videodb-cookbook/blob/main/quickstart/Scene%20Index%20QuickStart.ipynb) \n",
    "- [Scene Extraction Options](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/playground_scene_extraction.ipynb)\n",
    "- [Advanced Visual Search](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/advanced_visual_search.ipynb)\n",
    "- [Custom Annotation Pipelines](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/custom_annotations.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Support & Community\n",
    "---\n",
    "\n",
    "If you have any questions or feedback. Feel free to reach out to us üôåüèº\n",
    "\n",
    "* [Discord](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fdiscord.gg%2Fpy9P639jGz)\n",
    "* [GitHub](https://github.com/video-db)\n",
    "* [Email](mailto:ashu@videodb.io)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
