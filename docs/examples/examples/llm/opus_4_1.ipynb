{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Opus 4.1 with LlamaIndex\n",
    "\n",
    "In this notebook we are going to exploit [Claude Opus 4.1 by Anthropic](https://www.anthropic.com/news/claude-opus-4-1) advanced coding capabilities to create a cute website, and we're going to do it within LlamaIndex!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an LLM-based assistant with Opus 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Install needed dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q llama-index-llms-anthropic get-code-from-markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just define a helper function to help us fetch the code from Markdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_code_from_markdown import get_code_from_markdown\n",
    "\n",
    "\n",
    "def fetch_code_from_markdown(markdown: str) -> str:\n",
    "    return get_code_from_markdown(markdown, language=\"html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now initialize our LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.anthropic import Anthropic\n",
    "\n",
    "llm = Anthropic(model=\"claude-opus-4-1-20250805\", max_tokens=12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm.complete(\n",
    "    \"Can you build a llama-themed static HTML page, with cute little bouncing animations and blue/white/indigo as theme colors?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now get the code and write it to an HTML file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_code = fetch_code_from_markdown(res.text)\n",
    "\n",
    "with open(\"index.html\", \"w\") as f:\n",
    "    for block in html_code:\n",
    "        f.write(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now download `index.html` and take a look at the results :)\n",
    "\n",
    "![Llama Paradise HTML](./llama_paradise.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an agent with Opus 4.1\n",
    "\n",
    "We can also build a simple calculator agent using Claude Opus 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers and return an integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Sum two integers and return an integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "agent = FunctionAgent(\n",
    "    name=\"CalculatorAgent\",\n",
    "    description=\"Useful to perform basic arithmetic operations\",\n",
    "    system_prompt=\"You are a calculator agent, you should perform arithmetic operations using the tools available to you.\",\n",
    "    tools=[multiply, add],\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run the agent through and get the result for a multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling tool multiply with arguments:\n",
      "\n",
      "{'a': 60, 'b': 95}\n",
      "Result from calling tool multiply:\n",
      "\n",
      "5700\n",
      "Final response\n",
      "60 multiplied by 95 equals 5,700.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import ToolCall, ToolCallResult\n",
    "\n",
    "handler = agent.run(\"What is 60 multiplied by 95?\")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, ToolCallResult):\n",
    "        print(\n",
    "            f\"Result from calling tool {event.tool_name}:\\n\\n{event.tool_output}\"\n",
    "        )\n",
    "    if isinstance(event, ToolCall):\n",
    "        print(\n",
    "            f\"Calling tool {event.tool_name} with arguments:\\n\\n{event.tool_kwargs}\"\n",
    "        )\n",
    "\n",
    "response = await handler\n",
    "\n",
    "print(\"Final response\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also run it with a sum!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling tool add with arguments:\n",
      "\n",
      "{'a': 1234, 'b': 5678}\n",
      "Result from calling tool add:\n",
      "\n",
      "6912\n",
      "Final response\n",
      "1234 plus 5678 equals 6912.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import ToolCall, ToolCallResult\n",
    "\n",
    "handler = agent.run(\"What is 1234 plus 5678?\")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, ToolCallResult):\n",
    "        print(\n",
    "            f\"Result from calling tool {event.tool_name}:\\n\\n{event.tool_output}\"\n",
    "        )\n",
    "    if isinstance(event, ToolCall):\n",
    "        print(\n",
    "            f\"Calling tool {event.tool_name} with arguments:\\n\\n{event.tool_kwargs}\"\n",
    "        )\n",
    "\n",
    "response = await handler\n",
    "\n",
    "print(\"Final response\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want more content around Anthropic, make sure to check out our [general example notebook](./anthropic.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llama-index-work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
