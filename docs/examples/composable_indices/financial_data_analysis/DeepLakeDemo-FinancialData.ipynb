{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLake + LlamaIndex\n",
    "\n",
    "Look at financial statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in ./GPTIndex/lib/python3.9/site-packages (0.6.37)\n",
      "Requirement already satisfied: deeplake in ./GPTIndex/lib/python3.9/site-packages (3.6.7)\n",
      "Requirement already satisfied: urllib3<2 in ./GPTIndex/lib/python3.9/site-packages (from llama-index) (1.26.7)\n",
      "Requirement already satisfied: numpy in ./GPTIndex/lib/python3.9/site-packages (from llama-index) (1.24.2)\n",
      "Requirement already satisfied: sqlalchemy>=2.0.15 in ./GPTIndex/lib/python3.9/site-packages (from llama-index) (2.0.17)\n",
      "Requirement already satisfied: pandas in ./GPTIndex/lib/python3.9/site-packages (from llama-index) (2.0.0)\n",
      "Requirement already satisfied: typing-inspect==0.8.0 in ./GPTIndex/lib/python3.9/site-packages (from llama-index) (0.8.0)\n",
      "Requirement already satisfied: langchain>=0.0.218 in ./GPTIndex/lib/python3.9/site-packages (from llama-index) (0.0.219)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./GPTIndex/lib/python3.9/site-packages (from llama-index) (2023.6.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./GPTIndex/lib/python3.9/site-packages (from llama-index) (4.12.2)\n",
      "Requirement already satisfied: tiktoken in ./GPTIndex/lib/python3.9/site-packages (from llama-index) (0.3.3)\n",
      "Requirement already satisfied: typing-extensions==4.5.0 in ./GPTIndex/lib/python3.9/site-packages (from llama-index) (4.5.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in ./GPTIndex/lib/python3.9/site-packages (from llama-index) (8.2.2)\n",
      "Requirement already satisfied: dataclasses-json in ./GPTIndex/lib/python3.9/site-packages (from llama-index) (0.5.7)\n",
      "Requirement already satisfied: openai>=0.26.4 in ./GPTIndex/lib/python3.9/site-packages (from llama-index) (0.27.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./GPTIndex/lib/python3.9/site-packages (from typing-inspect==0.8.0->llama-index) (1.0.0)\n",
      "Requirement already satisfied: pillow in ./GPTIndex/lib/python3.9/site-packages (from deeplake) (9.5.0)\n",
      "Requirement already satisfied: boto3 in ./GPTIndex/lib/python3.9/site-packages (from deeplake) (1.24.59)\n",
      "Requirement already satisfied: click in ./GPTIndex/lib/python3.9/site-packages (from deeplake) (8.1.3)\n",
      "Requirement already satisfied: pathos in ./GPTIndex/lib/python3.9/site-packages (from deeplake) (0.3.0)\n",
      "Requirement already satisfied: humbug>=0.3.1 in ./GPTIndex/lib/python3.9/site-packages (from deeplake) (0.3.1)\n",
      "Requirement already satisfied: tqdm in ./GPTIndex/lib/python3.9/site-packages (from deeplake) (4.65.0)\n",
      "Requirement already satisfied: numcodecs in ./GPTIndex/lib/python3.9/site-packages (from deeplake) (0.11.0)\n",
      "Requirement already satisfied: pyjwt in ./GPTIndex/lib/python3.9/site-packages (from deeplake) (2.6.0)\n",
      "Requirement already satisfied: aioboto3>=10.4.0 in ./GPTIndex/lib/python3.9/site-packages (from deeplake) (10.4.0)\n",
      "Requirement already satisfied: nest_asyncio in ./GPTIndex/lib/python3.9/site-packages (from deeplake) (1.5.6)\n",
      "Requirement already satisfied: aiobotocore[boto3]==2.4.2 in ./GPTIndex/lib/python3.9/site-packages (from aioboto3>=10.4.0->deeplake) (2.4.2)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in ./GPTIndex/lib/python3.9/site-packages (from aiobotocore[boto3]==2.4.2->aioboto3>=10.4.0->deeplake) (1.15.0)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in ./GPTIndex/lib/python3.9/site-packages (from aiobotocore[boto3]==2.4.2->aioboto3>=10.4.0->deeplake) (0.11.0)\n",
      "Requirement already satisfied: botocore<1.27.60,>=1.27.59 in ./GPTIndex/lib/python3.9/site-packages (from aiobotocore[boto3]==2.4.2->aioboto3>=10.4.0->deeplake) (1.27.59)\n",
      "Requirement already satisfied: aiohttp>=3.3.1 in ./GPTIndex/lib/python3.9/site-packages (from aiobotocore[boto3]==2.4.2->aioboto3>=10.4.0->deeplake) (3.8.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./GPTIndex/lib/python3.9/site-packages (from aiohttp>=3.3.1->aiobotocore[boto3]==2.4.2->aioboto3>=10.4.0->deeplake) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./GPTIndex/lib/python3.9/site-packages (from aiohttp>=3.3.1->aiobotocore[boto3]==2.4.2->aioboto3>=10.4.0->deeplake) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./GPTIndex/lib/python3.9/site-packages (from aiohttp>=3.3.1->aiobotocore[boto3]==2.4.2->aioboto3>=10.4.0->deeplake) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./GPTIndex/lib/python3.9/site-packages (from aiohttp>=3.3.1->aiobotocore[boto3]==2.4.2->aioboto3>=10.4.0->deeplake) (3.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./GPTIndex/lib/python3.9/site-packages (from aiohttp>=3.3.1->aiobotocore[boto3]==2.4.2->aioboto3>=10.4.0->deeplake) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./GPTIndex/lib/python3.9/site-packages (from aiohttp>=3.3.1->aiobotocore[boto3]==2.4.2->aioboto3>=10.4.0->deeplake) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./GPTIndex/lib/python3.9/site-packages (from aiohttp>=3.3.1->aiobotocore[boto3]==2.4.2->aioboto3>=10.4.0->deeplake) (1.8.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./GPTIndex/lib/python3.9/site-packages (from boto3->deeplake) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in ./GPTIndex/lib/python3.9/site-packages (from boto3->deeplake) (0.6.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./GPTIndex/lib/python3.9/site-packages (from botocore<1.27.60,>=1.27.59->aiobotocore[boto3]==2.4.2->aioboto3>=10.4.0->deeplake) (2.8.2)\n",
      "Requirement already satisfied: requests in ./GPTIndex/lib/python3.9/site-packages (from humbug>=0.3.1->deeplake) (2.28.2)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in ./GPTIndex/lib/python3.9/site-packages (from langchain>=0.0.218->llama-index) (2.8.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in ./GPTIndex/lib/python3.9/site-packages (from langchain>=0.0.218->llama-index) (1.10.7)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in ./GPTIndex/lib/python3.9/site-packages (from langchain>=0.0.218->llama-index) (6.0)\n",
      "Requirement already satisfied: langchainplus-sdk>=0.0.17 in ./GPTIndex/lib/python3.9/site-packages (from langchain>=0.0.218->llama-index) (0.0.17)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in ./GPTIndex/lib/python3.9/site-packages (from langchain>=0.0.218->llama-index) (1.2.4)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in ./GPTIndex/lib/python3.9/site-packages (from dataclasses-json->llama-index) (1.5.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in ./GPTIndex/lib/python3.9/site-packages (from dataclasses-json->llama-index) (3.19.0)\n",
      "Requirement already satisfied: packaging>=17.0 in ./GPTIndex/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json->llama-index) (23.1)\n",
      "Requirement already satisfied: six>=1.5 in ./GPTIndex/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.60,>=1.27.59->aiobotocore[boto3]==2.4.2->aioboto3>=10.4.0->deeplake) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./GPTIndex/lib/python3.9/site-packages (from requests->humbug>=0.3.1->deeplake) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./GPTIndex/lib/python3.9/site-packages (from requests->humbug>=0.3.1->deeplake) (3.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./GPTIndex/lib/python3.9/site-packages (from beautifulsoup4->llama-index) (2.4.1)\n",
      "Requirement already satisfied: entrypoints in ./GPTIndex/lib/python3.9/site-packages (from numcodecs->deeplake) (0.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./GPTIndex/lib/python3.9/site-packages (from pandas->llama-index) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./GPTIndex/lib/python3.9/site-packages (from pandas->llama-index) (2023.3)\n",
      "Requirement already satisfied: dill>=0.3.6 in ./GPTIndex/lib/python3.9/site-packages (from pathos->deeplake) (0.3.6)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in ./GPTIndex/lib/python3.9/site-packages (from pathos->deeplake) (1.7.6.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in ./GPTIndex/lib/python3.9/site-packages (from pathos->deeplake) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in ./GPTIndex/lib/python3.9/site-packages (from pathos->deeplake) (0.70.14)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./GPTIndex/lib/python3.9/site-packages (from tiktoken->llama-index) (2023.3.23)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Users/adilkhansarsen/Documents/work/LlamaIndex/llama_index/GPTIndex/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My OpenAI Key\n",
    "import os\n",
    "import getpass\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass(\"OpenAI token: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adilkhansarsen/Documents/work/LlamaIndex/llama_index/GPTIndex/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleKeywordTableIndex, \n",
    "    SimpleDirectoryReader,\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    "    download_loader,\n",
    "    Document,\n",
    ")\n",
    "from llama_index.vector_stores import DeepLakeVectorStore\n",
    "from langchain.llms.openai import OpenAIChat\n",
    "from typing import List, Optional, Tuple\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Data (PDFs of Financial Statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# financial reports of amamzon, but can be replaced by any URLs of pdfs\n",
    "urls = ['https://s2.q4cdn.com/299287126/files/doc_financials/Q1_2018_-_8-K_Press_Release_FILED.pdf',\n",
    "        'https://s2.q4cdn.com/299287126/files/doc_financials/Q2_2018_Earnings_Release.pdf',\n",
    "        'https://s2.q4cdn.com/299287126/files/doc_news/archive/Q318-Amazon-Earnings-Press-Release.pdf',\n",
    "        'https://s2.q4cdn.com/299287126/files/doc_news/archive/AMAZON.COM-ANNOUNCES-FOURTH-QUARTER-SALES-UP-20-TO-$72.4-BILLION.pdf',\n",
    "        'https://s2.q4cdn.com/299287126/files/doc_financials/Q119_Amazon_Earnings_Press_Release_FINAL.pdf',\n",
    "        'https://s2.q4cdn.com/299287126/files/doc_news/archive/Amazon-Q2-2019-Earnings-Release.pdf',\n",
    "        'https://s2.q4cdn.com/299287126/files/doc_news/archive/Q3-2019-Amazon-Financial-Results.pdf',\n",
    "        'https://s2.q4cdn.com/299287126/files/doc_news/archive/Amazon-Q4-2019-Earnings-Release.pdf',\n",
    "        'https://s2.q4cdn.com/299287126/files/doc_financials/2020/Q1/AMZN-Q1-2020-Earnings-Release.pdf',\n",
    "        'https://s2.q4cdn.com/299287126/files/doc_financials/2020/q2/Q2-2020-Amazon-Earnings-Release.pdf',\n",
    "        'https://s2.q4cdn.com/299287126/files/doc_financials/2020/q4/Amazon-Q4-2020-Earnings-Release.pdf',\n",
    "        'https://s2.q4cdn.com/299287126/files/doc_financials/2021/q1/Amazon-Q1-2021-Earnings-Release.pdf',\n",
    "        'https://s2.q4cdn.com/299287126/files/doc_financials/2021/q2/AMZN-Q2-2021-Earnings-Release.pdf',\n",
    "        'https://s2.q4cdn.com/299287126/files/doc_financials/2021/q3/Q3-2021-Earnings-Release.pdf',\n",
    "        'https://s2.q4cdn.com/299287126/files/doc_financials/2021/q4/business_and_financial_update.pdf',\n",
    "        'https://s2.q4cdn.com/299287126/files/doc_financials/2022/q1/Q1-2022-Amazon-Earnings-Release.pdf',\n",
    "        'https://s2.q4cdn.com/299287126/files/doc_financials/2022/q2/Q2-2022-Amazon-Earnings-Release.pdf',\n",
    "        'https://s2.q4cdn.com/299287126/files/doc_financials/2022/q3/Q3-2022-Amazon-Earnings-Release.pdf',\n",
    "        'https://s2.q4cdn.com/299287126/files/doc_financials/2022/q4/Q4-2022-Amazon-Earnings-Release.pdf'\n",
    "    ]\n",
    "\n",
    "# hardcoding for now since we're missing q3 2020\n",
    "years = [\n",
    "    2018, 2018, 2018, 2018,\n",
    "    2019, 2019, 2019, 2019,\n",
    "    2020, 2020, 2020,\n",
    "    2021, 2021, 2021, 2021,\n",
    "    2022, 2022, 2022, 2022\n",
    "]\n",
    "months = [\n",
    "    1, 4, 7, 10,\n",
    "    1, 4, 7, 10,\n",
    "    1, 4, 10,\n",
    "    1, 4, 7, 10,\n",
    "    1, 4, 7, 10\n",
    "]\n",
    "\n",
    "zipped_data = list(zip(urls, months, years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDFReader = download_loader(\"PDFReader\")\n",
    "\n",
    "loader = PDFReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_reports(data: List[Tuple[str, int, int]], out_dir: Optional[str] = None) -> List[Document]:\n",
    "    \"\"\"Download pages from a list of urls.\"\"\"\n",
    "    docs = []\n",
    "    out_dir = Path(out_dir or \".\")\n",
    "    if not out_dir.exists():\n",
    "        print(out_dir)\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    \n",
    "    for url, month, year in tqdm.tqdm(data):\n",
    "        path_base = url.split('/')[-1]\n",
    "        out_path = out_dir / path_base\n",
    "        if not out_path.exists():\n",
    "            r = requests.get(url)\n",
    "            with open(out_path, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        doc = loader.load_data(file=Path(out_path))[0]\n",
    "        \n",
    "        date_str = f\"{month:02d}\" + \"-01-\" + str(year)\n",
    "        doc.extra_info = {\"Date\": date_str}\n",
    "        \n",
    "        docs.append(doc)\n",
    "    return docs\n",
    "\n",
    "def _get_quarter_from_month(month: int) -> str:\n",
    "    mapping = {\n",
    "        1: \"Q1\",\n",
    "        4: \"Q2\",\n",
    "        7: \"Q3\",\n",
    "        10: \"Q4\"\n",
    "    }\n",
    "    return mapping[month]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:13<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = download_reports(zipped_data, 'data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vector Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adilkhansarsen/Documents/work/LlamaIndex/llama_index/GPTIndex/lib/python3.9/site-packages/langchain/llms/openai.py:769: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm_predictor_chatgpt = LLMPredictor(\n",
    "    llm=OpenAIChat(temperature=0, model_name=\"gpt-3.5-turbo-16k-0613\")\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor_chatgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 1023 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 1023 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 1118 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 1118 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 917 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 917 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 793 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 793 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 927 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 927 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 1021 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 1021 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 845 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 845 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 866 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 866 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 895 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 895 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 846 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 846 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 702 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 702 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 715 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 715 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 763 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 763 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 848 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 848 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 842 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 842 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 842 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 842 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 839 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 839 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 638 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 638 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 701 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 701 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Build city document index\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "# build vector index for each quarterly statement, store in dictionary\n",
    "dataset_root = 'amazon_example/amazon_financial_'\n",
    "vector_indices = {}\n",
    "for idx, (_, month, year) in enumerate(zipped_data):\n",
    "    doc = docs[idx]\n",
    "\n",
    "    dataset_path = dataset_root + f\"{month:02d}_{year}\"\n",
    "    vector_store = DeepLakeVectorStore(\n",
    "        dataset_path=dataset_path,\n",
    "        overwrite=True,\n",
    "        verbose=False,\n",
    "    ) \n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        \n",
    "    vector_index = VectorStoreIndex.from_documents([doc], storage_context=storage_context, service_context=service_context)\n",
    "    vector_indices[(month, year)] = vector_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Querying a Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 7 tokens\n",
      "> [retrieve] Total embedding token usage: 7 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1081 tokens\n",
      "> [get_response] Total LLM token usage: 1081 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "response = vector_indices[(1, 2018)].as_query_engine(service_context=service_context).query(\"What is the operating cash flow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The operating cash flow for the trailing twelve months ended March 31, 2018, was $18.2 billion.\n",
      "> Source (Doc id: e764aa30-7451-4c93-aac3-402bb1dd7aba): 1 \n",
      "   \n",
      "AMAZON.COM ANNOUNCES FIRST QUARTER SALES UP 43% TO $51.0 BILLION  \n",
      "SEATTLE —(BUSINESS WIRE...\n",
      "\n",
      "> Source (Doc id: 934a2360-2fa6-4fbc-9706-2e0cc743d9be): and Insignia brands, available for purchase in 2018 through Best Buy stores, \n",
      "BestBuy.com, and Am...\n"
     ]
    }
   ],
   "source": [
    "print(str(response))\n",
    "print(response.get_formatted_sources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 8 tokens\n",
      "> [retrieve] Total embedding token usage: 8 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1073 tokens\n",
      "> [get_response] Total LLM token usage: 1073 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "response = vector_indices[(1, 2018)].as_query_engine(service_context=service_context).query(\"What are the updates on Whole Foods?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given context information does not provide any updates on Whole Foods.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph: Keyword Table Index on top of vector indices! \n",
    "\n",
    "We compose a keyword table index on top of all the vector indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.composability.graph import ComposableGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set summary text for city\n",
    "index_summaries = {}\n",
    "for idx, (_, month, year) in enumerate(zipped_data):\n",
    "    quarter_str = _get_quarter_from_month(month)\n",
    "    index_summaries[(month, year)] = f\"Amazon Financial Statement, {quarter_str}, {year}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "graph = ComposableGraph.from_indices(\n",
    "    SimpleKeywordTableIndex,\n",
    "    [index for _, index in vector_indices.items()], \n",
    "    [summary for _, summary in index_summaries.items()],\n",
    "    max_keywords_per_chunk=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.query.query_transform.base import DecomposeQueryTransform\n",
    "decompose_transform = DecomposeQueryTransform(\n",
    "    llm_predictor_chatgpt, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TMP \n",
    "query_str = \"Analyze revenue in Q1 of 2018.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with query decomposition in subindices\n",
    "from llama_index.query_engine.transform_query_engine import TransformQueryEngine\n",
    "\n",
    "\n",
    "custom_query_engines = {}\n",
    "for index in vector_indices.values():\n",
    "    query_engine = index.as_query_engine(service_context=service_context)\n",
    "    transform_metadata = {'index_summary': index.index_struct.summary}\n",
    "    tranformed_query_engine = TransformQueryEngine(query_engine, decompose_transform, transform_metadata=transform_metadata)\n",
    "    custom_query_engines[index.index_id] = tranformed_query_engine\n",
    "\n",
    "custom_query_engines[graph.root_index.index_id] = graph.root_index.as_query_engine(\n",
    "    retriever_mode='simple', \n",
    "    response_mode='tree_summarize', \n",
    "    service_context=service_context\n",
    ")\n",
    "\n",
    "query_engine_decompose = graph.as_query_engine(\n",
    "    custom_query_engines=custom_query_engines,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.query.query_transform.base import DecomposeQueryTransform\n",
    "decompose_transform = DecomposeQueryTransform(\n",
    "    llm_predictor_chatgpt, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.keyword_table.retrievers:> Starting query: Analyze revenue in Q1 of 2018.\n",
      "> Starting query: Analyze revenue in Q1 of 2018.\n",
      "INFO:llama_index.indices.keyword_table.retrievers:query keywords: ['revenue', '2018', 'q1', 'analyze']\n",
      "query keywords: ['revenue', '2018', 'q1', 'analyze']\n",
      "INFO:llama_index.indices.keyword_table.retrievers:> Extracted keywords: ['2018', 'q1']\n",
      "> Extracted keywords: ['2018', 'q1']\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q1 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the total revenue of Amazon in Q1 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 13 tokens\n",
      "> [retrieve] Total embedding token usage: 13 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q1 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the total revenue of Amazon in Q1 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1082 tokens\n",
      "> [get_response] Total LLM token usage: 1082 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q1 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the total revenue of Amazon in Q1 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 13 tokens\n",
      "> [retrieve] Total embedding token usage: 13 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q1 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 974 tokens\n",
      "> [get_response] Total LLM token usage: 974 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q1 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the total revenue of Amazon in Q1 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 13 tokens\n",
      "> [retrieve] Total embedding token usage: 13 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q1 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the total revenue of Amazon in Q1 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 852 tokens\n",
      "> [get_response] Total LLM token usage: 852 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q1 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the total revenue of Amazon in Q1 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 13 tokens\n",
      "> [retrieve] Total embedding token usage: 13 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q1 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the total revenue of Amazon in Q1 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1179 tokens\n",
      "> [get_response] Total LLM token usage: 1179 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q1 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "> [retrieve] Total embedding token usage: 12 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q1 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 901 tokens\n",
      "> [get_response] Total LLM token usage: 901 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q1 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "> [retrieve] Total embedding token usage: 12 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q1 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 982 tokens\n",
      "> [get_response] Total LLM token usage: 982 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q1 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "> [retrieve] Total embedding token usage: 12 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q1 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 774 tokens\n",
      "> [get_response] Total LLM token usage: 774 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q1 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 of 2020?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "> [retrieve] Total embedding token usage: 12 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q1 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 of 2020?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 950 tokens\n",
      "> [get_response] Total LLM token usage: 950 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 218 tokens\n",
      "> [get_response] Total LLM token usage: 218 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "response_chatgpt = query_engine_decompose.query(\n",
    "    \"Analyze revenue in Q1 of 2018.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context information, the revenue of Amazon in Q1 of 2018 was $51.0 billion.\n"
     ]
    }
   ],
   "source": [
    "print(str(response_chatgpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.keyword_table.retrievers:> Starting query: Analyze revenue in Q2 of 2018.\n",
      "> Starting query: Analyze revenue in Q2 of 2018.\n",
      "INFO:llama_index.indices.keyword_table.retrievers:query keywords: ['2018', 'revenue', 'analyze', 'q2']\n",
      "query keywords: ['2018', 'revenue', 'analyze', 'q2']\n",
      "INFO:llama_index.indices.keyword_table.retrievers:> Extracted keywords: ['2018', 'q2']\n",
      "> Extracted keywords: ['2018', 'q2']\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the total revenue of Amazon in Q2 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 13 tokens\n",
      "> [retrieve] Total embedding token usage: 13 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the total revenue of Amazon in Q2 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1177 tokens\n",
      "> [get_response] Total LLM token usage: 1177 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q3 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "> [retrieve] Total embedding token usage: 12 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q3 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 972 tokens\n",
      "> [get_response] Total LLM token usage: 972 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the total revenue of Amazon in Q2 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 13 tokens\n",
      "> [retrieve] Total embedding token usage: 13 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the total revenue of Amazon in Q2 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 852 tokens\n",
      "> [get_response] Total LLM token usage: 852 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q2 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "> [retrieve] Total embedding token usage: 12 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q2 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1081 tokens\n",
      "> [get_response] Total LLM token usage: 1081 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q2 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "> [retrieve] Total embedding token usage: 12 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q2 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1078 tokens\n",
      "> [get_response] Total LLM token usage: 1078 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q2 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "> [retrieve] Total embedding token usage: 12 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q2 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 898 tokens\n",
      "> [get_response] Total LLM token usage: 898 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q2 of 2021?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "> [retrieve] Total embedding token usage: 12 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q2 of 2021?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 818 tokens\n",
      "> [get_response] Total LLM token usage: 818 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q2 of 2020?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "> [retrieve] Total embedding token usage: 12 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze revenue in Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q2 of 2020?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 901 tokens\n",
      "> [get_response] Total LLM token usage: 901 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 210 tokens\n",
      "> [get_response] Total LLM token usage: 210 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "response_chatgpt = query_engine_decompose.query(\n",
    "    \"Analyze revenue in Q2 of 2018.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context information, the revenue of Amazon in Q2 of 2018 was $52.9 billion.\n"
     ]
    }
   ],
   "source": [
    "print(str(response_chatgpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.keyword_table.retrievers:> Starting query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "> Starting query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "INFO:llama_index.indices.keyword_table.retrievers:query keywords: ['comapre', 'q1', 'revenue', 'analyze', '2018', 'q2']\n",
      "query keywords: ['comapre', 'q1', 'revenue', 'analyze', '2018', 'q2']\n",
      "INFO:llama_index.indices.keyword_table.retrievers:> Extracted keywords: ['q1', '2018', 'q2']\n",
      "> Extracted keywords: ['q1', '2018', 'q2']\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "> [retrieve] Total embedding token usage: 12 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1080 tokens\n",
      "> [get_response] Total LLM token usage: 1080 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q2 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "> [retrieve] Total embedding token usage: 12 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q2 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1175 tokens\n",
      "> [get_response] Total LLM token usage: 1175 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 and Q2 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 15 tokens\n",
      "> [retrieve] Total embedding token usage: 15 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 and Q2 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 780 tokens\n",
      "> [get_response] Total LLM token usage: 780 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "> [retrieve] Total embedding token usage: 12 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 901 tokens\n",
      "> [get_response] Total LLM token usage: 901 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 and Q2 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 15 tokens\n",
      "> [retrieve] Total embedding token usage: 15 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 and Q2 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1001 tokens\n",
      "> [get_response] Total LLM token usage: 1001 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 and Q2 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 15 tokens\n",
      "> [retrieve] Total embedding token usage: 15 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 and Q2 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 960 tokens\n",
      "> [get_response] Total LLM token usage: 960 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q3 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "> [retrieve] Total embedding token usage: 12 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q3 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 972 tokens\n",
      "> [get_response] Total LLM token usage: 972 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q4 of 2018?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "> [retrieve] Total embedding token usage: 12 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q1 and Q2 of 2018 according to their financial statement?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 860 tokens\n",
      "> [get_response] Total LLM token usage: 860 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q2 of 2019?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "> [retrieve] Total embedding token usage: 12 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q2 of 2019?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1078 tokens\n",
      "> [get_response] Total LLM token usage: 1078 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q2 of 2022?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "> [retrieve] Total embedding token usage: 12 tokens\n",
      "\u001b[33;1m\u001b[1;3m> Current query: Analyze and comapre revenue in Q1 and Q2 of 2018.\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m> New query: What was the revenue of Amazon in Q2 of 2022?\n",
      "\u001b[0mINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 894 tokens\n",
      "> [get_response] Total LLM token usage: 894 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 351 tokens\n",
      "> [get_response] Total LLM token usage: 351 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "response_chatgpt = query_engine_decompose.query(\n",
    "    \"Analyze and comapre revenue in Q1 and Q2 of 2018.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context information, we can analyze and compare the revenue in Q1 and Q2 of 2018 for Amazon. \n",
      "\n",
      "The revenue of Amazon in Q1 of 2018 was $51.0 billion, while the revenue in Q2 of 2018 was $52.9 billion. Therefore, the revenue in Q2 of 2018 was higher than the revenue in Q1 of 2018. The difference between the two quarters is $1.9 billion.\n"
     ]
    }
   ],
   "source": [
    "print(str(response_chatgpt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPTIndex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
