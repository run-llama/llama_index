"""LocalMemoryAgent abstraction."""
import os
import json
from llama_index import GPTSimpleVectorIndex, GPTListIndex
from llama_index import LLMPredictor, Document
from langchain.chat_models import ChatOpenAI
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LocalFilesystemMemory:
    """
    LocalFilesystemMemory is a layer over Llama Index's GPTSimpleVectorIndex which
    abstracts away the details of managing indexes saved on disk. It assumes /
    creates a directory structure for storing indexes and provides a simple /
    interface for managing them.
    The directory structure is as follows:
    - index_folder/
        -  metadata.json
        -  indexes/
            -  index_1.json
            -  index_2.json
            -  ...
    The metadata.json file contains the following information:
    {
        "index_count": 2,
        "index_descriptions": [
            {
                "name": "index_1",
                "description": "Description of index generated by GPT Index.",
            },
            {
                "name": "index_2",
                "description": "Description of index generated by GPT Index.",
            },
        ]
    }
    """

    """Initialization and related private methods."""

    def __init__(self, folder, model="gpt-3.5-turbo"):
        self.folder = folder
        self.model = model
        self.index_folder = os.path.join(self.folder, "indexes")
        self.metadata_file = os.path.join(self.folder, "metadata.json")
        self.__create_folder_structure_if_not_exists()

        # Load the indexes from disk if they exist.
        self.indexes = {}
        self.__load_from_folder_if_exists()

    def __load_from_folder_if_exists(self):
        """Load the indexes from disk if they exist."""
        if os.path.exists(self.folder):
            logger.info("Loading indexes from disk...")
            metadata = self.metadata
            for index_description in metadata["index_descriptions"]:
                name = index_description["name"]
                index = GPTSimpleVectorIndex.load_from_disk(
                    self.__get_index_path(name)
                )
                self.indexes[name] = index

    def __create_folder_structure_if_not_exists(self):
        """Create the folder if it does not exist."""
        if not os.path.exists(self.folder):
            # Make the folder and the indexes folder.
            os.makedirs(self.folder)
            os.makedirs(self.index_folder)
            # Make the metadata file.
            metadata = {"index_count": 0, "index_descriptions": []}
            with open(self.metadata_file, "w") as f:
                json.dump(metadata, f)

    def __get_index_path(self, name):
        """Get the name of the index file to save."""
        return os.path.join(self.index_folder, name + ".json")

    def __save_metadata(self, metadata):
        """Save the metadata."""
        with open(self.metadata_file, "w") as f:
            json.dump(metadata, f)

    def _query_index(self, index, prompt, top_k=1):
        kwargs = {"temperature": 0, "model_name": "gpt-3.5-turbo"}
        llm = ChatOpenAI(**kwargs)
        llm_predictor = LLMPredictor(llm=llm)
        return index.query(prompt, llm_predictor=llm_predictor, similarity_top_k=top_k)

    def __get_docs_from_strings(self, strings):
        """Get a list of Document objects from a list of strings."""
        if isinstance(strings, str):
            docs = [Document(strings)]
        elif isinstance(strings, list):
            if isinstance(strings[0], str):
                docs = [Document(doc) for doc in strings]
            else:
                docs = strings
        return docs
    
    def __get_summaries_from_docs(self, docs):
        """Get a list of summaries from a list of Document objects."""
        summaries = []
        for doc in docs:
            index = GPTSimpleVectorIndex([doc])
            resp = self._query_index(index, "Summarize:")
            summary = resp.response
            summaries.append(summary)
        return summaries

    def __get_summary_description(self, summary_docs) -> str:
        """Get a summary description from a list of Documentt objects."""
        logger.info("Generating description for index...")
        summary_index = GPTListIndex(summary_docs)
        resp = summary_index.query("Summarize the documents.")
        description = resp.response
        return description

    """CRUD and related public methods for indexes."""

    @property
    def metadata(self):
        """Get the metadata."""
        with open(self.metadata_file, "r") as f:
            return json.load(f)

    def add_index(self, name, index, description=None):
        """Add an index to memory."""
        logger.info("Adding index to memory...")
        key = list(index._docstore.docs.keys())[0]
        docs_dict = index._docstore.docs[key].nodes_dict
        docs = []
        for key in docs_dict.keys():
            docs.append(Document(docs_dict[key].text))
        
        summaries = self.__get_summaries_from_docs(docs)
        summary_docs = [Document(summary) for summary in summaries]
        if description is None:
            description = self.__get_summary_description(summary_docs)

        metadata = self.metadata
        # Ensure that the index name is unique.
        for index_description in metadata["index_descriptions"]:
            if index_description["name"] == name:
                logger.info("Index name already exists.")
                return

        # Update the metadata.
        metadata["index_count"] += 1
        metadata["index_descriptions"].append(
            {
                "name": name,
                "description": description,
            }
        )
        self.__save_metadata(metadata)

        # Save the index to disk and add it to the indexes dictionary.
        index.save_to_disk(self.__get_index_path(name))
        self.indexes[name] = index

    def create_index(self, name, docs, description=None):
        """Create a new index."""
        docs = self.__get_docs_from_strings(docs)
        summaries = self.__get_summaries_from_docs(docs)
        summary_docs = [Document(summary) for summary in summaries]
        logger.info("Generating index...")
        index = GPTSimpleVectorIndex(docs)
        if description is None:
            description = self.__get_summary_description(summary_docs)

        metadata = self.metadata
        # Ensure that the index name is unique.
        for index_description in metadata["index_descriptions"]:
            if index_description["name"] == name:
                logger.info("Index name already exists.")
                return

        # Update the metadata.
        metadata["index_count"] += 1
        metadata["index_descriptions"].append(
            {
                "name": name,
                "description": description,
            }
        )
        self.__save_metadata(metadata)

        # Save the index to disk and add it to the indexes dictionary.
        index.save_to_disk(self.__get_index_path(name))
        self.indexes[name] = index

    def update_index(self, name, docs):
        """Update an existing index."""
        logger.info(f"Updating index: {name}...")
        docs = self.__get_docs_from_strings(docs)
        index = GPTSimpleVectorIndex(docs)
        index.save_to_disk(self.__get_index_path(name))
        self.indexes[name] = index

    def delete_index(self, name):
        """Delete an existing index."""
        metadata = self.metadata
        # Update the metadata.
        metadata["index_count"] -= 1
        for index_description in metadata["index_descriptions"]:
            if index_description["name"] == name:
                metadata["index_descriptions"].remove(index_description)
        self.__save_metadata(metadata)
        # Delete the index from disk.
        os.remove(self.__get_index_path(name))
        # Delete the index from the indexes dictionary.
        del self.indexes[name]

    def get_index(self, name):
        """Get an existing index."""
        if name in self.indexes:
            return self.indexes[name]
        else:
            raise Exception(f"Index {name} does not exist.")

    def query_index(self, name, prompt, top_k=1):
        """Query an existing index."""
        index = self.get_index(name)
        return self._query_index(index, prompt, top_k=top_k)

    def find_most_relevant_index(self, prompt):
        """Find the most relevant index."""
        metadata = self.metadata
        # Create a string of all the index names and descriptions, separated
        # by a newline. For example:
        # "index_name: index_description\nindex_name: index_description\n..."
        index_names_and_descriptions = "The below indexes are available:\n\n"
        for index_description in metadata["index_descriptions"]:
            index_names_and_descriptions += "\n=============================\n"
            index_names_and_descriptions += index_description["name"] + ": "
            index_names_and_descriptions += index_description["description"].strip()
            index_names_and_descriptions += "\n============================="
            index_names_and_descriptions += "\n"
        # Create an index from the string.
        index = GPTSimpleVectorIndex([Document(index_names_and_descriptions)])
        # Query the index.
        prompt_wrapper = f'I want to do address the following: "{prompt}"'
        prompt_wrapper += " Which index should I use? Answer with the index name and nothing else."
        resp = self._query_index(index, prompt_wrapper)
        return resp.response

    def query(self, prompt):
        """Query memory."""
        # Find the most relevant index.
        index_name = self.find_most_relevant_index(prompt)
        # Query the index.
        try:
            return self.query_index(index_name, prompt)
        except Exception:
            logger.info("Relevant index not found.")
            None
